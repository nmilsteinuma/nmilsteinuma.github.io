[
  {
    "path": "posts/2022-03-26-text-as-data-blog-post-4/",
    "title": "Text as Data Blog Post 4",
    "description": "In last weeks post I combined a the material for blog posts 2, 3, and began on the 4th post. As a result this will be a continuation of dictionary validation methods.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-26",
    "categories": [],
    "contents": "\n\n\n\nBlog Post 3 and 4:\nThird Blog Post and\nContinuation\nExplanation: This post covers material from week 5\npre-processing, week 6 representing text, and week 7 dictionary methods.\nIn this case I have slightly modified the data I am working with, this\nweek I am using 1000 new posts for a dictionary analysis rather than the\n“top” posts to see if there is a tangible difference in content.\nInital Loading and\nProcessing\nExplanation: My code below illustrates how I\ninitially got my information from reddit by scraping. In this case I\nused RedditExtractoR. The author of this packages describes it as a\nminimalist r wrapper it scrapes a limited number of posts from reddit.\nThe api on reddit itself only allows 60 requests per minute. In this\ncase I chose posts that were “new” as of march 26 2022 at 11:17 P.M.\nThis has resulted in 980 reddit posts, this subreddit is described as\n“Firearms and related articles” and much of the subreddit is\ndescriptions of, reviews, and highlights of firearms owned by users.\n\n\n# New_guns_urls <- find_thread_urls(subreddit=\"guns\", sort_by=\"new\")\n\n\nloadRData <- function(fileName){\n#loads an RData file, and returns it\n    load(fileName)\n    get(ls()[ls() != \"fileName\"])\n}\nNew_guns_urls_df <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Networks/Git_Projects/DACCS/nmilsteinuma.github.io/docs/_posts/2022-03-26-text-as-data-blog-post-4/New_reddit_posts_3_26.RData\")\n\n\n\n\n\nstr(New_guns_urls_df)\n\n\n'data.frame':   980 obs. of  7 variables:\n $ date_utc : chr  \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" ...\n $ timestamp: num  1.65e+09 1.65e+09 1.65e+09 1.65e+09 1.65e+09 ...\n $ title    : chr  \"With tiny ar15s and mcx/similar, where does a pcc fit in the arsenal now?\" \"I still don\\031t feel like this is good enough. Ha. Anyone else this anal about this stuff?\" \"Best Home Defense Round in 5.56?\" \"5\\0365*5/555&53 5\\r50535.5\\\"5-\" ...\n $ text     : chr  \"With the advent of small ar15s and similar foldy boys like the mcx, where does the pcc fit in now?\\n\\nSeems tha\"| __truncated__ \"\" \"I'm looking for some defensive rounds for my AR-15 that are reliable. Lately I've been shooting 55 Grain XTac g\"| __truncated__ \"\" ...\n $ subreddit: chr  \"guns\" \"guns\" \"guns\" \"guns\" ...\n $ comments : num  32 23 73 12 51 29 25 13 32 7 ...\n $ url      : chr  \"https://www.reddit.com/r/guns/comments/teruk1/with_tiny_ar15s_and_mcxsimilar_where_does_a_pcc/\" \"https://www.reddit.com/r/guns/comments/ter4zi/i_still_dont_feel_like_this_is_good_enough_ha/\" \"https://www.reddit.com/r/guns/comments/ter48v/best_home_defense_round_in_556/\" \"https://www.reddit.com/r/guns/comments/teqkl1/5\\0365*5/555&53_5\\r50535.5\\\"5-/\" ...\n\nConversion From Data Frame\nto Corpus\nExplanation: Below I have processed my initial\ndataframe from Reddit into a corpus and saved a summary of the resulting\ndata.\n\n\nnew_guns_urls_df<-New_guns_urls_df[,c(\"title\", \"date_utc\", \"comments\")]\n\nnew_guns_corpus<-corpus(new_guns_urls_df$title)\n\nnew_guns_documents<-new_guns_corpus[1:10,]\n\nnew_guns_corpus_summary <- summary(new_guns_corpus)\n\n\n\nBroad Characteristics\nExplanation: In order to clean the documents for\npre-processing and analysis I have removed punctuation, converted to\nlowercase and removed stopwords. Though the language of firearms is\noften associated with punctuation, such as 5.56, 3.57, and a variety of\nother calibers, which represent the diameter of the barrel required to\nfire each ammunition. However, losing the punctuation in caliber and\nfirearm titles would not reduce their comprehensibility in analysis if\nthey retain their form as 5.56 and 556 can be considered equally while\nreducing the complexity of tokens and potentially even sentences.\nConverting the documents to lowercase can also simplifies the data.\nHowever, for my inital analysis I will just being making a lowercase\ndocument feature matrix and a more edited one.\n\n\nnew_guns_corpus_dfm_tl<-tokens(new_guns_corpus) %>%  dfm(tolower=TRUE) \n\nnew_guns_corpus_dfm_punct_tl_sw <- tokens(new_guns_corpus,\n                                    remove_punct = TRUE,) %>%\n                           dfm(tolower=TRUE) %>%\n                           dfm_remove(stopwords('english'))\n\n\n\nTop features\nExplanation: Examining the top 20 features below we\nsee a fairly predictable set of response, as may be expected from the\ngun subreddit, the most used word is gun. Rifle and pistol are also in\nthe top 20. Individual letters such as “ar”, “s”, and “m” appear\nfrequently as they are commonly used designations for types of firearms\nor model names, ar-12, ar-15, m-4, m-16, m1911, 5-mm, and s559, s-12 and\nother designations. This indicates that these numbers are valuable, if\ndifficult to comprehend on their own. With no punctuation removed the\nfirst 20 features are not informative.\n\n\ntopfeatures(new_guns_corpus_dfm_tl, 20)\n\n\n    .     ?     a   the    my     ,     i    to   for   and    is \n  425   260   259   211   183   175   175   158   112   108    99 \n   in    it     /    of  with   gun  this    on first \n   93    89    84    81    80    78    76    76    60 \n\n\n\ntopfeatures(new_guns_corpus_dfm_punct_tl_sw, 20)\n\n\n     gun    first      new       22      can        s question \n      78       60       57       43       41       35       31 \n   rifle     just      amp     help   anyone       ar   pistol \n      29       29       28       27       26       26       26 \n    time     guns     know     good      got        m \n      25       24       23       22       22       22 \n\nWorld Cloud\nExplanation: Though not necessarily statistically\ninformative, the wordcloud below can give some sense of comparative\nfrequency using the limit of minimum count being 6. Reading through\nthese can give a sense of both the communal nature of the forum in\nasking for recommendations, but also the importance of the word purchase\nand other words associated with working with, and buying firearms. In\nthe case of the only lowercased data we can gather much less\ninformation.\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_tl, min_count = 12, random_order = T, rotation = 0)\n\n\n\n\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_punct_tl_sw, min_count = 6, random_order = T, rotation = 0)\n\n\n\n\nTypes, Tokens, and Sentances\nTypes\nExplanation: The gun corpus summary gives 3 counting\ncategories that we can interpret in order to get a sense of the\ncomplexity of the documents that we are using. Looking at the number of\ntypes on average we see a mean of 9.18 and qunantiles that indicate a\nrange of 2-45 with 50% being between 4 and 13 types.\n\n\nmean(new_guns_corpus_summary$Types)\n\n\n[1] 9.18\n\nquantile(new_guns_corpus_summary$Types)\n\n\n  0%  25%  50%  75% 100% \n 2.0  4.0  7.5 13.0 45.0 \n\nTokens\nExplanation: Tokens are relatively similar to types\nin this case. Here there is a mean of 9.72 but a range of 2-55 with the\nmiddle 50% ranging from 4-13 tokens, as was the case for types.\n\n\nmean(new_guns_corpus_summary$Tokens)\n\n\n[1] 9.72\n\nquantile(new_guns_corpus_summary$Tokens)\n\n\n  0%  25%  50%  75% 100% \n   2    4    8   13   55 \n\nSentances\nExplanation: As is indicated below, it appears that\nthe number of sentences in each post is generally one. Arroding to the\nqunatile statistics the most sentences in any post\n\n\nmean(new_guns_corpus_summary$Sentences)\n\n\n[1] 1.18\n\nquantile(new_guns_corpus_summary$Sentences)\n\n\n  0%  25%  50%  75% 100% \n   1    1    1    1    3 \n\nWord counts\nExplanation: Looking at word counts we see a similar\ntrend reflected where including stopwords and punctuation decreases the\nquality of data as little information but punctuation and stopwords are\nincluded.\n\n\nword_counts_new_1 <- as.data.frame(sort(colSums(new_guns_corpus_dfm_tl),dec=T))\n\ncolnames(word_counts_new_1) <- c(\"Frequency\")\n\nword_counts_new_1$Rank <- c(1:ncol(new_guns_corpus_dfm_tl))\n\nhead(word_counts_new_1)\n\n\n    Frequency Rank\n.         425    1\n?         260    2\na         259    3\nthe       211    4\nmy        183    5\n,         175    6\n\n\n\nword_counts_new <- as.data.frame(sort(colSums(new_guns_corpus_dfm_punct_tl_sw),dec=T))\n\ncolnames(word_counts_new) <- c(\"Frequency\")\n\nword_counts_new$Rank <- c(1:ncol(new_guns_corpus_dfm_punct_tl_sw))\n\nhead(word_counts_new)\n\n\n      Frequency Rank\ngun          78    1\nfirst        60    2\nnew          57    3\n22           43    4\ncan          41    5\ns            35    6\n\nZipf’s Law\nExplanation: As can be seen from the frequency\ngraphs below, Ziph’s Law of inverse proportion. In this case a words\nrank in freqency is inversely prorportional to the number of times it is\nobserved. Though the uncleaned dataset has far more frequency for its\nmost common words (much of which is punctuation) it appears to follow\nthe law.\n\n\nggplot(word_counts_new, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\n\n\nggplot(word_counts_new_1, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nData Trimming\nExplanation: Much of what I do here will be\nexplained in the code and results. Many words appear with a minimum\nfrequency of 4, though non are included in 10% and only 3 words are\nincluded in 5%. At a level of 2.5% we get 4 words.\n\n\n# First I trim the data to only include words that appear at least 4 times\n\nsmaller_dfm_4_freq <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 4)\n\n# Next I will look at proportions are see if there are words that are seen in\n# More than 10% and 5% of documents\nsmaller_dfm_10_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm_5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.05, docfreq_type = \"prop\")\n\nsmaller_dfm_2.5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.025,  docfreq_type = \"prop\")\n\n\n\nData Readability\nExplanation: Before making general modifications to\nthe data, it is valuable to also get a sense of readability, as in week\n5. In this case we will calculate readability scores based on 3\ndifferent measures, FOG, Coleman Liau, and Flesch Kincaid. Though this\nstep will not indicate what sort of pre-processing is best, or how the\ndata should be reduced, it does give us some insight into the complexity\nof the language in our data. In this case we just observe the\nreadability based on the post number.\n\n\nreadability_new_guns <- textstat_readability(new_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\", \"FOG\", \"Coleman.Liau.grade\")) \n\n# add in a chapter number\n\nreadability_new_guns$reddit_post <- c(1:nrow(readability_new_guns))\n\n# plot results\nggplot(readability_new_guns, aes(x = reddit_post)) +\n  geom_line(aes(y = Flesch.Kincaid), color = \"black\",  alpha=0.3) + \n  geom_line(aes(y = FOG), color = \"red\", alpha=0.3) + \n  geom_line(aes(y = Coleman.Liau.grade), color = \"blue\", alpha=0.3) + \n  theme_bw()\n\n\n\n\nExplanation: In this part we will add dates to our\ndata to see how the complexity changes over time or if it was relatively\nconstant. As can be seen below, the amount of complexity in the data\nvaries more smoothly when the data are sorted by data and not\narbitrarily by their post number, in this case all 3 complexity method\nexhibit similar trends.\n\n\nreadability_new_guns$added_dates <- as.Date(New_guns_urls_df$date_utc)\n\nggplot(readability_new_guns, aes(x = added_dates)) +\n  geom_smooth(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_smooth(aes(y = FOG), color = \"red\") + \n  geom_smooth(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_minimal()\n\n\n\n\nExplanation: Looking at the readability of the we\nsee that all correlations between the methods of complexity measurement\nare similar except of FOG and Coleman Liau, however the graphs above do\nindicate some similarity in trend between them, though not direct\ncorrelation in their estimates potentially.\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$FOG, use = \"complete.obs\")\n\n\n[1] 0.8321041\n\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.751674\n\n\n\ncor(readability_new_guns$FOG, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.5859859\n\nPre-processing\nBefore Reduction and Co-Occurance\nExplanation: Next I used the\nfactorial_preprocessing() command to both use n-grams processing and use\nan infrequent term threshold. This is in order to see what techniques,\nsuch as removing punctuation, stopwords, etc lead to a pre-text score\ndevised by Denny and Spirling. This pre-text score indicatess how many\nk-pairs of terms change the most when the pre-processing strategy is\nchanged. Lower scores indicate more usual results while higher scores\nindicate more unusual results and they are between 0 and 1. Here we have\nused n-grams and set an infreqent term threshold. Because of the nature\nof our data I will use 30% of documents as\n\n\n?factorial_preprocessing\npreprocessed_documents <- factorial_preprocessing(\n    new_guns_corpus,\n    use_ngrams = TRUE,\n    infrequent_term_threshold = 0.3,\n    verbose = FALSE)\n\n\nPreprocessing 980 documents 128 different ways...\n\n\n\nnames(preprocessed_documents)\n\n\n[1] \"choices\"  \"dfm_list\" \"labels\"  \n\nExplanation: As can be seen below the possible\nchoices are coded on the first column with each subsequent column\nindicating whether or not each choice includes each of the specified\nchoices in its assessment.\n\n\nhead(preprocessed_documents$choices)\n\n\n              removePunctuation removeNumbers lowercase stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE TRUE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\n\nExplanation: Next preText is calculated using 50\ncomparisons and a cosine distance calculation.\n\n\n#set.seed(12366)\n#preText_results <- preText(\n#    preprocessed_documents,\n#   dataset_name = \"Gun Pretext Results\",\n#   distance_method = \"cosine\",\n#   num_comparisons = 50,\n#  verbose = TRUE)\n\n\n\n\n\n#save(preText_results, file=\"preText_results_3_27_gun_50_comp.RData\")\n\n\n\n\n\npreText_results <- loadRData(\"~/Desktop/Spring 2022/Networks/Git_Projects/DACCS/nmilsteinuma.github.io/docs/_posts/2022-03-26-text-as-data-blog-post-4/preText_results_3_27_gun_50_comp.RData\")\n\npreText_results\n\n\n$preText_scores\n    preText_score preprocessing_steps\n1      0.04157825       P-N-L-S-W-I-3\n2      0.04157825         N-L-S-W-I-3\n3      0.04157825         P-L-S-W-I-3\n4      0.04157825           L-S-W-I-3\n5      0.04157825         P-N-S-W-I-3\n6      0.04157825           N-S-W-I-3\n7      0.04157825           P-S-W-I-3\n8      0.04157825             S-W-I-3\n9      0.04157825         P-N-L-W-I-3\n10     0.04157825           N-L-W-I-3\n11     0.04157825           P-L-W-I-3\n12     0.04157825             L-W-I-3\n13     0.04157825           P-N-W-I-3\n14     0.04157825             N-W-I-3\n15     0.04157825             P-W-I-3\n16     0.04157825               W-I-3\n17     0.04157825         P-N-L-S-I-3\n18     0.04157825           N-L-S-I-3\n19     0.04157825           P-L-S-I-3\n20     0.04157825             L-S-I-3\n21     0.04157825           P-N-S-I-3\n22     0.04157825             N-S-I-3\n23     0.04157825             P-S-I-3\n24     0.04157825               S-I-3\n25     0.04157825           P-N-L-I-3\n26     0.04157825             N-L-I-3\n27     0.04157825             P-L-I-3\n28     0.04157825               L-I-3\n29     0.04157825             P-N-I-3\n30     0.04157825               N-I-3\n31     0.04157825               P-I-3\n32     0.04157825                 I-3\n33     0.04815400         P-N-L-S-W-3\n34     0.08058534           N-L-S-W-3\n35     0.04403401           P-L-S-W-3\n36     0.08427290             L-S-W-3\n37     0.04864279           P-N-S-W-3\n38     0.07599185             N-S-W-3\n39     0.04470926             P-S-W-3\n40     0.08186325               S-W-3\n41     0.02723578           P-N-L-W-3\n42     0.02629912             N-L-W-3\n43     0.02280946             P-L-W-3\n44     0.02777490               L-W-3\n45     0.02723578             P-N-W-3\n46     0.02629912               N-W-3\n47     0.02280946               P-W-3\n48     0.02777490                 W-3\n49     0.02647384           P-N-L-S-3\n50     0.08304385             N-L-S-3\n51     0.02334213             P-L-S-3\n52     0.07225887               L-S-3\n53     0.02706859             P-N-S-3\n54     0.08697989               N-S-3\n55     0.02384351               P-S-3\n56     0.07767802                 S-3\n57     0.02647384             P-N-L-3\n58     0.02777727               N-L-3\n59     0.02334213               P-L-3\n60     0.01888648                 L-3\n61     0.02647384               P-N-3\n62     0.02777727                 N-3\n63     0.02334213                 P-3\n64     0.01888648                   3\n65     0.04157825         P-N-L-S-W-I\n66     0.04157825           N-L-S-W-I\n67     0.04157825           P-L-S-W-I\n68     0.04157825             L-S-W-I\n69     0.04157825           P-N-S-W-I\n70     0.04157825             N-S-W-I\n71     0.04157825             P-S-W-I\n72     0.04157825               S-W-I\n73     0.04157825           P-N-L-W-I\n74     0.04157825             N-L-W-I\n75     0.04157825             P-L-W-I\n76     0.04157825               L-W-I\n77     0.04157825             P-N-W-I\n78     0.04157825               N-W-I\n79     0.04157825               P-W-I\n80     0.04157825                 W-I\n81     0.04157825           P-N-L-S-I\n82     0.04157825             N-L-S-I\n83     0.04157825             P-L-S-I\n84     0.04157825               L-S-I\n85     0.04157825             P-N-S-I\n86     0.04157825               N-S-I\n87     0.04157825               P-S-I\n88     0.04157825                 S-I\n89     0.04157825             P-N-L-I\n90     0.04157825               N-L-I\n91     0.04157825               P-L-I\n92     0.04157825                 L-I\n93     0.04157825               P-N-I\n94     0.04157825                 N-I\n95     0.04157825                 P-I\n96     0.04157825                   I\n97     0.07109905           P-N-L-S-W\n98     0.16021925             N-L-S-W\n99     0.06767075             P-L-S-W\n100    0.17095529               L-S-W\n101    0.07098343             P-N-S-W\n102    0.15776534               N-S-W\n103    0.06802275               P-S-W\n104    0.16850136                 S-W\n105    0.02723578             P-N-L-W\n106    0.02344916               N-L-W\n107    0.02280946               P-L-W\n108    0.02739263                 L-W\n109    0.02723578               P-N-W\n110    0.02344916                 N-W\n111    0.02280946                 P-W\n112    0.02739263                   W\n113    0.05006683             P-N-L-S\n114    0.22396649               N-L-S\n115    0.04704759               P-L-S\n116    0.23717659                 L-S\n117    0.05064631               P-N-S\n118    0.21022978                 N-S\n119    0.04752479                 P-S\n120    0.23146371                   S\n121    0.02635536               P-N-L\n122    0.03527452                 N-L\n123    0.02342946                 P-L\n124    0.10226138                   L\n125    0.02635536                 P-N\n126    0.03527452                   N\n127    0.02342946                   P\n\n$ranked_preText_scores\n    preText_score preprocessing_steps\n1      0.23717659                 L-S\n2      0.23146371                   S\n3      0.22396649               N-L-S\n4      0.21022978                 N-S\n5      0.17095529               L-S-W\n6      0.16850136                 S-W\n7      0.16021925             N-L-S-W\n8      0.15776534               N-S-W\n9      0.10226138                   L\n10     0.08697989               N-S-3\n11     0.08427290             L-S-W-3\n12     0.08304385             N-L-S-3\n13     0.08186325               S-W-3\n14     0.08058534           N-L-S-W-3\n15     0.07767802                 S-3\n16     0.07599185             N-S-W-3\n17     0.07225887               L-S-3\n18     0.07109905           P-N-L-S-W\n19     0.07098343             P-N-S-W\n20     0.06802275               P-S-W\n21     0.06767075             P-L-S-W\n22     0.05064631               P-N-S\n23     0.05006683             P-N-L-S\n24     0.04864279           P-N-S-W-3\n25     0.04815400         P-N-L-S-W-3\n26     0.04752479                 P-S\n27     0.04704759               P-L-S\n28     0.04470926             P-S-W-3\n29     0.04403401           P-L-S-W-3\n30     0.04157825       P-N-L-S-W-I-3\n31     0.04157825         N-L-S-W-I-3\n32     0.04157825         P-L-S-W-I-3\n33     0.04157825           L-S-W-I-3\n34     0.04157825         P-N-S-W-I-3\n35     0.04157825           N-S-W-I-3\n36     0.04157825           P-S-W-I-3\n37     0.04157825             S-W-I-3\n38     0.04157825         P-N-L-W-I-3\n39     0.04157825           N-L-W-I-3\n40     0.04157825           P-L-W-I-3\n41     0.04157825             L-W-I-3\n42     0.04157825           P-N-W-I-3\n43     0.04157825             N-W-I-3\n44     0.04157825             P-W-I-3\n45     0.04157825               W-I-3\n46     0.04157825         P-N-L-S-I-3\n47     0.04157825           N-L-S-I-3\n48     0.04157825           P-L-S-I-3\n49     0.04157825             L-S-I-3\n50     0.04157825           P-N-S-I-3\n51     0.04157825             N-S-I-3\n52     0.04157825             P-S-I-3\n53     0.04157825               S-I-3\n54     0.04157825           P-N-L-I-3\n55     0.04157825             N-L-I-3\n56     0.04157825             P-L-I-3\n57     0.04157825               L-I-3\n58     0.04157825             P-N-I-3\n59     0.04157825               N-I-3\n60     0.04157825               P-I-3\n61     0.04157825                 I-3\n62     0.04157825         P-N-L-S-W-I\n63     0.04157825           N-L-S-W-I\n64     0.04157825           P-L-S-W-I\n65     0.04157825             L-S-W-I\n66     0.04157825           P-N-S-W-I\n67     0.04157825             N-S-W-I\n68     0.04157825             P-S-W-I\n69     0.04157825               S-W-I\n70     0.04157825           P-N-L-W-I\n71     0.04157825             N-L-W-I\n72     0.04157825             P-L-W-I\n73     0.04157825               L-W-I\n74     0.04157825             P-N-W-I\n75     0.04157825               N-W-I\n76     0.04157825               P-W-I\n77     0.04157825                 W-I\n78     0.04157825           P-N-L-S-I\n79     0.04157825             N-L-S-I\n80     0.04157825             P-L-S-I\n81     0.04157825               L-S-I\n82     0.04157825             P-N-S-I\n83     0.04157825               N-S-I\n84     0.04157825               P-S-I\n85     0.04157825                 S-I\n86     0.04157825             P-N-L-I\n87     0.04157825               N-L-I\n88     0.04157825               P-L-I\n89     0.04157825                 L-I\n90     0.04157825               P-N-I\n91     0.04157825                 N-I\n92     0.04157825                 P-I\n93     0.04157825                   I\n94     0.03527452                 N-L\n95     0.03527452                   N\n96     0.02777727               N-L-3\n97     0.02777727                 N-3\n98     0.02777490               L-W-3\n99     0.02777490                 W-3\n100    0.02739263                 L-W\n101    0.02739263                   W\n102    0.02723578           P-N-L-W-3\n103    0.02723578             P-N-W-3\n104    0.02723578             P-N-L-W\n105    0.02723578               P-N-W\n106    0.02706859             P-N-S-3\n107    0.02647384           P-N-L-S-3\n108    0.02647384             P-N-L-3\n109    0.02647384               P-N-3\n110    0.02635536               P-N-L\n111    0.02635536                 P-N\n112    0.02629912             N-L-W-3\n113    0.02629912               N-W-3\n114    0.02384351               P-S-3\n115    0.02344916               N-L-W\n116    0.02344916                 N-W\n117    0.02342946                 P-L\n118    0.02342946                   P\n119    0.02334213             P-L-S-3\n120    0.02334213               P-L-3\n121    0.02334213                 P-3\n122    0.02280946             P-L-W-3\n123    0.02280946               P-W-3\n124    0.02280946               P-L-W\n125    0.02280946                 P-W\n126    0.01888648                 L-3\n127    0.01888648                   3\n\n$choices\n              removePunctuation removeNumbers lowercase  stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE  TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE  TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE  TRUE\nP-S-W-I-3                  TRUE         FALSE     FALSE  TRUE\nS-W-I-3                   FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I-3                TRUE          TRUE      TRUE FALSE\nN-L-W-I-3                 FALSE          TRUE      TRUE FALSE\nP-L-W-I-3                  TRUE         FALSE      TRUE FALSE\nL-W-I-3                   FALSE         FALSE      TRUE FALSE\nP-N-W-I-3                  TRUE          TRUE     FALSE FALSE\nN-W-I-3                   FALSE          TRUE     FALSE FALSE\nP-W-I-3                    TRUE         FALSE     FALSE FALSE\nW-I-3                     FALSE         FALSE     FALSE FALSE\nP-N-L-S-I-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-I-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-I-3                  TRUE         FALSE      TRUE  TRUE\nL-S-I-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-I-3                  TRUE          TRUE     FALSE  TRUE\nN-S-I-3                   FALSE          TRUE     FALSE  TRUE\nP-S-I-3                    TRUE         FALSE     FALSE  TRUE\nS-I-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-I-3                  TRUE          TRUE      TRUE FALSE\nN-L-I-3                   FALSE          TRUE      TRUE FALSE\nP-L-I-3                    TRUE         FALSE      TRUE FALSE\nL-I-3                     FALSE         FALSE      TRUE FALSE\nP-N-I-3                    TRUE          TRUE     FALSE FALSE\nN-I-3                     FALSE          TRUE     FALSE FALSE\nP-I-3                      TRUE         FALSE     FALSE FALSE\nI-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-3                  TRUE         FALSE      TRUE  TRUE\nL-S-W-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-3                  TRUE          TRUE     FALSE  TRUE\nN-S-W-3                   FALSE          TRUE     FALSE  TRUE\nP-S-W-3                    TRUE         FALSE     FALSE  TRUE\nS-W-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-3                  TRUE          TRUE      TRUE FALSE\nN-L-W-3                   FALSE          TRUE      TRUE FALSE\nP-L-W-3                    TRUE         FALSE      TRUE FALSE\nL-W-3                     FALSE         FALSE      TRUE FALSE\nP-N-W-3                    TRUE          TRUE     FALSE FALSE\nN-W-3                     FALSE          TRUE     FALSE FALSE\nP-W-3                      TRUE         FALSE     FALSE FALSE\nW-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-3                  TRUE          TRUE      TRUE  TRUE\nN-L-S-3                   FALSE          TRUE      TRUE  TRUE\nP-L-S-3                    TRUE         FALSE      TRUE  TRUE\nL-S-3                     FALSE         FALSE      TRUE  TRUE\nP-N-S-3                    TRUE          TRUE     FALSE  TRUE\nN-S-3                     FALSE          TRUE     FALSE  TRUE\nP-S-3                      TRUE         FALSE     FALSE  TRUE\nS-3                       FALSE         FALSE     FALSE  TRUE\nP-N-L-3                    TRUE          TRUE      TRUE FALSE\nN-L-3                     FALSE          TRUE      TRUE FALSE\nP-L-3                      TRUE         FALSE      TRUE FALSE\nL-3                       FALSE         FALSE      TRUE FALSE\nP-N-3                      TRUE          TRUE     FALSE FALSE\nN-3                       FALSE          TRUE     FALSE FALSE\nP-3                        TRUE         FALSE     FALSE FALSE\n3                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-I                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I                  TRUE         FALSE      TRUE  TRUE\nL-S-W-I                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I                  TRUE          TRUE     FALSE  TRUE\nN-S-W-I                   FALSE          TRUE     FALSE  TRUE\nP-S-W-I                    TRUE         FALSE     FALSE  TRUE\nS-W-I                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I                  TRUE          TRUE      TRUE FALSE\nN-L-W-I                   FALSE          TRUE      TRUE FALSE\nP-L-W-I                    TRUE         FALSE      TRUE FALSE\nL-W-I                     FALSE         FALSE      TRUE FALSE\nP-N-W-I                    TRUE          TRUE     FALSE FALSE\nN-W-I                     FALSE          TRUE     FALSE FALSE\nP-W-I                      TRUE         FALSE     FALSE FALSE\nW-I                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-I                  TRUE          TRUE      TRUE  TRUE\nN-L-S-I                   FALSE          TRUE      TRUE  TRUE\nP-L-S-I                    TRUE         FALSE      TRUE  TRUE\nL-S-I                     FALSE         FALSE      TRUE  TRUE\nP-N-S-I                    TRUE          TRUE     FALSE  TRUE\nN-S-I                     FALSE          TRUE     FALSE  TRUE\nP-S-I                      TRUE         FALSE     FALSE  TRUE\nS-I                       FALSE         FALSE     FALSE  TRUE\nP-N-L-I                    TRUE          TRUE      TRUE FALSE\nN-L-I                     FALSE          TRUE      TRUE FALSE\nP-L-I                      TRUE         FALSE      TRUE FALSE\nL-I                       FALSE         FALSE      TRUE FALSE\nP-N-I                      TRUE          TRUE     FALSE FALSE\nN-I                       FALSE          TRUE     FALSE FALSE\nP-I                        TRUE         FALSE     FALSE FALSE\nI                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W                  TRUE          TRUE      TRUE  TRUE\nN-L-S-W                   FALSE          TRUE      TRUE  TRUE\nP-L-S-W                    TRUE         FALSE      TRUE  TRUE\nL-S-W                     FALSE         FALSE      TRUE  TRUE\nP-N-S-W                    TRUE          TRUE     FALSE  TRUE\nN-S-W                     FALSE          TRUE     FALSE  TRUE\nP-S-W                      TRUE         FALSE     FALSE  TRUE\nS-W                       FALSE         FALSE     FALSE  TRUE\nP-N-L-W                    TRUE          TRUE      TRUE FALSE\nN-L-W                     FALSE          TRUE      TRUE FALSE\nP-L-W                      TRUE         FALSE      TRUE FALSE\nL-W                       FALSE         FALSE      TRUE FALSE\nP-N-W                      TRUE          TRUE     FALSE FALSE\nN-W                       FALSE          TRUE     FALSE FALSE\nP-W                        TRUE         FALSE     FALSE FALSE\nW                         FALSE         FALSE     FALSE FALSE\nP-N-L-S                    TRUE          TRUE      TRUE  TRUE\nN-L-S                     FALSE          TRUE      TRUE  TRUE\nP-L-S                      TRUE         FALSE      TRUE  TRUE\nL-S                       FALSE         FALSE      TRUE  TRUE\nP-N-S                      TRUE          TRUE     FALSE  TRUE\nN-S                       FALSE          TRUE     FALSE  TRUE\nP-S                        TRUE         FALSE     FALSE  TRUE\nS                         FALSE         FALSE     FALSE  TRUE\nP-N-L                      TRUE          TRUE      TRUE FALSE\nN-L                       FALSE          TRUE      TRUE FALSE\nP-L                        TRUE         FALSE      TRUE FALSE\nL                         FALSE         FALSE      TRUE FALSE\nP-N                        TRUE          TRUE     FALSE FALSE\nN                         FALSE          TRUE     FALSE FALSE\nP                          TRUE         FALSE     FALSE FALSE\n                          FALSE         FALSE     FALSE FALSE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\nP-S-W-I-3                TRUE             TRUE       TRUE\nS-W-I-3                  TRUE             TRUE       TRUE\nP-N-L-W-I-3              TRUE             TRUE       TRUE\nN-L-W-I-3                TRUE             TRUE       TRUE\nP-L-W-I-3                TRUE             TRUE       TRUE\nL-W-I-3                  TRUE             TRUE       TRUE\nP-N-W-I-3                TRUE             TRUE       TRUE\nN-W-I-3                  TRUE             TRUE       TRUE\nP-W-I-3                  TRUE             TRUE       TRUE\nW-I-3                    TRUE             TRUE       TRUE\nP-N-L-S-I-3             FALSE             TRUE       TRUE\nN-L-S-I-3               FALSE             TRUE       TRUE\nP-L-S-I-3               FALSE             TRUE       TRUE\nL-S-I-3                 FALSE             TRUE       TRUE\nP-N-S-I-3               FALSE             TRUE       TRUE\nN-S-I-3                 FALSE             TRUE       TRUE\nP-S-I-3                 FALSE             TRUE       TRUE\nS-I-3                   FALSE             TRUE       TRUE\nP-N-L-I-3               FALSE             TRUE       TRUE\nN-L-I-3                 FALSE             TRUE       TRUE\nP-L-I-3                 FALSE             TRUE       TRUE\nL-I-3                   FALSE             TRUE       TRUE\nP-N-I-3                 FALSE             TRUE       TRUE\nN-I-3                   FALSE             TRUE       TRUE\nP-I-3                   FALSE             TRUE       TRUE\nI-3                     FALSE             TRUE       TRUE\nP-N-L-S-W-3              TRUE            FALSE       TRUE\nN-L-S-W-3                TRUE            FALSE       TRUE\nP-L-S-W-3                TRUE            FALSE       TRUE\nL-S-W-3                  TRUE            FALSE       TRUE\nP-N-S-W-3                TRUE            FALSE       TRUE\nN-S-W-3                  TRUE            FALSE       TRUE\nP-S-W-3                  TRUE            FALSE       TRUE\nS-W-3                    TRUE            FALSE       TRUE\nP-N-L-W-3                TRUE            FALSE       TRUE\nN-L-W-3                  TRUE            FALSE       TRUE\nP-L-W-3                  TRUE            FALSE       TRUE\nL-W-3                    TRUE            FALSE       TRUE\nP-N-W-3                  TRUE            FALSE       TRUE\nN-W-3                    TRUE            FALSE       TRUE\nP-W-3                    TRUE            FALSE       TRUE\nW-3                      TRUE            FALSE       TRUE\nP-N-L-S-3               FALSE            FALSE       TRUE\nN-L-S-3                 FALSE            FALSE       TRUE\nP-L-S-3                 FALSE            FALSE       TRUE\nL-S-3                   FALSE            FALSE       TRUE\nP-N-S-3                 FALSE            FALSE       TRUE\nN-S-3                   FALSE            FALSE       TRUE\nP-S-3                   FALSE            FALSE       TRUE\nS-3                     FALSE            FALSE       TRUE\nP-N-L-3                 FALSE            FALSE       TRUE\nN-L-3                   FALSE            FALSE       TRUE\nP-L-3                   FALSE            FALSE       TRUE\nL-3                     FALSE            FALSE       TRUE\nP-N-3                   FALSE            FALSE       TRUE\nN-3                     FALSE            FALSE       TRUE\nP-3                     FALSE            FALSE       TRUE\n3                       FALSE            FALSE       TRUE\nP-N-L-S-W-I              TRUE             TRUE      FALSE\nN-L-S-W-I                TRUE             TRUE      FALSE\nP-L-S-W-I                TRUE             TRUE      FALSE\nL-S-W-I                  TRUE             TRUE      FALSE\nP-N-S-W-I                TRUE             TRUE      FALSE\nN-S-W-I                  TRUE             TRUE      FALSE\nP-S-W-I                  TRUE             TRUE      FALSE\nS-W-I                    TRUE             TRUE      FALSE\nP-N-L-W-I                TRUE             TRUE      FALSE\nN-L-W-I                  TRUE             TRUE      FALSE\nP-L-W-I                  TRUE             TRUE      FALSE\nL-W-I                    TRUE             TRUE      FALSE\nP-N-W-I                  TRUE             TRUE      FALSE\nN-W-I                    TRUE             TRUE      FALSE\nP-W-I                    TRUE             TRUE      FALSE\nW-I                      TRUE             TRUE      FALSE\nP-N-L-S-I               FALSE             TRUE      FALSE\nN-L-S-I                 FALSE             TRUE      FALSE\nP-L-S-I                 FALSE             TRUE      FALSE\nL-S-I                   FALSE             TRUE      FALSE\nP-N-S-I                 FALSE             TRUE      FALSE\nN-S-I                   FALSE             TRUE      FALSE\nP-S-I                   FALSE             TRUE      FALSE\nS-I                     FALSE             TRUE      FALSE\nP-N-L-I                 FALSE             TRUE      FALSE\nN-L-I                   FALSE             TRUE      FALSE\nP-L-I                   FALSE             TRUE      FALSE\nL-I                     FALSE             TRUE      FALSE\nP-N-I                   FALSE             TRUE      FALSE\nN-I                     FALSE             TRUE      FALSE\nP-I                     FALSE             TRUE      FALSE\nI                       FALSE             TRUE      FALSE\nP-N-L-S-W                TRUE            FALSE      FALSE\nN-L-S-W                  TRUE            FALSE      FALSE\nP-L-S-W                  TRUE            FALSE      FALSE\nL-S-W                    TRUE            FALSE      FALSE\nP-N-S-W                  TRUE            FALSE      FALSE\nN-S-W                    TRUE            FALSE      FALSE\nP-S-W                    TRUE            FALSE      FALSE\nS-W                      TRUE            FALSE      FALSE\nP-N-L-W                  TRUE            FALSE      FALSE\nN-L-W                    TRUE            FALSE      FALSE\nP-L-W                    TRUE            FALSE      FALSE\nL-W                      TRUE            FALSE      FALSE\nP-N-W                    TRUE            FALSE      FALSE\nN-W                      TRUE            FALSE      FALSE\nP-W                      TRUE            FALSE      FALSE\nW                        TRUE            FALSE      FALSE\nP-N-L-S                 FALSE            FALSE      FALSE\nN-L-S                   FALSE            FALSE      FALSE\nP-L-S                   FALSE            FALSE      FALSE\nL-S                     FALSE            FALSE      FALSE\nP-N-S                   FALSE            FALSE      FALSE\nN-S                     FALSE            FALSE      FALSE\nP-S                     FALSE            FALSE      FALSE\nS                       FALSE            FALSE      FALSE\nP-N-L                   FALSE            FALSE      FALSE\nN-L                     FALSE            FALSE      FALSE\nP-L                     FALSE            FALSE      FALSE\nL                       FALSE            FALSE      FALSE\nP-N                     FALSE            FALSE      FALSE\nN                       FALSE            FALSE      FALSE\nP                       FALSE            FALSE      FALSE\n                        FALSE            FALSE      FALSE\n\n$regression_results\n    Coefficient          SE                Variable\n1  0.0692437554 0.008765654               Intercept\n2 -0.0248352010 0.006026387      Remove Punctuation\n3 -0.0012912608 0.006026387          Remove Numbers\n4  0.0008042152 0.006026387               Lowercase\n5  0.0315263364 0.006026387                Stemming\n6 -0.0031236074 0.006026387        Remove Stopwords\n7 -0.0194667091 0.006026387 Remove Infrequent Terms\n8 -0.0194780796 0.006026387              Use NGrams\n                Model\n1 Gun Pretext Results\n2 Gun Pretext Results\n3 Gun Pretext Results\n4 Gun Pretext Results\n5 Gun Pretext Results\n6 Gun Pretext Results\n7 Gun Pretext Results\n8 Gun Pretext Results\n\n\n\n#load(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData\")\n\npreText_score_plot(preText_results)\n\n\n\n\nExplanation: After plotting we access the pretext\nscore with the minimum score, which is least unusual. This is the row\nwith the pre-processing steps refered to as “3” in the data. In addition\nL-3 results in the same preText Score.\n\n\nscores_new_pretext<-preText_results$preText_score \n\n# head(sort(scores_new_pretext))\n\n\n\nExplanation: Looking at the choices below I see that\n“3” does not do anything but use n-grams. L-3 does use lowercase and\nn-grams.\n\n\n# preprocessed_documents$choices\n\n\n\nExplanation Continued: Looking at the regression\ncoefficients we see negative scores as usual results and positive\ncoefficients as unusual ones. In this case removing puncuation,\nstopwords, and n-grams would not lead to a great deal of abnormality.\nThe scores below indicate that stemming would result in the most\nabnormality while all others but lowercase is the only other that has a\nnon-negative coefficinet.\n\n\nregression_coefficient_plot(preText_results,\n                            remove_intercept = TRUE)\n\n\n\n\nFeature Co-occurance Matrix\nExplanation: The feature co-occurance matrix can\ngive us a sense of which words in the dataset are occurring together\n\n\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 5)\n\n#smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n\n\n[1] 226 226\n\n\n\n# pull the top features\nmyFeatures <- names(topfeatures(smaller_fcm, 40))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n\n\n[1] 40 40\n\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n\n\n\n\nSentiment Results Using NRC\n\n\n# get_sentiments(\"nrc\")\n# get_sentiments(\"bing\")\n# get_sentiments(\"afinn\")\n\n\n\n\n\nsentimetnsdf <- read_csv(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv\")\n\n\n\n\n\nnew_guns_urls_df_2<-new_guns_urls_df\n\nnew_guns_urls_df_2$text<- seq(1, 980, by=1)\n\nnrc_joy <- sentimetnsdf %>% \n  filter(sentiment == \"joy\")\n\ntidy_posts_for_guns <- new_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\ngood\n22\nlove\n12\nfinally\n11\nsafe\n10\nfun\n7\nfavorite\n6\n\n\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\nnrc_sentiment <- get_sentiments(\"nrc\")\n\n\nnrc_guns_word_counts <- tidy_posts_for_guns %>%\n  inner_join(nrc_sentiment) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nnrc_guns_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\nBing_sentiments<-get_sentiments(\"bing\")\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(Bing_sentiments) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\nbing_word_counts <- tidy_posts_for_guns %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\ntidy_posts_for_guns$added_dates <- as.Date(tidy_posts_for_guns$date_utc)\n\n\nafinn <- tidy_posts_for_guns %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(index = added_dates) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\nafinn\n\n\n# A tibble: 14 × 3\n   index      sentiment method\n   <date>         <dbl> <chr> \n 1 2022-03-14         9 AFINN \n 2 2022-03-15        17 AFINN \n 3 2022-03-16         0 AFINN \n 4 2022-03-17        23 AFINN \n 5 2022-03-18         7 AFINN \n 6 2022-03-19        30 AFINN \n 7 2022-03-20        13 AFINN \n 8 2022-03-21        11 AFINN \n 9 2022-03-22        19 AFINN \n10 2022-03-23         8 AFINN \n11 2022-03-24        32 AFINN \n12 2022-03-25         7 AFINN \n13 2022-03-26         6 AFINN \n14 2022-03-27         0 AFINN \n\n\n\nafinn %>%\n  ggplot(aes(index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE,   width = 0.7)  + \n  geom_smooth(aes(y = sentiment), color = \"black\")+\nfacet_wrap(~method, ncol = 1, scales = \"free_y\")+\n  theme_minimal()\n\n\n\n\nSentiment Results Using BING\nExplanation: Using nrc appears to have had some\nunintended effects that may require an analysis of the specific words\nused to describe sentiment. One difficult part of the data being used is\nthat firearms, and the words used to describe them, are percieved\n\n\nlibrary(methods)\n\ntoo_gun_dfm<- quanteda::dfm(new_guns_corpus, verbose = FALSE)\n\ntoo_gun_dfm\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))\ngun_dfm_lda\n\n\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\ngun_dfm_lda_topics\n\n\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\nbeta_wide <- gun_dfm_lda_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n\n\n\n\nbeta_wide %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\nTopic Modeling analysis\nResponse: As can be seen above topic modeling may\nbenefit from some data reduction, removing punctuation and stop words\nwould likely be beneficial as can be seen above where a number of the\ndifferences between topics are modeled as punctuation and stop\nwords.\n\n\ngun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(new_guns_corpus, remove_punct = TRUE), c(stopwords(\"english\")))\n\ngun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=\" \")\n)\n\ngun_corpus_stopwords_and_punct_removed\n\n\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm_no_punct_stopwords<- quanteda::dfm(tokens(gun_corpus_stopwords_and_punct_removed), verbose = FALSE)\n\ntoo_gun_dfm_no_punct_stopwords\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))\n\ngun_dfm_lda_nopunct_stop\n\n\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\nbeta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide_no_punct_stop %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\nTokens and Corpus Work\n\n\ntop_guns_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\ntop_guns_tokens_no_punct <- tokens(new_guns_corpus, \n    remove_punct = T)\n\nprint(top_guns_tokens_no_punct)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"Ha\"     \"Anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"AR\"        \n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)\n\nprint(top_guns_tokens_no_punct_no_upper)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"with\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"i\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"ha\"     \"anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"my\"         \"suppressed\" \"12.5\"       \"ar\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"ar\"        \n\ntext6 :\n [1] \"springfield\" \"hellcat\"     \"rdp\"         \"with\"       \n [5] \"silencerco\"  \"omega\"       \"9k\"          \"and\"        \n [9] \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords(\"en\"), selection = \"remove\")\n\nlength(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\n[1] 980\n\nprint(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\nTokens consisting of 980 documents.\ntext1 :\n[1] \"tiny\"    \"ar15s\"   \"mcx\"     \"similar\" \"pcc\"     \"fit\"    \n[7] \"arsenal\" \"now\"    \n\ntext2 :\n [1] \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"good\"   \"enough\"\n [8] \"ha\"     \"anyone\" \"else\"   \"anal\"   \"stuff\" \n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n[1] \"suppressed\" \"12.5\"       \"ar\"         \"size\"       \"14.5\"      \n[6] \"ar\"        \n\ntext6 :\n[1] \"springfield\" \"hellcat\"     \"rdp\"         \"silencerco\" \n[5] \"omega\"       \"9k\"          \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_corpus_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_corpus_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\nhead(annotated.guns_corpus$token)\n\n\n# A tibble: 6 × 11\n  doc_id   sid tid   token token_with_ws lemma upos  xpos  feats      \n   <int> <int> <chr> <chr> <chr>         <chr> <chr> <chr> <chr>      \n1      1     1 1     With  \"With \"       with  ADP   IN    <NA>       \n2      1     1 2     tiny  \"tiny \"       tiny  ADJ   JJ    Degree=Pos \n3      1     1 3     ar    \"ar\"          ar    NOUN  NN    Number=Sing\n4      1     1 4     15s   \"15s \"        15s   NOUN  NNS   Number=Plur\n5      1     1 5     and   \"and \"        and   CCONJ CC    <NA>       \n6      1     1 6     mcx/  \"mcx/\"        mcx/  SYM   NFP   <NA>       \n# … with 2 more variables: tid_source <chr>, relation <chr>\n\nhead(annotated.guns_corpus$document)\n\n\n  doc_id\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n\ndoc_id_guns<-annotated.guns_corpus$document\n\ndoc_id_guns$date<-new_guns_urls_df$date_utc\n\nannoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = \"doc_id\")\n\nannoData$date<-as.Date(annoData$date)\n\n\n\n\n\nannoData %>% \n  group_by(date) %>% \n  summarize(Sentences = max(sid)) %>%\n  ggplot(aes(date, Sentences)) +\n    geom_line() +\n    geom_smooth() +\n    theme_bw()\n\n\n\n\n\n\n#sentimetnsdf<-get_sentiments(\"nrc\")\n\n#write.csv(sentimetnsdf, file = \"sentimetnsdf.csv\")\n\n#save(sentimetnsdf, file=\"sentimetnsdf_2\")\n\n\n\n`\nTopic\nModeling analysis with stopwords and punctuation removed\nResponse: As can be seen from the results above,\nremoving stopwords and punctuation removes a good deal of the unwanted\nlanguage from the corpus and does a slightly more comprehensible job in\ndisplaying the information. However, any kind of stemming or reduction\nwill be difficult with posts about firearms for a number of reasons.\nFirstly the language surrounding firearms involves numbers for model\nnumbers, ammunition calibers and the capacity of magazines and other\ndevices that hold bullets. This results in difficulty removing both\npunctuation and numbers from the data as they give a sense of what sort\nof each of the aforementioned items people are interesting in talking\nabout. As a results removing the punctuation is difficult because it\nallows for more comprehensible data by reducing the usage of unneeded\npunctuation like exclamaintion points and questions marks that are\ncommon on a forum of this nature but not useful in analyzing the common\ntopics and language.\n\n\n\n",
    "preview": "posts/2022-03-26-text-as-data-blog-post-4/text-as-data-blog-post-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-03-28T12:26:00-04:00",
    "input_file": "text-as-data-blog-post-4.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-21-blog-post-7/",
    "title": "Blog Post 7, Integrating ML",
    "description": "This post is an analysis of community structure and machine learning techniques on my medieval dataset.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-21",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nAn Introduction to\nthe Project and Dataset\nThe project that I am doing involves conflict in\nthe high middle ages. This was the period between 1000 and 1200\n\n\n\nPart 1:\nDescribe the Dataset You Are\nUsing:\nThe Dataset Being Used: The dataset that I am using\nis wikipedia list of wars throughout history, this article is the “List\nof wars: 1000–1499” which acts as a subset of the “2nd-millennium\nconflicts” I chose this dataset as an exemplar of popular history’s\ndepiction of the centralization of worldwide conflict. Wikipedia, being\nan accessible source generally created from relevant citations makes it\na good case study to see where historical writers and academics center\ntheir world are relevant conflicts.\nIdentify initial network\nformat:\nAnswer: The initial network format is as an edge\nlist, the first, in column contains the winners of each\nwar while the second, out column contains the losers of\neach. These sets of belligerents are directed\nNetwork\nStructure: Wars Startings in the 1000s\n\n Network attributes:\n  vertices = 111 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 153 \n    missing edges= 0 \n    non-missing edges= 153 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork\nStructure: Wars Startings in the 1100s\n\n Network attributes:\n  vertices = 97 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 238 \n    missing edges= 0 \n    non-missing edges= 238 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork Structure:\nWars Starting in the 1200s\n\n Network attributes:\n  vertices = 161 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 313 \n    missing edges= 0 \n    non-missing edges= 313 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nIdentify Nodes: Describe and identify the nodes\n(including how many nodes are in the dataset)\nAnswer: Nodes or vertices in these datasets\nrepresent belligerents in wars throughout history, the involved parties\nin each conflict can be a nation, province, individual, or group so long\nas they are listed as involved in the conflict. In the 1000s there are\n117, in the 1100s there are 78 and in the 1200s there are 161.\nWhat Constitutes a Tie: What constitutes a tie or\nedge (including how many ties, whether ties are directed/undirected and\nweighted/binary, and how to interpret the value of the tie if any)\nAnswer: A tie or edge in this dataset represents a\nwar, this war can be between two nations or groups within a nation.\nThese edges can represent a war that involved many more nations but are\nalways tied to each and every party involved on both sides. These edges\nare directed and the direction indicates which side “won” the conflict\n(if an edge has an arrow pointing to another the node that originated\nthat arrow won the war against them. There are 153 edges in the 1000s,\n225 edges in 1100s and 313 edges in the 1200s.\nEdge Attributes and Subset: Whether or not there are\nedge attributes that might be used to subset data or stack multiple\nnetworks (e.g., tie type, year, etc).\nAnswer: There are a number of attributes that could\nbe used to subset the data, year that the conflict began or the length\nof time it lasted are available. Aspects like each side’s religion and\nthe area where the conflict took place could be used to subset the data\nitself.\nPart 2:\nBrokerage and Betweeness\ncentrality\nWhat are betweeness and brokerage cenrrality\nCalculate brokerage and betweenneess centrality measures for one or more\nsubsets of your network data, and write up the results and your\ninterpretation of them.\nAnswer: I will be calculating these measures for\nwars in 1000-1099, 1100-1199, and 1200-1399.\n\n\n\nBrokerage scores in the\n1000s\n\n\n\n\n\n(wars_in_1000s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,11:15)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nByzantine Empire\n22.7376579\nNaN\n3.1654785\nNaN\nNaN\nHoly Roman Empire\n9.2813605\nNaN\n2.2468427\nNaN\nNaN\nSultanate of Rum\n9.2813605\nNaN\n-0.5090648\nNaN\nNaN\nEngland\n6.9745666\nNaN\n5.0036896\n-0.0853606\n-0.0853606\nKingdom of Sicily\n5.0522384\n-0.0176111\n4.0866123\n-0.1201631\n-0.1201631\nSeljuk Empire\n1.9765133\n-0.0176111\n-0.5084146\n3.4677529\n-0.1201631\nKingdom of France\n1.9765133\nNaN\n-0.5090648\nNaN\nNaN\nKingdom of Georgia\n0.8231164\n-0.0176111\n-0.5084146\n-0.1201631\n-0.1201631\nPapal States\n0.4386507\n-0.0176111\n-0.5084146\n-0.1201631\n10.6435850\nGhaznavids\n0.0541851\n-0.1380791\n-0.4907567\n-0.2903366\n-0.2903366\n\nBrokerage scores in the\n1100s\n\n\n\n\n\n\n\n\n(wars_in_1100s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,10:14)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nKingdom of Jerusalem\n17.1050061\nNaN\n2.8705599\n24.5610650\n-0.1357675\nFatimid Caliphate\n10.2415178\nNaN\n-0.6472506\nNaN\nNaN\nAyyubid Dynasty\n9.3615834\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nZengid Dynasty\n7.4257278\nNaN\n0.7591543\nNaN\nNaN\nByzantine Empire\n6.8977671\nNaN\n0.7602887\n-0.1357675\n-0.1357675\nEngland\n5.8418459\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nHoly Roman Empire\n3.0260558\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of France\n1.6181608\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of Sicily\n0.5622395\n-0.1467125\n-0.6293842\n-0.3476788\n-0.3476788\nPapal States\n0.0342789\n-0.1264908\n-0.6336748\n-0.3236913\n2.5014147\n\nBrokerage scores in the\n1200s\n\n\n\n\n\n\n\n\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nMongol Empire\n47.964825\nNaN\n-0.5966483\nNaN\nNaN\nKingdom of France\n28.663539\nNaN\n-0.5966483\nNaN\nNaN\nAyyubid Dynasty\n26.995527\nNaN\n2.3528915\nNaN\nNaN\nKingdom of England\n21.991489\nNaN\n8.9893561\nNaN\nNaN\nRepublic of Genoa\n11.983415\nNaN\n-0.5966483\nNaN\nNaN\nKnights Templar\n10.077115\nNaN\n1.6155066\nNaN\nNaN\nHoly Roman Empire\n4.834790\n-0.0170801\n-0.5961482\n10.865523\n10.865523\nPrincipality of Antioch\n4.834790\n-0.0170801\n2.3541101\n13.613565\n-0.126648\nKingdom of Cyprus\n4.596503\n58.5391124\n0.1414163\n13.613565\n10.865523\nArmenian Kingdom of Cilicia\n3.881640\n-0.0170801\n-0.5961482\n-0.126648\n-0.126648\n\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\nCounty of Apulia\n-0.1201631\nKingdom of Sicily\n-0.1201631\nKingdom of Georgia\n-0.1201631\nGreat Seljuq Empire\n-0.1201631\nSeljuk Empire\n-0.1201631\nname\nbroker.tot\nByzantine Empire\n22.7376579\nHoly Roman Empire\n9.2813605\nSultanate of Rum\n9.2813605\nEngland\n6.9745666\nKingdom of Sicily\n5.0522384\nSeljuk Empire\n1.9765133\nKingdom of France\n1.9765133\nKingdom of Georgia\n0.8231164\nPapal States\n0.4386507\nGhaznavids\n0.0541851\n\nOption 2.A\nFor a Specific Research Question: If you have a\nspecific research question, please feel free to use that to guide your\nanalysis. Otherwise, you may want to orient your analysis as follows in\norder to identify a compelling question or noteworthy pattern in the\ndata that can be interpreted.\nAnswer: Since I am interested in the relative power\nof nations by their relative position ad centrality in the worldwide\nconflict, network brokerage can be used to illustrate significant\npositions in global conflict. Below I wanted to look at 4 kinds of\nbrokerage, these are broker.gate or gatekeeper, coordinator, liason, and\nitinerant. I am interested to see if these specific coordination types\nare primarily done by specific nations.\n\n\n\n\n\n\n\n\n\nTotal Brokerage\nExplanation: Looking at total brokerage in this\ndataset gives a sense of which factions were responsible for highest\nconnection of unconnected actors through conflict. Given the crusades\nigniting conflict between Europe and the middle east it is sensible that\nthe Byzantine Empire in the center of both connects the most unconnected\nactors through conflict closely followed by the Sultanate of Rum, a\nmajor Muslim faction that fought against the crusades and third being\nthe Holy Roman Empire who participated in many conflicts including the\ncrusades. These are followed by England who centered the wars in the\nBritish isles and the Kingdom of Sicily who were also in a position of\nconflict.\n\nname\nbroker.tot\nByzantine Empire\n22.737658\nHoly Roman Empire\n9.281360\nSultanate of Rum\n9.281360\nEngland\n6.974567\nKingdom of Sicily\n5.052238\n\nCoordinator Brokerage\nExplanation: In this case no particular country is\nvery high above any other in terms of their coordinator brokerage,\nmeaning that within groups no particular nations appear to be brokering\nmore within the groups.\n\nname\nbroker.coord\nCounty of Apulia\n-0.0176111\nKingdom of Sicily\n-0.0176111\nKingdom of Georgia\n-0.0176111\nGreat Seljuq Empire\n-0.0176111\nPapal States\n-0.0176111\n\nItinerant Brokerage\nExplanation: Itinerant brokerage represents when a\nnon-group actor connects 2 actors in a group it is no in to each other,\nin this case England has the highest score. Looking at the network graph\nthey do appear to connect 2 actors in a group together.\n\nname\nbroker.itin\nEngland\n5.0036896\nKingdom of Sicily\n4.0866123\nByzantine Empire\n3.1654785\nHoly Roman Empire\n2.2468427\nPrincipality of Kiev\n0.4812412\n\nRepresentative Brokerage\nExplanation: Representative brokerage indicates that\nthe broker, or nation in question loses a war to another in their group,\nbut wins another against a faction outside of their group. This can be\nthough of as their directed connections to them. In this case the Seljuk\nEmpire and Kingdom of Aragon have instances in which they lose to\nfactions within their group before beating those outside of it.\n\nname\nbroker.rep\nSeljuk Empire\n3.4677529\nKingdom of Aragon\n0.9281821\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\n\nGatekeeper Brokerage\nExplanation: The Papal states being ranked highest\nin gatekeeper brokerage is an interesting observation as no other nation\nin the dataset appears to be close to their level as most are negative\nin this category. In this cae being a gatekeeper means that they are in\nat conflict in a group with another while the nation in a different\ngroup of conflicts is only at war with them from the group. This is an\ninteresting observation given the Papal states role as a coordinator of\nthe war, but not a participant in the conflcit as directly as other\nbelligerents. (This being the crusade given the period)\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\n\nLiaison Brokerage\nExplanation: A liaison broker, in this case, is a\nfaction that loses a war to a group they do not belong to and wins a war\nagainst a different group than the first that they also do not belong\nto. The Byzantine Empire, Sultanate of Rum, and Holy Roman Empire are\nhighest in this category likely owing to their frequent states of\nconflict beyond the crusades against a variety of groups.\n\nname\nbroker.lia\nByzantine Empire\n28.140866\nSultanate of Rum\n12.477603\nHoly Roman Empire\n10.961803\nEngland\n6.548214\nKingdom of Sicily\n4.589419\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1000s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1000s Plot igraph\n\n\n\nNetwork Graphing 1100s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1100s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1100s Plot igraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity Grouping\nLabel Propagation 1000s:\nThe first community cluster below is done using label propagation.\nThis results in 39 groups\n\n\nset.seed(23)\ncomm.lab<-label.propagation.community(wars_in_1000s.ig)\n#Inspect clustering object\n# igraph::groups(comm.lab)\n\n\n\n\n\n\nWalktrap 1000s:\nWalktrap classification as seen below results in 19 distinct\ncommunities.\n\n\nset.seed(238)\n\nwars_in_1000s.wt<-walktrap.community(wars_in_1000s.ig)\n\n#igraph::groups(wars_in_1000s.wt)\n\n\n\nAdding more steps resulted in 19 groups for both 10 and 20 steps.\n\n\n#Run & inspect clustering algorithm: 10 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) \n#Run & inspect clustering algorithm: 20 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))\n#Run & inspect clustering algorithm\n\n\n\n\n\n\nLabel Propagation 1100s:\nIn the 1100s data, label propagation leads to 30 groups, despite the\ncomparatively smaller number of nations, or observations in the\ndataset.\n\n\nset.seed(12376)\n\ncomm.lab_1100s<-label.propagation.community(wars_in_1100s.ig)\n#igraph::groups(comm.lab_1100s)\n\n\n\n\n\n\nWalktrap 1100s:\nThe walktrap method results in 16 communities, which is significantly\nless than with the label propogation method.\n\n\nwars_in_1100s.wt<-walktrap.community(wars_in_1100s.ig)\n\n# igraph::groups(wars_in_1100s.wt)\n\n\n\nA walktrap with 10 steps further reduces the number of groups to 14\nwhile 20 steps results in 15 communities.\n\n\nset.seed(982)\n#Run & inspect clustering algorithm: 10 steps\n#igraph::groups(walktrap.community(wars_in_1100s.ig, steps=10)) \n#Run & inspect clustering algorithm: 20 steps\n#igraph::groups(walktrap.community(wars_in_1100s.ig ,steps=20))\n#Run & inspect clustering algorithm\n\n\n\n\n\n\n\n\nwars_in_1000s_edgelist <- as.matrix(wars_in_1000s)\n\nwars_in_1000s_edgelist_network_edgelist <- graph.edgelist(wars_in_1000s_edgelist, directed=TRUE)\n\nwars_in_1000s.ig<-graph_from_data_frame(wars_in_1000s)\n\nwars_in_1000s_network <- asNetwork(wars_in_1000s.ig)\n\n\n\n\n\naspects_of_1000s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1000s_states.xlsx\")\n\ntotal_1000s <- merge(aspects_of_1000s_states, wars_in_1000s.nodes.stat_2, by=\"name\")\n\n\n\nMachine\nLearning, Regression and Principle Components:\nThe machine learning techniques I will use in the paper will\nprimarily involve.\n\n\ntotal_1000s_brokerag_reg<-total_1000s\n\ntotal_1000s_brokerag_reg$win_rate <- (total_1000s_brokerag_reg$outdegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg$loss_rate <- (total_1000s_brokerag_reg$indegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg_binom <- total_1000s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-name-totdegree-indegree-outdegree-dc-eigen.dc-win_rate-loss_rate, total_1000s_brokerag_reg_binom, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - name - totdegree - indegree - \n    outdegree - dc - eigen.dc - win_rate - loss_rate, family = binomial, \n    data = total_1000s_brokerag_reg_binom)\n\nCoefficients:\n (Intercept)      Catholic         Islam      Orthodox      Buddhist  \n  -2.090e+01     1.446e-01    -7.108e-02    -4.043e-01    -8.572e-02  \n       Pagan      Tengrism        Shinto         Hindu     Shamanism  \n   5.506e-01    -5.656e+01     1.820e+00    -2.142e+00    -1.506e+00  \n       eigen         close            rc      eigen.rc    broker.tot  \n  -1.877e+03     5.146e+03    -3.979e+00     1.574e+03     2.378e+02  \nbroker.coord   broker.itin    broker.rep   broker.gate    broker.lia  \n  -9.610e+01    -9.449e+01    -7.164e+01    -2.810e+01    -1.298e+02  \n\nDegrees of Freedom: 101 Total (i.e. Null);  82 Residual\n  (8 observations deleted due to missingness)\nNull Deviance:      140.8 \nResidual Deviance: 4.53e-09     AIC: 40\n\n\n\nset.seed(292)\n\ntotal_1000s_for_regression <- total_1000s[,-c(1, 20:25)]\n\ntotal_1000s_for_regression$win_rate <- (total_1000s_for_regression$outdegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression$loss_rate <- (total_1000s_for_regression$indegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression <- total_1000s_for_regression %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - loss_rate - win_rate - totdegree - \n    indegree - outdegree - dc - eigen.dc, family = binomial, \n    data = total_1000s_for_regression)\n\nCoefficients:\n(Intercept)     Catholic        Islam     Orthodox     Buddhist  \n   -15.1948      13.9008      12.7531      14.6893      15.0858  \n      Pagan     Tengrism       Shinto        Hindu    Shamanism  \n     0.9610      11.6691      16.0623       9.1358      -0.1497  \n      eigen        close           rc     eigen.rc  \n   -82.1100     256.5294      -3.3322     -17.3152  \n\nDegrees of Freedom: 109 Total (i.e. Null);  96 Residual\nNull Deviance:      152.3 \nResidual Deviance: 58.4     AIC: 86.4\n\n\n\nset.seed(6738)\n\nin_training<- sample(1:nrow(total_1000s_for_regression),  nrow(total_1000s_for_regression) * 0.7 )\n\ntraining_1000s <- total_1000s_for_regression[in_training,]\n\ntest_1000s <- total_1000s_for_regression[-in_training,]\n\nlm_1000s_binom_subset_1 <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial, subset = in_training )\n\nlogsitic_1_1000s_prob <- predict(lm_1000s_binom_subset_1, test_1000s,\ntype = \"response\")\n\nlog_preds_1<-ifelse(logsitic_1_1000s_prob >= 0.5, 1, 0)\n\nprediction_1_logs <-mean(log_preds_1 == test_1000s$more_win_or_loss)\n\nprediction_1_logs %>% kable()\n\n\nx\n0.9090909\n\n\n\nlibrary(glmnet)\nlibrary(MASS)\n\n\n\n\n\nset.seed(246)\n\nx_ridge <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_ridge <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)\n\ndim(coef(ridge.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\ntrain_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_ridge <- (-train_ridge)\n\ny.test_ridge <- y_ridge[test_ridge]\n\n\n\n\n\nset.seed(9292)\n\nridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 0, lambda = grid, thresh = 1e-12)\n\nridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.2416376\n\n\n\nset.seed(231)\nridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], \n                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])\n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\", \n        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:14, ]\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.21024033  0.21827317 -0.01160454  0.21312966  0.35601806 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.08955257  0.14069809  0.38278477 -0.07034364 -0.01038790 \n      eigen       close          rc    eigen.rc \n-4.61480591 12.51011844 -0.29977861  4.64835194 \n\n\n\nset.seed(9292)\n\ncv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) \n\nplot(cv.out)\n\n\n\n\n\n\nset.seed(9292)\n\nbestlam <- cv.out$lambda.min\n\nbestlam\n\n\n[1] 0.415338\n\n\n\nset.seed(9292)\n\nridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.174632\n\n\n\nset.seed(2897)\n\nx_lasso <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_lasso <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nlasso.mod <- glmnet(x_lasso, y_lasso, alpha = 0, lambda = grid)\n\ndim(coef(lasso.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\n\ntrain_lasso <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_lasso <- (-train_lasso)\n\ny.test_lasso <- y_lasso[test_lasso]\n\n\n\n\n\nset.seed(9292)\n\nlasso.mod <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n                    alpha = 1, lambda = grid)\n\nplot(lasso.mod)\n\n\n\n\n\n\nset.seed(1029)\n\ncv.out_2 <- cv.glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], alpha = 1) \n\nplot(cv.out_2)\n\n\n\n\n\n\nset.seed(1920)\n\nbestlam_2 <- cv.out_2$lambda.min\n\nlasso.pred <- predict(cv.out_2, s = bestlam_2, newx = x_ridge[test_ridge,])\n\nmean((lasso.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.1749583\n\n\n\nset.seed(2739)\n\nout <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n              alpha = 1, lambda = grid)\n\nlasso.coef <- predict(out, type = \"coefficients\", s = bestlam_2)[1:14, ]\n\nlasso.coef\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.42561685  0.05577020 -0.09275344  0.00000000  0.00000000 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 \n      eigen       close          rc    eigen.rc \n 0.00000000  3.22570629 -0.21240622  0.00000000 \n\n\n\naspects_of_1100s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1100s_states.xlsx\")\n\ntotal_1100s <- merge(aspects_of_1100s_states, wars_in_1100s.nodes.stat_2, by=\"name\")\n\n\n\n\n\naspects_of_1200s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1200s_states.xlsx\")\n\ntotal_1200s <- merge(aspects_of_1200s_states, wars_in_1200s.nodes.stat_2, by=\"name\")\n\n\n\n\n\ntotal_1000s_for_PCA <- total_1000s_brokerag_reg_binom[-c(20:27)]\n\napply(total_1000s_for_PCA[-1], 2, mean)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\napply(total_1000s_for_PCA[-1], 2, var)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n    0.2502085071     0.1501251043     0.1318598832     0.0601334445 \n           Pagan         Tengrism           Shinto            Hindu \n    0.0353628023     0.0180150125     0.0520433695     0.0437864887 \n       Shamanism        totdegree         indegree        outdegree \n    0.0090909091     8.9208507089     2.6656380317     6.3189324437 \n           eigen            close               rc         eigen.rc \n    0.0076304265     0.0019575460     0.1260782284     0.0004728954 \n              dc         eigen.dc more_win_or_loss \n    0.1260782284     0.0056490031     0.2519599666 \n\n\n\npr.out <- prcomp(total_1000s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out$center\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\npr.out$scale\n\n\n        Catholic            Islam         Orthodox         Buddhist \n      0.50020846       0.38745981       0.36312516       0.24522122 \n           Pagan         Tengrism           Shinto            Hindu \n      0.18805000       0.13422002       0.22813016       0.20925221 \n       Shamanism        totdegree         indegree        outdegree \n      0.09534626       2.98677932       1.63267818       2.51374868 \n           eigen            close               rc         eigen.rc \n      0.08735231       0.04424416       0.35507496       0.02174616 \n              dc         eigen.dc more_win_or_loss \n      0.35507496       0.07515985       0.50195614 \n\n\n\n\n\n\nggbiplot(pr.out, labels =  total_1000s_for_PCA$name, labels.size  =1.5)\n\n\n\n\n\n\npr.out$rotation = -pr.out$rotation \n\npr.out$x = -pr.out$x\n\nggbiplot(pr.out, labels =  total_1000s_for_PCA$name, labels.size  =1.5)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var <- pr.out$sdev^2\n\npr.var\n\n\n [1] 4.917311e+00 2.827605e+00 1.535720e+00 1.467004e+00 1.136318e+00\n [6] 1.076804e+00 1.059884e+00 1.022359e+00 1.010879e+00 9.053146e-01\n[11] 7.829594e-01 6.056623e-01 3.797690e-01 1.959146e-01 6.458828e-02\n[16] 1.190694e-02 5.771849e-31 3.917271e-31 4.729037e-32\n\n\n\npve <- pr.var / sum(pr.var)\n\npve\n\n\n [1] 2.588059e-01 1.488213e-01 8.082739e-02 7.721075e-02 5.980622e-02\n [6] 5.667390e-02 5.578337e-02 5.380835e-02 5.320417e-02 4.764814e-02\n[11] 4.120839e-02 3.187696e-02 1.998784e-02 1.031129e-02 3.399383e-03\n[16] 6.266808e-04 3.037815e-32 2.061722e-32 2.488967e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\nnames(total_1200s)\n\n\n [1] \"name\"         \"Catholic\"     \"Islam\"        \"Orthodox\"    \n [5] \"Buddhist\"     \"Pagan\"        \"Tengrism\"     \"Shinto\"      \n [9] \"Hindu\"        \"Shamanism\"    \"totdegree\"    \"indegree\"    \n[13] \"outdegree\"    \"eigen\"        \"rc\"           \"eigen.rc\"    \n[17] \"dc\"           \"eigen.dc\"     \"broker.tot\"   \"broker.coord\"\n[21] \"broker.itin\"  \"broker.rep\"   \"broker.gate\"  \"broker.lia\"  \n\n\n\ntotal_1200s_brokerag_reg<-total_1200s\n\n\n\n\n\ntotal_1200s_brokerag_reg$win_rate <- (total_1200s_brokerag_reg$outdegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg$loss_rate <- (total_1200s_brokerag_reg$indegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg_binom <- total_1200s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\n\n\n\n\ntotal_1200s_for_PCA <- total_1200s_brokerag_reg_binom[-c(20:27)]\n\n\napply(total_1200s_for_PCA[-1], 2, mean)\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism      Shinto       Hindu   Shamanism   totdegree \n0.025000000 0.000000000 0.006250000 0.000000000 3.918750000 \n   indegree   outdegree       eigen          rc    eigen.rc \n1.962500000 1.956250000 0.025567955 0.158754617 0.002192746 \n         dc    eigen.dc  broker.tot \n0.841245383 0.023375209 0.341581810 \n\n\n\napply(total_1200s_for_PCA[-1], 2, var)\n\n\n    Catholic        Islam     Orthodox     Buddhist        Pagan \n2.061321e-01 6.442610e-02 8.034591e-02 8.034591e-02 1.242138e-02 \n    Tengrism       Shinto        Hindu    Shamanism    totdegree \n2.452830e-02 0.000000e+00 6.250000e-03 0.000000e+00 2.666631e+01 \n    indegree    outdegree        eigen           rc     eigen.rc \n6.237579e+00 1.595405e+01 5.631476e-03 7.141295e-02 7.316162e-05 \n          dc     eigen.dc   broker.tot \n7.141295e-02 4.574350e-03 3.001236e+01 \n\n\n\n# I cannot scale variables with \n\ntotal_1200s_for_PCA<-total_1200s_for_PCA[-c(8,10)]\n\n\n\n\n\npr.out_2 <- prcomp(total_1200s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out_2)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out_2$center\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.025000000 0.006250000 3.918750000 1.962500000 1.956250000 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.025567955 0.158754617 0.002192746 0.841245383 0.023375209 \n broker.tot \n0.341581810 \n\n\n\npr.out_2$scale\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.454017704 0.253822971 0.283453545 0.283453545 0.111451261 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.156615139 0.079056942 5.163943541 2.497514488 3.994251963 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.075043164 0.267232010 0.008553457 0.267232010 0.067633938 \n broker.tot \n5.478353760 \n\n\n\n\n\n\nggbiplot(pr.out_2, labels =  total_1200s_for_PCA$name, labels.size  =1.5)\n\n\n\n#biplot(pr.out_2, scale = 0)\n\n\n\n\n\npr.out_2$rotation = -pr.out_2$rotation \n\npr.out_2$x = -pr.out_2$x\n\nggbiplot(pr.out_2, labels =  total_1200s_for_PCA$name, labels.size  =1.5)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var_2 <- pr.out_2$sdev^2\n\npr.var_2\n\n\n [1] 4.903737e+00 2.344663e+00 1.670548e+00 1.250176e+00 1.132904e+00\n [6] 1.097802e+00 1.011326e+00 9.460639e-01 8.661454e-01 5.139677e-01\n[11] 1.659928e-01 9.667541e-02 2.916516e-30 4.832251e-31 2.292490e-31\n[16] 1.889562e-32\n\n\n\npve_2 <- pr.var_2 / sum(pr.var_2)\n\npve_2\n\n\n [1] 3.064835e-01 1.465414e-01 1.044092e-01 7.813602e-02 7.080651e-02\n [6] 6.861260e-02 6.320785e-02 5.912899e-02 5.413409e-02 3.212298e-02\n[11] 1.037455e-02 6.042213e-03 1.822822e-31 3.020157e-32 1.432806e-32\n[16] 1.180977e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve_2, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve_2), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n(information regarding the meaning of each type of brokerage was\nacquired from https://edis.ifas.ufl.edu/publication/WC197)\n\n\n\n",
    "preview": "posts/2022-03-21-blog-post-7/images/silly_cat.png",
    "last_modified": "2022-03-28T12:25:21-04:00",
    "input_file": "blog-post-7.knit.md",
    "preview_width": 1022,
    "preview_height": 1064
  },
  {
    "path": "posts/2022-03-07-into-the-20th-century-conflict-data/",
    "title": "Into the 20th Century (Conflict Data) Homework 6",
    "description": "In this post I begin my analysis of the 20th century conflicts dataset.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-07",
    "categories": [],
    "contents": "\n\n\n\nLoading and Summarizing Data\nLoading: Similarly to the datasets I used for the\nprior assignments, this one will involve the use of a conflict dataset.\nGiven the relative success of the analysis techniques on dyadic conflict\nin the 9th, 10th, and 11th centuries I decided to try it on a much more\nrecent period, that being the height of the cold war beginning in 1945\nand officially ending with the dissolution of the Soviet Union in 1992,\nhowever the dataset in question ends in 1989.\n\n\n\n\n\n\n\n\n\nInterpretation and Inital\nAnalysis\nInitial Interpretation: After briefly cleaning the\ndataset to remove excess spaces resulting in nations, or factions being\ncounted twice, I then converted my data in 4 different kinds of network\nobjects, being a matrix, edgelist graph, igraph, and network. A ggplot\nobject of the initial network can be seen below.\n\n\n\nContinued Visualization\nVisuals Though not necessarily practical for\nunderstanding the nature of the network the two visualization below are\nused to exhibit the increased complexity of this dataset relative to\nprior ones, as in previous models of conflict there were under 200 nodes\n(or factions) being visualized, as will be illustrated in subsequent\nanalysis there were about 600 nations, rebel groups, factions, and\nwarring parties considered direct belligerents in this dataset. It is\nalso important to note that this network does not include direct\nmilitary support that did did not constitute direct\ninvolvement, this will later be included as a grouping dummy variable,\nbut in its current form military support that is not directly\nintervention will not be considered.\n\n\nset.seed(2)\n\nggraph(Wars_in_latter_half_of_20th_network, 'dendrogram', circular = TRUE) + \n    geom_edge_elbow() + \n    coord_fixed() +\n    geom_edge_link0(edge_alpha = 0.001)+\n    geom_node_text(aes(label = name), size=1, repel=FALSE)\n\n\n\n\n\n\nggraph(Wars_in_latter_half_of_20th_network, layout = \"treemap\") + \n  geom_node_tile(aes(fill = depth))+    \n  geom_node_text(aes(label = name), size=1, repel=FALSE)\n\n\n\n\ninterpret the data, identifying at least two results of interest.\nQuestions you may want to consider include (but are not limited to) the\nfollowing. Calculate structural equivalence models for your network\ndata, and use clustering and blockmodeling to identify nodes in similar\npositions. Can you find any patterns in the data, and do the blocks\n“make sense.” What types of behavior would we expect to see (or do we\nsee) on the basis of equivalence and block assignment? Do different\nclustering methods and/or the use of weights make a difference? How much\ninsight can you get from plotting the block role assignments. You may\nalso want to see if nodes that are equivalent (and/or belong to same\nblock) are similar on measures of centrality introduced in earlier\nweeks.\nInital Analysis and\nInterpretation\nNetwork Summary: As can be seen by the network print\nbelow the conflict dataset comprises 599 nodes, factions, or\nbelligerents that are direct combatants in each\nconflict, these are directed with an in-degree being a lost war, and an\nout-degree being a won war. There are loops as many factions fight each\nother multiple times and amongst different sets of allies losing some\nwars and winning others.\n\n Network attributes:\n  vertices = 599 \n  directed = TRUE \n  hyper = FALSE \n  loops = TRUE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 1090 \n    missing edges= 0 \n    non-missing edges= 1090 \n\n Vertex attribute names: \n    vertex.names \n\n Edge attribute names not shown \n\n\n\n\n\n\ntemp<-data.frame(brokerage(Wars_in_latter_half_of_20th_network, cl = Wars_in_latter_half_of_20th_network.nodes.stat$totdegree)$z.nli)\n\nWars_in_latter_half_of_20th_network.nodes.stat_2 <- Wars_in_latter_half_of_20th_network.nodes.stat %>%\n  mutate(broker.tot = temp$t,\n         broker.coord = temp$w_I,\n         broker.itin = temp$w_O,\n         broker.rep = temp$b_IO,\n         broker.gate = temp$b_OI,\n         broker.lia = temp$b_O)\n\n\n\nBrokerage and Network\nAttributes\nBrokerage Scores: In this case brokerage scores are\ncalculated using the function brokerage() and appended to the data frame\nwith the these measures\nTotal Degree\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(totdegree))%>%\n  slice(1:10))[,c(1,2)] %>%kable()\n\n\nname\ntotdegree\nUnited States\n63\nIsrael\n47\nChina\n47\nSoviet Union\n39\nFrance\n38\nPhilippines\n35\nIndia\n34\nUnited Kingdom\n31\nNorth Korea\n30\nThailand\n27\n\nIn-Degree\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(indegree))%>%\n  slice(1:10))[,c(1,3)] %>%kable()\n\n\nname\nindegree\nSoviet Union\n25\nUnited States\n23\nNorth Korea\n21\nChina\n20\nSyria\n17\nCuba\n16\nSouth Vietnam\n15\nSLA\n13\nIsreal\n13\nLebanese Front\n13\n\nOut-Degree\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(outdegree))%>%\n  slice(1:10))[,c(1,4)] %>%kable()\n\n\nname\noutdegree\nIsrael\n47\nUnited States\n40\nIndia\n32\nFrance\n31\nChina\n27\nPhilippines\n27\nItaly\n26\nUnited Kingdom\n20\nIran\n19\nNorth Vietnam\n19\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(broker.coord))%>%\n  slice(0:5))[,c(1,12)] %>%kable()\n\n\nname\nbroker.coord\nKhmer Issarak\n-0.0042969\nSouth Africa\n-0.0042969\nKhmer Rouge\n-0.0042969\nPLO\n-0.0042969\nKhmer Republic\n-0.0042969\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(broker.itin))%>%\n  slice(0:5))[,c(1,13)] %>%kable()\n\n\nname\nbroker.itin\nUnited States\n30.978006\nEthiopia\n20.451964\nChina\n13.215216\nFrance\n11.241500\nUnited Kingdom\n7.294199\n\n\nname\nbroker.rep\nPKK\n6.358704\nSouth Yemen\n3.288637\nIran\n-0.044345\nIsrael\n-0.044345\nAustralia\n-0.044345\n\n\nname\nbroker.gate\nMorocco\n9.0531209\nPUK\n4.1844336\nArab Socialist Ba’ath Party\n0.7091118\nSenegal\n0.7091118\nPapua New Guinea\n0.5196169\n\n\nname\nbroker.lia\nUnited States\n286.23524\nChina\n167.99725\nSoviet Union\n110.36960\nPhilippines\n69.88732\nUnited Kingdom\n67.23274\n\n\n\n\n\n\n\n\n\n\nExamining Centrality\n\n\nsna::dyad.census(Wars_in_latter_half_of_20th_network)\n\n\n     Mut Asym   Null\n[1,] 115  859 178127\n\nsna::triad.census(Wars_in_latter_half_of_20th_network)\n\n\n          003    012   102 021D 021U 021C 111D 111U 030T 030C 201\n[1,] 35068530 545959 17760 3817 1904 1822  526  639   46    0  83\n     120D 120U 120C 210 300\n[1,]    0    3   10   0   0\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(eigen))%>%\n  slice(0:5))[,c(1,5)] %>%kable()\n\n\nname\neigen\nChina\n0.3435636\nNorth Vietnam\n0.2973257\nPathet Lao\n0.2836275\nKhmer Rouge\n0.2223014\nKhmer Issarak\n0.2153606\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(close))%>%\n  slice(0:5))[,c(1,6)] %>%kable()\n\n\nname\nclose\nIsrael\n0.1664994\nUnited States\n0.1536232\nChina\n0.1497492\nFrance\n0.1449833\nPathet Lao\n0.1380435\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(eigen.rc))%>%\n  slice(0:5))[,c(1,8)] %>%kable()\n\n\nname\neigen.rc\nEthiopia\n0.0801873\nFrance\n0.0627097\nSouth Africa\n0.0451530\nUnited Kingdom\n0.0303952\nUnited States\n0.0294664\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(eigen.dc))%>%\n  slice(0:5))[,c(1,10)] %>%kable()\n\n\nname\neigen.dc\nChina\n0.3166654\nNorth Vietnam\n0.2973257\nPathet Lao\n0.2836275\nKhmer Rouge\n0.2223014\nKhmer Issarak\n0.2153606\n\n\n\nWars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  select(-name) %>% \n  gather() %>% \n  ggplot(aes(value)) +\n    geom_histogram() +\n    facet_wrap(~key, scales = \"free\")\n\n\n\n\n\n\nwars_correlation_latter_half<-Wars_in_latter_half_of_20th_network.nodes.stat_2 %>% \n  select(totdegree,indegree,outdegree,eigen,eigen.rc,eigen.dc)%>%\n  correlate() \nfashion(wars_correlation_latter_half)\n\n\n       term totdegree indegree outdegree eigen eigen.rc eigen.dc\n1 totdegree                .71       .87   .64      .55      .60\n2  indegree       .71                .28   .34      .45      .30\n3 outdegree       .87      .28             .64      .45      .62\n4     eigen       .64      .34       .64            .53      .99\n5  eigen.rc       .55      .45       .45   .53               .42\n6  eigen.dc       .60      .30       .62   .99      .42         \n\n\n\nrplot(wars_correlation_latter_half)\n\n\n\n\n\n\nlibrary(threejs)\nlibrary(htmlwidgets)\nlibrary(igraph)\n\n\n\n\n\n#net.js <- Wars_in_latter_half_of_20th.ig\n#graph_attr(net.js, \"layout\") <- NULL \n\n\n\n\n\n#gjs <- graphjs(net.js, main=\"Cold War Interactive Network\", bg=\"gray10\", #vertex.size=0.5, showLabels=T, vertex.label = V(net.js)$name, stroke=F, curvature=0.1, #attraction=0.9, repulsion=0.7, opacity=0.9)\n#print(gjs)\n#saveWidget(gjs, file=\"Media-Network-gjs.html\")\n#browseURL(\"Media-Network-gjs.html\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-07-into-the-20th-century-conflict-data/into-the-20th-century-conflict-data_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-08T11:42:15-05:00",
    "input_file": "into-the-20th-century-conflict-data.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-05-an-attempt-to-scrape-wikipedia/",
    "title": "An Attempt to Scrape Wikipedia",
    "description": "This Post Involves my Attempt to Scrape and Clean the Output of a Wikipedia article to get the Same Data I had to Make by Hand.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-05",
    "categories": [],
    "contents": "\n\n\nlibrary(textreadr)\nlibrary(tidyr)\nlibrary(rvest)\nlibrary(aRtsy)\n\n\n\n\n\n\n\n\nlist_of_wars_1000 <- \"https://en.wikipedia.org/wiki/List_of_wars:_1000%E2%80%931499\"\n\nwars_1000s_df <- read_html(list_of_wars_1000) \n\nwars_1000s<- wars_1000s_df %>% \n  html_nodes(\"table\") %>% `[[`(2) %>% html_table()\n\n\n\n\n\nwars_1000s_subset <- wars_1000s[,1:5]\nwars_1000s_subset\n\n\n# A tibble: 58 × 5\n   Start  Finish `Name of conflict`          Belligerents Belligerents\n   <chr>  <chr>  <chr>                       <chr>        <chr>       \n 1 Start  Finish Name of conflict            Victorious … \"Defeated p…\n 2 1000   1139   Norman conquest of souther… County of A… \"Principali…\n 3 1001   1001   Battle of Peshawar (1001)   Ghaznavids   \"Kabul Shah…\n 4 1002   1018   German–Polish War           Kingdom of … \"Holy Roman…\n 5 1008   1008   Hungarian–Ahtum War         Kingdom of … \"Voivodeshi…\n 6 1008   1008   Battle of Chach             Ghaznavids   \"Kabul Shah…\n 7 1007–8 1007–8 Battle at Herdaler          Kingdom of … \"Finnish tr…\n 8 1009   1031   Fitna of al-Andalus         Hammudid dy… \"Caliphate …\n 9 1010   1011   Second conflict in the Gor… Liao dynasty \"Goryeo\"    \n10 1014   1014   Battle of Clontarf          High King o… \"LeinsterDu…\n# … with 48 more rows\n\n\n\nwars_1100s<- wars_1000s_df %>% \n  html_nodes(\"table\") %>% `[[`(3) %>% html_table()\n\n\nwars_1100s_subset <- wars_1100s[,1:5]\nwars_1100s_subset\n\n\n# A tibble: 39 × 5\n   Start          Finish    `Name of Confl…` Belligerents Belligerents\n   <chr>          <chr>     <chr>            <chr>        <chr>       \n 1 Start          Finish    Name of Conflict \"Victorious… \"Defeated p…\n 2 Summer of 1101 Summer o… Crusade of 1101… \"Sultanate … \"Crusaders\\…\n 3 1101           1101      Battle of Ramla… \"Kingdom of… \"Fatimid Ca…\n 4 1102           1102      Battle of Ramla… \"Fatimids o… \"Kingdom of…\n 5 1107           1110      Norwegian Crusa… \"Kingdom of… \"Muslim Kin…\n 6 1110           1110      Chola invasion … \"Chola Empi… \"Kalinga\"   \n 7 1113           1115      1113–15 Baleari… \"Republic o… \"Taifa of M…\n 8 1122           1124      Venetian Crusad… \"Republic o… \"Fatimid Ca…\n 9 1107           1119      Muhammad Tapar'… \"Seljuq Emp… \"Nizari Ism…\n10 1123           1123      Kalmare ledungP… \"Kingdom of… \"Swedish Pa…\n# … with 29 more rows\n\n\n\nwars_1100s<- wars_1000s_df %>% \n  html_nodes(\"table\") %>% `[[`(3) %>% html_table()\n\n\nwars_1100s_subset <- wars_1100s[,1:5]\nwars_1100s_subset\n\n\n# A tibble: 39 × 5\n   Start          Finish    `Name of Confl…` Belligerents Belligerents\n   <chr>          <chr>     <chr>            <chr>        <chr>       \n 1 Start          Finish    Name of Conflict \"Victorious… \"Defeated p…\n 2 Summer of 1101 Summer o… Crusade of 1101… \"Sultanate … \"Crusaders\\…\n 3 1101           1101      Battle of Ramla… \"Kingdom of… \"Fatimid Ca…\n 4 1102           1102      Battle of Ramla… \"Fatimids o… \"Kingdom of…\n 5 1107           1110      Norwegian Crusa… \"Kingdom of… \"Muslim Kin…\n 6 1110           1110      Chola invasion … \"Chola Empi… \"Kalinga\"   \n 7 1113           1115      1113–15 Baleari… \"Republic o… \"Taifa of M…\n 8 1122           1124      Venetian Crusad… \"Republic o… \"Fatimid Ca…\n 9 1107           1119      Muhammad Tapar'… \"Seljuq Emp… \"Nizari Ism…\n10 1123           1123      Kalmare ledungP… \"Kingdom of… \"Swedish Pa…\n# … with 29 more rows\n\n\n\nwars_1200s<- wars_1000s_df %>% \n  html_nodes(\"table\") %>% `[[`(4) %>% html_table()\n\n\nwars_1200s_subset <- wars_1200s[,1:5]\nwars_1200s_subset\n\n\n# A tibble: 83 × 5\n   Start Finish `Name of Conflict`           Belligerents Belligerents\n   <chr> <chr>  <chr>                        <chr>        <chr>       \n 1 Start Finish Name of Conflict             \"Victorious… \"Defeated p…\n 2 1201  1219   War of the Antiochene Succe… \"Forces of … \"Forces of …\n 3 1202  1204   Fourth CrusadePart of the C… \"Holy Roman… \"Byzantine …\n 4 1204  1206   Intervention in Chaldia      \"Kingdom of… \"Byzantine …\n 5 1202  1204   Anglo-Norman War (1202–04)   \"Kingdom of… \"Kingdom of…\n 6 1202  1214   Anglo-French War of 1202–12… \"Kingdom of… \"Kingdom of…\n 7 1203  1206   Loon War                     \"Holland Ki… \"Loon Franc…\n 8 1204  1261   Bulgarian–Latin wars         \"Bulgarian … \"Latin Empi…\n 9 1206  1337   Mongol invasions and conque… \"Mongol Emp… \"西夏 Weste…\n10 1208  1209   Lombard Rebellion            \"Latin Empi… \"Rebel baro…\n# … with 73 more rows\n\n\n\nwars_1300s<- wars_1000s_df %>% \n  html_nodes(\"table\") %>% `[[`(5) %>% html_table()\n\n\nwars_1300s_subset <- wars_1300s[,1:5]\nwars_1300s_subset\n\n\n# A tibble: 79 × 5\n   Start           Finish   `Name of Confl…` Belligerents Belligerents\n   <chr>           <chr>    <chr>            <chr>        <chr>       \n 1 Start           Finish   Name of Conflict Victorious … Defeated pa…\n 2 1300            1301     Second Mongol i… Myinsaing K… Yuan dynasty\n 3 1300            1300     Lembu Sora rebe… Majapahit E… Lembu Sora …\n 4 c. 14th century c. 14th… K'aissape–Hvals… Inuit under… Norsemen un…\n 5 1303            1303     Conquest of Syl… Independent… Gaur Kingdo…\n 6 1308            1308     Teutonic takeov… Teutonic Kn… Margraviate…\n 7 1309            1309     Crusade of the … Duchy of Br… Crusaders   \n 8 1311            1312     Rebellion of ma… Władysław I… Kraków      \n 9 1311            1318     Delhi–Seuna War  Delhi Sulta… Seuna Empire\n10 1314            1318     Esen Buqa–Ayurb… Yuan dynast… Chagatai Kh…\n# … with 69 more rows\n\n\n\nwars_1400s<- wars_1000s_df %>% \n  html_nodes(\"table\") %>% `[[`(6) %>% html_table()\n\n\nwars_1400s_subset <- wars_1400s[,1:5]\nwars_1400s_subset\n\n\n# A tibble: 123 × 5\n   Start Finish `Name of Conflict`           Belligerents Belligerents\n   <chr> <chr>  <chr>                        <chr>        <chr>       \n 1 Start Finish Name of Conflict             Victorious … \"Defeated p…\n 2 1400  1400   English invasion of Scotland Kingdom of … \"Kingdom of…\n 3 1400  1420   Glyndŵr Rising               Kingdom of … \"Welsh rebe…\n 4 1401  1404   First Samogitian Uprising    Teutonic St… \"Grand Duch…\n 5 1402  1402   Battle of Ankara             Timurid Emp… \"Ottoman Em…\n 6 1402  1413   Ottoman Interregnum          Faction of … \"Faction of…\n 7 1402  1496   Conquest of the Canary Isla… Union of Ca… \"Guanches\"  \n 8 1403  1403   Percy Rebellion              Kingdom of … \"English re…\n 9 1404  1406   Paregreg war                 Western cou… \"Eastern co…\n10 1405  1405   Scrope Rebellion             Kingdom of … \"English re…\n# … with 113 more rows\n\n\n\nlist_of_wars_1500 <- \"https://en.wikipedia.org/wiki/List_of_wars:_1500%E2%80%931799\"\n\nwars_1500s_df <- read_html(list_of_wars_1500) \n\nwars_1500s <- wars_1500s_df %>% \n  html_nodes(\"table\") %>% `[[`(1) %>% html_table()\n\nwars_1500s_subset <- wars_1500s[,1:5]\n\nwars_1500s_subset\n\n\n# A tibble: 161 × 5\n   Start Finish `Name of conflict`           Belligerents Belligerents\n   <chr> <chr>  <chr>                        <chr>        <chr>       \n 1 Start Finish \"Name of conflict\"           Victorious … \"Defeated p…\n 2 1500  1503   \"Second Muscovite–Lithuania… Grand Duchy… \"Grand Duch…\n 3 1500  1500   \"Battle of Hemmingstedt\"     Peasantry o… \"Kalmar Uni…\n 4 1501  1512   \"Dano-Swedish War (1501–151… Sweden Free… \"Kalmar Uni…\n 5 1502  1510   \"Persian–Uzbek wars\"         Persian Emp… \"Shaybanid …\n 6 1502  1543   \"Guelders Wars\"              Holy Roman … \"Duchy of G…\n 7 1503  1505   \"War of the Succession of L… Duchy of Ba… \"Duchy of B…\n 8 1505  1517   \"Portuguese–Mamluk naval wa… Portugal     \"Mamluk Sul…\n 9 1507  1508   \"Third Muscovite–Lithuanian… Grand Duchy… \"Grand Duch…\n10 1508  1516   \"War of the League of Cambr… 1508–10: Pa… \"1508–10: V…\n# … with 151 more rows\n\n\n\n\n",
    "preview": "posts/2022-03-05-an-attempt-to-scrape-wikipedia/an-attempt-to-scrape-wikipedia_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-03-08T08:46:40-05:00",
    "input_file": "an-attempt-to-scrape-wikipedia.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-03-03-text-as-data-blog-post-2/",
    "title": "Text as Data Blog Post 2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-03",
    "categories": [],
    "contents": "\n\n\n\nBlog Post 2:\nQuestion: For my second blog post I will be\nobserving the content of the sub-reddit I am scraping, extracting\ninformation, and performing initial natural language processing and text\nanalysis on it.\nNavigate and describe the characteristics of the data source of your\ninterest, if you’ve specified one (or plan B). The characteristics\ninclude (1) its ‘content’ and/or (2) how it can be scraped. The\ncharacteristics include\nits ‘content’ and/or (2) how it can be scraped. I Summon up your\nknowledge of some useful packages we’ve reviewed\nand/or NLP tools in relation to your research project.\nI Summon up your knowledge of some useful packages we’ve reviewed\nand/or NLP tools in relation to your research project. Sorting out\nadjectives? Extracting major verbs or named entities? …\nInital Loading and\nProcessing\nExplanation: My code below illustrates how I\ninitially got my information from reddit by scraping. In this case I\nused RedditExtractoR. The author of this packages describes it as a\nminimalist r wrapper it scrapes a limited number of posts from reddit.\nThe api on reddit itself only allows 60 requests per minute so I will\nhave to extract the next set of data later and pick a period before my\nfirst post occured.\n\n\n#top_guns_urls <- find_thread_urls(subreddit=\"guns\", sort_by=\"top\")\n\nload(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/df_guns.RData\")\n\nstr(top_guns_urls)\n\ntop_guns_urls_df=top_guns_urls[,c(\"title\", \"date_utc\", \"comments\")]\n\n#guns_contents <- get_thread_content(top_guns_urls_df$url[1:1000])\n#str(guns_contents$threads)\n\n\n\nConversion From Data Frame\nto Corpus\nExplanation: Below I have processed my inital\ndataframe from reddit into a corpus.\n\n\ntop_guns_urls_df=top_guns_urls[,c(\"title\", \"date_utc\", \"comments\")]\n\ntop_guns_corpus<-corpus(top_guns_urls_df$title)\n\ntop_guns_documents<-top_guns_corpus[1:10,]\n\ntop_guns_corpus_summary <- summary(top_guns_corpus)\n\n\n\nPre-processing\nExplanation: Next I used the\nfactorial_preprocessing() command to both use n-grams processing and use\nan infrequent term threshold.\n\n\npreprocessed_documents <- factorial_preprocessing(\n    top_guns_corpus,\n    use_ngrams = TRUE,\n    infrequent_term_threshold = 0.2,\n    verbose = FALSE)\n\n\nPreprocessing 998 documents 128 different ways...\n\n\n\nnames(preprocessed_documents)\n\n\n[1] \"choices\"  \"dfm_list\" \"labels\"  \n\n\n\nhead(preprocessed_documents$choices)\n\n\n              removePunctuation removeNumbers lowercase stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE TRUE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\n\npreText Results\nExplanation: Next I used preText() to pre-process\nthe documents that I have so far to acquire pre-text scores that can\ngive me a sense of what techniques may be necessary for natural language\nprocessing as the project develops.\n\n\n#preText_results <- preText(\n#    preprocessed_documents,\n#    dataset_name = \"Gun Pretext Results\",\n#    distance_method = \"cosine\",\n#    num_comparisons = 50,\n#    verbose = TRUE)\n\n\n\nExplanation Continued: Next we look at the pre-text\nscores with 50 comparisons, which was acquired from the code above.\nBelow these are graphed with intercept.\n\n\nload(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData\")\n\npreText_score_plot(preText_results)\n\n\n\n\nThe lowest score with intercepts according to the graph is N-3 which\nremoves numbers, and uses n-grams. This plot represents the potential\nrisk of using more complex pre-processing at the pre-text score goes up\nbut may remove more information. The highest score belongs to L-S which\nis lowercased and stemmed which is quite risky to do.\nExplanation Continued: Looking at the regression\ncoefficients we see negative scores as usual results and positive\ncoefficients as unusual ones. In this case removing puncuation,\nstopwords, and n-grams would not lead to a great deal of\nabnormalilty.\n\n\nregression_coefficient_plot(preText_results,\n                            remove_intercept = TRUE)\n\n\n\n\nTokens and Corpus Work\n\n\ntop_guns_tokens <- tokens(top_guns_corpus)\n\nprint(top_guns_tokens)\n\n\nTokens consisting of 998 documents.\ntext1 :\n[1] \"Smith\"    \"and\"      \"Wesson\"   \"Saturday\" \"anyone\"   \"?\"       \n\ntext2 :\n[1] \"My\"       \"two\"      \"favorite\" \"9mm\"      \"s\"       \n\ntext3 :\n [1] \"My\"       \"Arex\"     \"Zero\"     \"1\"        \"Tactical\" \"w\"       \n [7] \"/\"        \"a\"        \"Trijicon\" \"RMR\"      \",\"        \"Viridian\"\n[ ... and 8 more ]\n\ntext4 :\n [1] \"Opinion\" \"of\"      \"the\"     \"fnx9\"    \"?\"       \"Already\"\n [7] \"have\"    \"an\"      \"xd\"      \"and\"     \"a\"       \"p320\"   \n[ ... and 10 more ]\n\ntext5 :\n[1] \"Howa\" \"1500\" \"in\"   \".\"    \"308\" \n\ntext6 :\n[1] \"A\"          \"little\"     \"before\"     \"and\"        \"after\"     \n[6] \"comparison\" \".\"         \n\n[ reached max_ndoc ... 992 more documents ]\n\ntop_guns_tokens_no_punct <- tokens(top_guns_corpus, \n    remove_punct = T)\n\nprint(top_guns_tokens_no_punct)\n\n\nTokens consisting of 998 documents.\ntext1 :\n[1] \"Smith\"    \"and\"      \"Wesson\"   \"Saturday\" \"anyone\"  \n\ntext2 :\n[1] \"My\"       \"two\"      \"favorite\" \"9mm\"      \"s\"       \n\ntext3 :\n [1] \"My\"       \"Arex\"     \"Zero\"     \"1\"        \"Tactical\" \"w\"       \n [7] \"a\"        \"Trijicon\" \"RMR\"      \"Viridian\" \"XL5\"      \"Gen\"     \n[ ... and 5 more ]\n\ntext4 :\n [1] \"Opinion\" \"of\"      \"the\"     \"fnx9\"    \"Already\" \"have\"   \n [7] \"an\"      \"xd\"      \"and\"     \"a\"       \"p320\"    \"m18\"    \n[ ... and 6 more ]\n\ntext5 :\n[1] \"Howa\" \"1500\" \"in\"   \"308\" \n\ntext6 :\n[1] \"A\"          \"little\"     \"before\"     \"and\"        \"after\"     \n[6] \"comparison\"\n\n[ reached max_ndoc ... 992 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)\n\nprint(top_guns_tokens_no_punct_no_upper)\n\n\nTokens consisting of 998 documents.\ntext1 :\n[1] \"smith\"    \"and\"      \"wesson\"   \"saturday\" \"anyone\"  \n\ntext2 :\n[1] \"my\"       \"two\"      \"favorite\" \"9mm\"      \"s\"       \n\ntext3 :\n [1] \"my\"       \"arex\"     \"zero\"     \"1\"        \"tactical\" \"w\"       \n [7] \"a\"        \"trijicon\" \"rmr\"      \"viridian\" \"xl5\"      \"gen\"     \n[ ... and 5 more ]\n\ntext4 :\n [1] \"opinion\" \"of\"      \"the\"     \"fnx9\"    \"already\" \"have\"   \n [7] \"an\"      \"xd\"      \"and\"     \"a\"       \"p320\"    \"m18\"    \n[ ... and 6 more ]\n\ntext5 :\n[1] \"howa\" \"1500\" \"in\"   \"308\" \n\ntext6 :\n[1] \"a\"          \"little\"     \"before\"     \"and\"        \"after\"     \n[6] \"comparison\"\n\n[ reached max_ndoc ... 992 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords(\"en\"), selection = \"remove\")\n\nlength(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\n[1] 998\n\nprint(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\nTokens consisting of 998 documents.\ntext1 :\n[1] \"smith\"    \"wesson\"   \"saturday\" \"anyone\"  \n\ntext2 :\n[1] \"two\"      \"favorite\" \"9mm\"      \"s\"       \n\ntext3 :\n [1] \"arex\"     \"zero\"     \"1\"        \"tactical\" \"w\"        \"trijicon\"\n [7] \"rmr\"      \"viridian\" \"xl5\"      \"gen\"      \"3\"        \"hogue\"   \n[ ... and 2 more ]\n\ntext4 :\n [1] \"opinion\"     \"fnx9\"        \"already\"     \"xd\"         \n [5] \"p320\"        \"m18\"         \"sa\"          \"da\"         \n [9] \"interesting\" \"sure\"       \n\ntext5 :\n[1] \"howa\" \"1500\" \"308\" \n\ntext6 :\n[1] \"little\"     \"comparison\"\n\n[ reached max_ndoc ... 992 more documents ]\n\n\n\ntop_guns_corpus_tokens <- tokens(top_guns_corpus)\n\nprint(top_guns_corpus_tokens)\n\n\nTokens consisting of 998 documents.\ntext1 :\n[1] \"Smith\"    \"and\"      \"Wesson\"   \"Saturday\" \"anyone\"   \"?\"       \n\ntext2 :\n[1] \"My\"       \"two\"      \"favorite\" \"9mm\"      \"s\"       \n\ntext3 :\n [1] \"My\"       \"Arex\"     \"Zero\"     \"1\"        \"Tactical\" \"w\"       \n [7] \"/\"        \"a\"        \"Trijicon\" \"RMR\"      \",\"        \"Viridian\"\n[ ... and 8 more ]\n\ntext4 :\n [1] \"Opinion\" \"of\"      \"the\"     \"fnx9\"    \"?\"       \"Already\"\n [7] \"have\"    \"an\"      \"xd\"      \"and\"     \"a\"       \"p320\"   \n[ ... and 10 more ]\n\ntext5 :\n[1] \"Howa\" \"1500\" \"in\"   \".\"    \"308\" \n\ntext6 :\n[1] \"A\"          \"little\"     \"before\"     \"and\"        \"after\"     \n[6] \"comparison\" \".\"         \n\n[ reached max_ndoc ... 992 more documents ]\n\n\n\nhead(annotated.guns_corpus$token)\n\n\n# A tibble: 6 × 11\n  doc_id   sid tid   token    token_with_ws lemma    upos  xpos  feats\n   <int> <int> <chr> <chr>    <chr>         <chr>    <chr> <chr> <chr>\n1      1     1 1     Smith    \"Smith \"      Smith    PROPN NNP   Numb…\n2      1     1 2     and      \"and \"        and      CCONJ CC    <NA> \n3      1     1 3     Wesson   \"Wesson \"     Wesson   PROPN NNP   Numb…\n4      1     1 4     Saturday \"Saturday \"   Saturday PROPN NNP   Numb…\n5      1     1 5     anyone   \"anyone\"      anyone   PRON  NN    Numb…\n6      1     1 6     ?        \"?\"           ?        PUNCT .     <NA> \n# … with 2 more variables: tid_source <chr>, relation <chr>\n\nhead(annotated.guns_corpus$document)\n\n\n  doc_id\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n\ndoc_id_guns<-annotated.guns_corpus$document\n\ndoc_id_guns$date<-top_guns_urls_df$date_utc\n\nannoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = \"doc_id\")\n\nannoData$date<-as.Date(annoData$date)\n\n\n\n\n\nannoData %>% \n  group_by(date) %>% \n  summarize(Sentences = max(sid)) %>%\n  ggplot(aes(date, Sentences)) +\n    geom_line() +\n    geom_smooth() +\n    theme_bw()\n\n\n\n\n\n\n# calculate readability\nreadability <- textstat_readability(top_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\")) \n\n# add in a chapter number\nreadability$chapter <- c(1:nrow(readability))\n\n# plot results\nggplot(readability, aes(x = chapter, y = Flesch.Kincaid)) +\n  geom_line() + \n  geom_smooth() + \n  theme_bw()\n\n\n\n\n\n\nreadability <- textstat_readability(top_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\", \"FOG\", \"Coleman.Liau.grade\")) \n\n# add in a chapter number\n\nreadability$post <- c(1:nrow(readability))\n\n# plot results\nggplot(readability, aes(x = post)) +\n  geom_line(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_line(aes(y = FOG), color = \"red\") + \n  geom_line(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_bw()\n\n\n\n\n\n\nannoData$date<-as.Date(annoData$date)\n\nreadability$added_dates<-as.Date(top_guns_urls_df$date_utc)\n\nggplot(readability, aes(x = added_dates)) +\n  geom_smooth(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_smooth(aes(y = FOG), color = \"red\") + \n  geom_smooth(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_minimal()\n\n\n\n\n\n\ncor(readability$Flesch.Kincaid, readability$FOG, use = \"complete.obs\")\n\n\n[1] 0.8007756\n\n\n\ncor(readability$Flesch.Kincaid, readability$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.7308223\n\n\n\ncor(readability$FOG, readability$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.5551364\n\n\n\n#sentimetnsdf<-get_sentiments(\"nrc\")\n\n#write.csv(sentimetnsdf, file = \"sentimetnsdf.csv\")\n\n#save(sentimetnsdf, file=\"sentimetnsdf_2\")\n\n\n\n\n\nlibrary(readr)\nsentimetnsdf <- read_csv(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv\")\n\n\n\nSentiment Results Using NRC\n\n\ntop_guns_urls_df_2<-top_guns_urls_df\ntop_guns_urls_df_2$text<- seq(1, 998, by=1)\n\n\nnrc_joy <- sentimetnsdf %>% \n  filter(sentiment == \"joy\")\n\ntidy_posts_for_guns <- top_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\nlove\n24\nfinally\n22\nhappy\n18\nfun\n14\nfavorite\n12\nfound\n10\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\ntidy_posts_for_guns_sentiment\n\n\n# A tibble: 580 × 12\n    text trust   joy positive anger anticipation disgust  fear\n   <dbl> <int> <int>    <int> <int>        <int>   <int> <int>\n 1     1     1     0        0     0            0       0     0\n 2     2     1     1        1     0            0       0     0\n 3     4     0     0        1     0            0       0     0\n 4     9     1     0        2     0            0       0     0\n 5    11     0     0        1     2            1       1     1\n 6    12     0     0        1     0            0       0     0\n 7    14     2     2        3     0            3       0     0\n 8    15     1     1        3     0            2       0     0\n 9    16     2     1        1     0            1       0     0\n10    22     0     0        0     1            0       0     1\n# … with 570 more rows, and 4 more variables: negative <int>,\n#   sadness <int>, surprise <int>, sentiment <int>\n\nsentimetnsdf\n\n\n# A tibble: 13,875 × 3\n    ...1 word        sentiment\n   <dbl> <chr>       <chr>    \n 1     1 abacus      trust    \n 2     2 abandon     fear     \n 3     3 abandon     negative \n 4     4 abandon     sadness  \n 5     5 abandoned   anger    \n 6     6 abandoned   fear     \n 7     7 abandoned   negative \n 8     8 abandoned   sadness  \n 9     9 abandonment anger    \n10    10 abandonment fear     \n# … with 13,865 more rows\n\n\n\nnrc_guns_word_counts <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\n\n\n\n\nnrc_guns_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\nExplanation: Using nrc appears to have had some\nunintended effects that may require an analysis of the specific words\nused to describe sentiment. One difficult part of the data being used is\nthat firearms, and the words used to describe them, are percieved\nSentiment Results Using BING\n\n\nBing_sentiments<-get_sentiments(\"bing\")\n\ntidy_posts_for_guns <- top_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\nlove\n24\nfinally\n22\nhappy\n18\nfun\n14\nfavorite\n12\nfound\n10\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(Bing_sentiments) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\nbing_word_counts <- tidy_posts_for_guns %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm<- quanteda::dfm(top_guns_corpus, verbose = FALSE)\n\ntoo_gun_dfm\n\n\nDocument-feature matrix of: 998 documents, 2,661 features (99.65% sparse) and 0 docvars.\n       features\ndocs    smith and wesson saturday anyone ? my two favorite 9mm\n  text1     1   1      1        1      1 1  0   0        0   0\n  text2     0   0      0        0      0 0  1   1        1   1\n  text3     0   1      0        0      0 0  1   0        0   0\n  text4     0   1      0        0      0 1  0   0        0   0\n  text5     0   0      0        0      0 0  0   0        0   0\n  text6     0   1      0        0      0 0  0   0        0   0\n[ reached max_ndoc ... 992 more documents, reached max_nfeat ... 2,651 more features ]\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))\ngun_dfm_lda\n\n\nA LDA_VEM topic model with 2 topics.\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\ngun_dfm_lda_topics\n\n\n# A tibble: 5,322 × 3\n   topic term         beta\n   <int> <chr>       <dbl>\n 1     1 smith    0.000540\n 2     2 smith    0.000679\n 3     1 and      0.0254  \n 4     2 and      0.00320 \n 5     1 wesson   0.000669\n 6     2 wesson   0.000956\n 7     1 saturday 0.000655\n 8     2 saturday 0.000564\n 9     1 anyone   0.00138 \n10     2 anyone   0.00167 \n# … with 5,312 more rows\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nbeta_wide <- gun_dfm_lda_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n\n# A tibble: 192 × 4\n   term       topic1    topic2 log_ratio\n   <chr>       <dbl>     <dbl>     <dbl>\n 1 and      0.0254   0.00320      -2.99 \n 2 anyone   0.00138  0.00167       0.275\n 3 ?        0.0118   0.0109       -0.112\n 4 my       0.0380   0.0117       -1.70 \n 5 two      0.00163  0.00203       0.311\n 6 favorite 0.00134  0.00110      -0.291\n 7 s        0.00659  0.00356      -0.888\n 8 1        0.000486 0.00114       1.23 \n 9 tactical 0.00137  0.0000479    -4.84 \n10 w        0.00193  0.000101     -4.26 \n# … with 182 more rows\n\n\n\nbeta_wide %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\n\nTopic Modeling analysis\nResponse: As can be seen above topic modeling may\nbenefit from some data reduction, removing punctuation and stop words\nwould likely be beneficial as can be seen above where a number of the\ndifferences between topics are modeled as punctuation and stop\nwords.\n\n\ngun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(top_guns_corpus, remove_punct = TRUE), c(stopwords(\"english\")))\n\ngun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=\" \")\n)\n\ngun_corpus_stopwords_and_punct_removed\n\n\nCorpus consisting of 998 documents.\ntext1 :\n\"Smith Wesson Saturday anyone\"\n\ntext2 :\n\"two favorite 9mm s\"\n\ntext3 :\n\"Arex Zero 1 Tactical w Trijicon RMR Viridian XL5 Gen 3 Hogue...\"\n\ntext4 :\n\"Opinion fnx9 Already xd p320 m18 SA DA interesting sure\"\n\ntext5 :\n\"Howa 1500 308\"\n\ntext6 :\n\"little comparison\"\n\n[ reached max_ndoc ... 992 more documents ]\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm_no_punct_stopwords<- quanteda::dfm(gun_corpus_stopwords_and_punct_removed, verbose = FALSE)\n\ntoo_gun_dfm_no_punct_stopwords\n\n\nDocument-feature matrix of: 998 documents, 2,518 features (99.78% sparse) and 0 docvars.\n       features\ndocs    smith wesson saturday anyone two favorite 9mm s arex zero\n  text1     1      1        1      1   0        0   0 0    0    0\n  text2     0      0        0      0   1        1   1 1    0    0\n  text3     0      0        0      0   0        0   0 0    1    1\n  text4     0      0        0      0   0        0   0 0    0    0\n  text5     0      0        0      0   0        0   0 0    0    0\n  text6     0      0        0      0   0        0   0 0    0    0\n[ reached max_ndoc ... 992 more documents, reached max_nfeat ... 2,508 more features ]\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))\ngun_dfm_lda_nopunct_stop\n\n\nA LDA_VEM topic model with 2 topics.\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n# A tibble: 5,036 × 3\n   topic term         beta\n   <int> <chr>       <dbl>\n 1     1 smith    0.00135 \n 2     2 smith    0.000782\n 3     1 wesson   0.00234 \n 4     2 wesson   0.000503\n 5     1 saturday 0.00154 \n 6     2 saturday 0.000596\n 7     1 anyone   0.00339 \n 8     2 anyone   0.00194 \n 9     1 two      0.00320 \n10     2 two      0.00320 \n# … with 5,026 more rows\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nbeta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide_no_punct_stop %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\n\nTopic\nModeling analysis with stopwords and punctuation removed\nResponse: As can be seen from the results above,\nremoving stopwords and punctuation removes a good deal of the unwanted\nlanguage from the corpus and does a slightly more comprehensible job in\ndisplaying the information. However, any kind of stemming or reduction\nwill be difficult with posts about firearms for a number of reasons.\nFirstly the language surrounding firearms involves numbers for model\nnumbers, ammunition calibers and the capacity of magazines and other\ndevices that hold bullets. This results in difficulty removing both\npunctuation and numbers from the data as they give a sense of what sort\nof each of the aforementioned items people are interesting in talking\nabout. As a results removing the punctuation is difficult because it\nallows for more comprehensible data by reducing the usage of unneeded\npunctuation like exclamaintion points and questions marks that are\ncommon on a forum of this nature but not useful in analyzing the common\ntopics and language.\n\n\n\n",
    "preview": "posts/2022-03-03-text-as-data-blog-post-2/text-as-data-blog-post-2_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-03-06T16:27:02-05:00",
    "input_file": "text-as-data-blog-post-2.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-21-week-5-interpretaive-assignment/",
    "title": "Week 5 (and 6) Interpretaive Assignment",
    "description": "A brief analysis of wars in 1000s, 1100s, and 1200s comparing faction centrality and brokerage.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-02-21",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nBackground and Research\nQuestion:\nWikipedia is self-described as a “free content, multilingual online\nencyclopedia written and maintained by a community of volunteers through\na model of open collaboration,” information on the website i shared and\nmaintained “using a wiki-based editing system. [and] Individual\ncontributors,” being the 5th most visited website in the world it is\nalso the largest and most-read reference work in history.” (“https://en.wikipedia.org/wiki/Wikipedia”) Because of\nWikipedia’s position, as an arbiter of information and reference for a\nnumber of subjects, it is essential to understand the nature of the\nrequired citations and how they inform the websites portrayal of history\nand information. (Chase 2021)\nConflict is a defining feature of history, the results of war and the\ngroups involved are essential to understanding dynamics of power\nglobally. A war can represent the transfer of material, territorial, and\nstrategic power between groups. As a result the networks of wars between\nnations can give some notion of power centrality among warring\nnations.\nSince wikipedia has become a widely accepted (if often critiqued)\nsource of information, its citations and the information resulting from\nthem can give us a sense of how where the center of global conflict, and\nthus the most central nations, according to popular and accessible\nacademic literature.\nhttps://www.visualcapitalist.com/the-50-most-visited-websites-in-the-world/\nChase, Matt. “Wikipedia is 20, and its reputation has never been\nhigher”. The Economist. January 9, 2021. Retrieved February 25,\n2021.\n\n\n\nPart 1:\nDescribe the Dataset You Are\nUsing:\nThe Dataset Being Used: The dataset that I am using\nis wikipedia list of wars throughout history, this article is the “List\nof wars: 1000–1499” which acts as a subset of the “2nd-millennium\nconflicts” I chose this dataset as an exemplar of popular history’s\ndepiction of the centralization of worldwide conflict. Wikipedia, being\nan accessible source generally created from relevant citations makes it\na good case study to see where historical writers and academics center\ntheir world are relevant conflicts.\nIdentify initial network\nformat:\nAnswer: The initial network format is as an edge\nlist, the first, in column contains the winners of each\nwar while the second, out column contains the losers of\neach. These sets of belligerents are directed\nNetwork\nStructure: Wars Startings in the 1000s\n\n Network attributes:\n  vertices = 117 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 153 \n    missing edges= 0 \n    non-missing edges= 153 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork\nStructure: Wars Startings in the 1100s\n\n Network attributes:\n  vertices = 97 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 238 \n    missing edges= 0 \n    non-missing edges= 238 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork\nStructure: Wars Startings in the 1200s\n\n Network attributes:\n  vertices = 162 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 313 \n    missing edges= 0 \n    non-missing edges= 313 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nIdentify Nodes: Describe and identify the nodes\n(including how many nodes are in the dataset)\nAnswer: Nodes or vertices in these datasets\nrepresent belligerents in wars throughout history, the involved parties\nin each conflict can be a nation, province, individual, or group so long\nas they are listed as involved in the conflict. In the 1000s there are\n117, in the 1100s there are 78 and in the 1200s there are 161.\nWhat Constitutes a Tie: What constitutes a tie or\nedge (including how many ties, whether ties are directed/undirected and\nweighted/binary, and how to interpret the value of the tie if any)\nAnswer: A tie or edge in this dataset represents a\nwar, this war can be between two nations or groups within a nation.\nThese edges can represent a war that involved many more nations but are\nalways tied to each and every party involved on both sides. These edges\nare directed and the direction indicates which side “won” the conflict\n(if an edge has an arrow pointing to another the node that originated\nthat arrow won the war against them. There are 153 edges in the 1000s,\n225 edges in 1100s and 313 edges in the 1200s.\nEdge Attributes and Subset: Whether or not there are\nedge attributes that might be used to subset data or stack multiple\nnetworks (e.g., tie type, year, etc).\nAnswer: There are a number of attributes that could\nbe used to subset the data, year that the conflict began or the length\nof time it lasted are available. Aspects like each side’s religion and\nthe area where the conflict took place could be used to subset the data\nitself.\nPart 2:\nBrokerage and Betweeness\ncentrality\nWhat are betweeness and brokerage cenrrality\nCalculate brokerage and betweenneess centrality measures for one or more\nsubsets of your network data, and write up the results and your\ninterpretation of them.\nAnswer: I will be calculating these measures for\nwars in 1000-1099, 1100-1199, and 1200-1399.\n\n\n\nBrokerage scores in the\n1000s\n\n\n\n\n\n(wars_in_1000s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,11:15)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nByzantine Empire\n25.2728731\nNaN\n3.9930250\nNaN\nNaN\nSultanate of Rum\n10.4364880\nNaN\n-0.5014965\nNaN\nNaN\nHoly Roman Empire\n10.0243662\nNaN\n4.8919293\nNaN\nNaN\nEngland\n7.5516353\nNaN\n6.6906430\n-0.0800286\n-0.0800286\nKingdom of Sicily\n5.4910263\nNaN\n3.9930250\nNaN\nNaN\nKingdom of France\n2.1940518\nNaN\n-0.5014965\nNaN\nNaN\nSeljuk Empire\n1.7819300\n-0.0271615\n-0.5004595\n6.3140099\n-0.1374029\nKingdom of Georgia\n0.5455645\n-0.0158536\n-0.5009747\n-0.1126843\n-0.1126843\nPapal States\n0.5455645\n-0.0158536\n-0.5009747\n-0.1126843\n11.6872209\nNormandy\n0.1334427\n-0.1256118\n0.4229345\n-0.2730778\n-0.2730778\n\nBrokerage scores in the\n1100s\n\n\n\n\n\n\n\n\n(wars_in_1100s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,10:14)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nKingdom of Jerusalem\n17.1050061\nNaN\n2.8705599\n24.5610650\n-0.1357675\nFatimid Caliphate\n10.2415178\nNaN\n-0.6472506\nNaN\nNaN\nAyyubid Dynasty\n9.3615834\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nZengid Dynasty\n7.4257278\nNaN\n0.7591543\nNaN\nNaN\nByzantine Empire\n6.8977671\nNaN\n0.7602887\n-0.1357675\n-0.1357675\nEngland\n5.8418459\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nHoly Roman Empire\n3.0260558\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of France\n1.6181608\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of Sicily\n0.5622395\n-0.1467125\n-0.6293842\n-0.3476788\n-0.3476788\nPapal States\n0.0342789\n-0.1264908\n-0.6336748\n-0.3236913\n2.5014147\n\nBrokerage scores in the\n1200s\n\n\n\n\n\n\n\n\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nMongol Empire\n48.379636\nNaN\n-0.5944061\nNaN\nNaN\nKingdom of France\n28.915776\nNaN\n-0.5944061\nNaN\nNaN\nAyyubid Dynasty\n27.233714\nNaN\n1.6475606\nNaN\nNaN\nKingdom of England\n22.187528\nNaN\n9.1207832\nNaN\nNaN\nRepublic of Genoa\n12.095156\nNaN\n-0.5944061\nNaN\nNaN\nKnights Templar\n10.172799\nNaN\n1.6475606\nNaN\nNaN\nHoly Roman Empire\n4.886318\n-0.0168705\n-0.5939130\n10.98895\n10.9889530\nPrincipality of Antioch\n4.886318\n-0.0168705\n2.3961055\n13.76762\n-0.1257267\nKingdom of Cyprus\n4.646024\n59.2666679\n0.1535916\n13.76762\n10.9889530\nCounty of Tripoli\n3.684845\nNaN\n0.9005226\n19.49749\n-0.0891814\n\n\nname\nbroker.gate\nPapal States\n11.6872209\nCounty of Sicily\n-0.0800286\nEngland\n-0.0800286\nCounty of Aversa\n-0.1126843\nKingdom of Georgia\n-0.1126843\nGreat Seljuq Empire\n-0.1126843\nChola Empire\n-0.1126843\nTaifa of Lérida\n-0.1126843\nCounty of Apulia\n-0.1374029\nSeljuk Empire\n-0.1374029\nname\nbroker.tot\nByzantine Empire\n25.2728731\nSultanate of Rum\n10.4364880\nHoly Roman Empire\n10.0243662\nEngland\n7.5516353\nKingdom of Sicily\n5.4910263\nKingdom of France\n2.1940518\nSeljuk Empire\n1.7819300\nKingdom of Georgia\n0.5455645\nPapal States\n0.5455645\nNormandy\n0.1334427\n\nOption 2.A\nFor a Specific Research Question: If you have a\nspecific research question, please feel free to use that to guide your\nanalysis. Otherwise, you may want to orient your analysis as follows in\norder to identify a compelling question or noteworthy pattern in the\ndata that can be interpreted.\nAnswer: Since I am interested in the relative power\nof nations by their relative position ad centrality in the worldwide\nconflict, network brokerage can be used to illustrate significant\npositions in global conflict. Below I wanted to look at 4 kinds of\nbrokerage, these are broker.gate or gatekeeper, coordinator, liason, and\nitinerant. I am interested to see if these specific coordination types\nare primarily done by specific nations.\n\n\n\n\n\n\n\n\n\nTotal Brokerage\nExplanation: Looking at total brokerage in this\ndataset gives a sense of which factions were responsible for highest\nconnection of unconnected actors through conflict. Given the crusades\nigniting conflict between Europe and the middle east it is sensible that\nthe Byzantine Empire in the center of both connects the most unconnected\nactors through conflict closely followed by the Sultanate of Rum, a\nmajor Muslim faction that fought against the crusades and third being\nthe Holy Roman Empire who participated in many conflicts including the\ncrusades. These are followed by England who centered the wars in the\nBritish isles and the Kingdom of Sicily who were also in a position of\nconflict.\n\nname\nbroker.tot\nByzantine Empire\n25.272873\nSultanate of Rum\n10.436488\nHoly Roman Empire\n10.024366\nEngland\n7.551635\nKingdom of Sicily\n5.491026\n\nCoordinator Brokerage\nExplanation: In this case no particular country is\nvery high above any other in terms of their coordinator brokerage,\nmeaning that within groups no particular nations appear to be brokering\nmore within the groups.\n\nname\nbroker.coord\nCounty of Aversa\n-0.0158536\nKingdom of Georgia\n-0.0158536\nGreat Seljuq Empire\n-0.0158536\nPapal States\n-0.0158536\nChola Empire\n-0.0158536\n\nItinerant Brokerage\nExplanation: Itinerant brokerage represents when a\nnon-group actor connects 2 actors in a group it is no in to each other,\nin this case England has the highest score. Looking at the network graph\nthey do appear to connect 2 actors in a group together.\n\nname\nbroker.itin\nEngland\n6.6906430\nHoly Roman Empire\n4.8919293\nKingdom of Sicily\n3.9930250\nByzantine Empire\n3.9930250\nPrincipality of Kiev\n0.4413428\n\nRepresentative Brokerage\nExplanation: Representative brokerage indicates that\nthe broker, or nation in question loses a war to another in their group,\nbut wins another against a faction outside of their group. This can be\nthough of as their directed connections to them. In this case the Seljuk\nEmpire and Kingdom of Aragon have instances in which they lose to\nfactions within their group before beating those outside of it.\n\nname\nbroker.rep\nSeljuk Empire\n6.3140099\nKingdom of Aragon\n1.1415607\nCounty of Sicily\n-0.0800286\nEngland\n-0.0800286\nCounty of Aversa\n-0.1126843\n\nGatekeeper Brokerage\nExplanation: The Papal states being ranked highest\nin gatekeeper brokerage is an interesting observation as no other nation\nin the dataset appears to be close to their level as most are negative\nin this category. In this cae being a gatekeeper means that they are in\nat conflict in a group with another while the nation in a different\ngroup of conflicts is only at war with them from the group. This is an\ninteresting observation given the Papal states role as a coordinator of\nthe war, but not a participant in the conflcit as directly as other\nbelligerents. (This being the crusade given the period)\n\nname\nbroker.gate\nPapal States\n11.6872209\nCounty of Sicily\n-0.0800286\nEngland\n-0.0800286\nCounty of Aversa\n-0.1126843\nKingdom of Georgia\n-0.1126843\n\nLiaison Brokerage\nExplanation: A liaison broker, in this case, is a\nfaction that loses a war to a group they do not belong to and wins a war\nagainst a different group than the first that they also do not belong\nto. The Byzantine Empire, Sultanate of Rum, and Holy Roman Empire are\nhighest in this category likely owing to their frequent states of\nconflict beyond the crusades against a variety of groups.\n\nname\nbroker.lia\nByzantine Empire\n31.767840\nSultanate of Rum\n14.454659\nHoly Roman Empire\n10.545231\nEngland\n6.202342\nKingdom of Sicily\n4.960334\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1000s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1000s Plot igraph\n\n\n\nNetwork Graphing 1100s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1100s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1100s Plot igraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInto the 20th Century\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(information regarding the meaning of each type of brokerage was\nacquired from https://edis.ifas.ufl.edu/publication/WC197)\n\n\n\n",
    "preview": "posts/2022-02-21-week-5-interpretaive-assignment/week-5-interpretaive-assignment_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-03-07T21:37:51-05:00",
    "input_file": "week-5-interpretaive-assignment.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-10-workwithmedievalnetworks/",
    "title": "Work with Medieval Networks",
    "description": "A Brief Analysis of Networks of Medieval Conflict.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-02-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\nBriefly Describe the Dataset You Are Using:\nQuestion: Identify initial network format\n\n\n\nNetwork Structure: Wars Startings in the 1000s\n\n Network attributes:\n  vertices = 117 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 153 \n    missing edges= 0 \n    non-missing edges= 153 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork Structure: Wars Startings in the 1100s\n\n Network attributes:\n  vertices = 78 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 225 \n    missing edges= 0 \n    non-missing edges= 225 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork Structure: Wars Startings in the 1200s\n\n Network attributes:\n  vertices = 161 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 313 \n    missing edges= 0 \n    non-missing edges= 313 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nQuestion: Describe and identify the nodes (including how many nodes are in the dataset)\nAnswer: Nodes or vertices in these datasets represent belligerents in wars throughout history, the involved parties in each conflict can be a nation, province, individual, or group so long as they are listed as involved in the conflict.\nQuestion: What constitutes a tie or edge (including how many ties, whether ties are directed/undirected and weighted/binary, and how to interpret the value of the tie if any)\nAnswer: A tie or edge in this dataset represents a war, this war can be between two nations or groups within a nation. These edges can represent a war that involved many more nations but are always tied to each and every party involved on both sides. These edges are directed and the direction indicates which side “won” the conflict.\nQuestion: Whether or not there are edge attributes that might be used to subset data or stack multiple networks (e.g., tie type, year, etc).\nAnswer: There are a number of attributes that could be used to subset the data, years that the conflict began or the length of time it lasted are available. Aspects like each side’s religion and the area where the conflict took place.\nCloseness Betweeness and Eigenvector Centrality\nQuestion: Calculate closeness, betweenness and eigenvector centrality measures for your network data, and bonachic-power if possible. Compare these measures to basic degree centrality measures. Try to interpret the results. Are there any interesting patterns in the distribution of measures or correlations between them that provide insight into the measures?\nCentralization: Wars Startings in the 1000s\n\nname\ntotdegree\nindegree\noutdegree\neigen\nCounty of Apulia\n7\n0\n7\n0.000000\nCounty of Aversa\n5\n0\n5\n0.000000\nKingdom of Sicily\n8\n3\n5\n0.000000\nGhaznavids\n2\n0\n2\n0.000000\nKingdom of Poland\n1\n0\n1\n0.182744\nKingdom of Hungary\n3\n0\n3\n0.000000\n\n\n\n\n\n\n\n\n\ncentralization(wars_in_1000s_network, degree, cmode=\"outdegree\") %>% kable()\n\n\nx\n0.1016647\n\ncentralization(wars_in_1000s_network, degree, cmode=\"indegree\") %>% kable()\n\n\nx\n0.0494946\n\ncentralization(wars_in_1000s_network, degree) %>% kable()\n\n\nx\n0.0674663\n\nAs can be seen above I have coded measures of indegree, outdegree, and total centralization in the wars in the 1000s dataset.\nCentralization: Wars Startings in the 1100s\n\nname\ntotdegree\nindegree\noutdegree\neigen\nSultanate of Rum\n23\n0\n23\n0.2893116\nDanishmends\n13\n0\n13\n0.0690251\nSeljuk Emirate of Aleppo\n13\n0\n13\n0.0690251\nKingdom of Jerusalem\n23\n11\n12\n0.5378974\nFatimids of Egypt\n1\n0\n1\n0.0973860\nKingdom of Norway\n4\n0\n4\n0.0398827\n\n\n\n\n\n\n\n\n\ncentralization(wars_in_1100s_network, degree, cmode=\"outdegree\") %>% kable()\n\n\nx\n0.2646315\n\ncentralization(wars_in_1100s_network, degree, cmode=\"indegree\") %>% kable()\n\n\nx\n0.1593861\n\ncentralization(wars_in_1100s_network, degree) %>% kable()\n\n\nx\n0.1148325\n\nI do the same above and calculate measures of indegree, outdegree, and total centralization in the wars in the 1100s dataset.\nCentralization: Wars Startings in the 1200s\n\nname\ntotdegree\nindegree\noutdegree\neigen\nForces of Bohemond IV of Antioch\n3\n0\n3\n0.0000000\nKnights Templar\n22\n3\n19\n0.4663665\nAyyubid Emirate of Aleppo\n3\n0\n3\n0.0000000\nKingdom of Jerusalem\n3\n0\n3\n0.0000000\nSultanate of Rum\n3\n0\n3\n0.0000000\nHoly Roman Empire\n11\n6\n5\n0.1770181\n\n\n\n\n\n\n\n\n\ncentralization(wars_in_1200s_network, degree, cmode=\"outdegree\") %>% kable()\n\n\nx\n0.2016016\n\ncentralization(wars_in_1200s_network, degree, cmode=\"indegree\") %>% kable()\n\n\nx\n0.0758203\n\ncentralization(wars_in_1200s_network, degree) %>% kable()\n\n\nx\n0.1174332\n\nFinally I calculate measures of indegree, outdegree, and total centralization in the wars in the 1200s dataset.\nEigenvector Centralization\nQuestion: You may also want to identify whether the same node(s) are more/less central using the different measures, and see if you can find any patterns in the results based on which nodes are high/low on each measures. Discuss (with any related evidence) whether or not the node(s) behavior is in line with or violates expectations based on the degree centrality measure, comparing across those measures using a data frame similar to that constructed in the syntax. Be sure to share your assignment with group members for comments.\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nByzantine Empire\n18\n5\n13\n0.478430\n0.3571429\n0.1708679\n0.6428571\n0.3075621\nHoly Roman Empire\n15\n2\n13\n0.295686\n1.0000000\n0.2956860\n0.0000000\n0.0000000\nMirdasid Emirate of Aleppo\n1\n0\n1\n0.295686\n0.0000000\n0.0000000\n1.0000000\n0.2956860\nKingdom of Georgia\n5\n1\n4\n0.295686\n0.0769231\n0.0227451\n0.9230769\n0.2729409\nDuklja\n1\n0\n1\n0.295686\n0.0000000\n0.0000000\n1.0000000\n0.2956860\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nKingdom of Jerusalem\n23\n11\n12\n0.5378974\n0.2980769\n0.1603348\n0.7019231\n0.3775626\nAyyubid Dynasty\n18\n10\n8\n0.5188525\n0.3200000\n0.1660328\n0.6800000\n0.3528197\nSultanate of Rum\n23\n0\n23\n0.2893116\n0.0000000\n0.0000000\n1.0000000\n0.2893116\nHoly Roman Empire\n12\n8\n4\n0.2303733\n0.0892857\n0.0205690\n0.9107143\n0.2098042\nAlmoravid Dynasty\n10\n0\n10\n0.2202864\n0.0000000\n0.0000000\n1.0000000\n0.2202864\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nKnights Templar\n22\n3\n19\n0.4663665\n0.1428571\n0.0666238\n0.8571429\n0.3997427\nAyyubid Dynasty\n23\n9\n14\n0.4263748\n0.1046512\n0.0446206\n0.8953488\n0.3817542\nBahris\n7\n0\n7\n0.2911519\n0.0000000\n0.0000000\n1.0000000\n0.2911519\nRepublic of Genoa\n18\n5\n13\n0.2880895\n0.1428571\n0.0411556\n0.8571429\n0.2469339\nMongol Empire\n41\n7\n34\n0.2228895\n0.1206897\n0.0269005\n0.8793103\n0.1959891\n\nAnswer: Looking at the outputs above we see that regular eigenvector centrality, which accounts for the centrality of nodes attached to our node of interest. Here we see the top 5 highest observations in each dataset, in the 1000s the Byzantine Empire has the highest war, which is logical historically as they represented a bridge between major fighting factions in the Crusades between the Muslim Middle East and European west. The Kingdom of Jerusalem in the 1100s would also be connected, through its wars to other important powers being contested territory between Muslim empires like the Ayyubid Dynasty and crusading states like the Holy Roman Empire.\n\n\narrange(wars_in_1000s.nodes.stat, desc(eigen.rc\n))%>%\n  slice(1:5) %>% kable()\n\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nHoly Roman Empire\n15\n2\n13\n0.295686\n1.0000000\n0.2956860\n0.0000000\n0.0000000\nByzantine Empire\n18\n5\n13\n0.478430\n0.3571429\n0.1708679\n0.6428571\n0.3075621\nSultanate of Rum\n11\n7\n4\n0.295686\n0.1186441\n0.0350814\n0.8813559\n0.2606046\nKingdom of Georgia\n5\n1\n4\n0.295686\n0.0769231\n0.0227451\n0.9230769\n0.2729409\nKingdom of France\n10\n3\n7\n0.182744\n0.1034483\n0.0189046\n0.8965517\n0.1638394\n\narrange(wars_in_1100s.nodes.stat, desc(eigen.rc\n))%>%\n  slice(1:5) %>% kable()\n\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nAyyubid Dynasty\n18\n10\n8\n0.5188525\n0.3200000\n0.1660328\n0.6800000\n0.3528197\nKingdom of Jerusalem\n23\n11\n12\n0.5378974\n0.2980769\n0.1603348\n0.7019231\n0.3775626\nFatimid Caliphate\n17\n7\n10\n0.2202864\n0.2884615\n0.0635442\n0.7115385\n0.1567423\nZengid Dynasty\n16\n6\n10\n0.2202864\n0.1714286\n0.0377634\n0.8285714\n0.1825230\nRepublic of Venice\n6\n1\n5\n0.0797655\n0.3333333\n0.0265885\n0.6666667\n0.0531770\n\narrange(wars_in_1200s.nodes.stat, desc(eigen.rc\n))%>%\n  slice(1:5) %>% kable()\n\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nKnights Templar\n22\n3\n19\n0.4663665\n0.1428571\n0.0666238\n0.8571429\n0.3997427\nAyyubid Dynasty\n23\n9\n14\n0.4263748\n0.1046512\n0.0446206\n0.8953488\n0.3817542\nRepublic of Genoa\n18\n5\n13\n0.2880895\n0.1428571\n0.0411556\n0.8571429\n0.2469339\nKingdom of Cyprus\n11\n4\n7\n0.2025283\n0.1428571\n0.0289326\n0.8571429\n0.1735957\nMongol Empire\n41\n7\n34\n0.2228895\n0.1206897\n0.0269005\n0.8793103\n0.1959891\n\nAnswer: Reflected centrality represents the centrality one nation receives from another another that is attributable to the first nations contribution to the other nations score. Looking at reflected centrality through wars we see relatively similar trends. In the 1000s the Holy Roman Empire and Byzantine Empire play a distinct role in the crusades so in this case we see how central a nation is in conflict dependent on how much centrality they contribute to a nation they have a conflict with. Being focal points in the center of Europe in the case of the HRE and the focal point of Christian, Muslim conflict in the case of the Byzantine empire. For both fighting minor nations gives the two a great deal of centrality to other, potentially less central groups that they fight. The Ayyubids and Kingdom of Jerusalem that each occupy a central position in conflict. Into the 1200s the Knights Templar and Ayyubids occupy this position.\n\n\narrange(wars_in_1000s.nodes.stat, desc(eigen.dc\n\n))%>%\n  slice(1:5) %>% kable()\n\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nByzantine Empire\n18\n5\n13\n0.478430\n0.3571429\n0.1708679\n0.6428571\n0.3075621\nMirdasid Emirate of Aleppo\n1\n0\n1\n0.295686\n0.0000000\n0.0000000\n1.0000000\n0.2956860\nDuklja\n1\n0\n1\n0.295686\n0.0000000\n0.0000000\n1.0000000\n0.2956860\nGreat Seljuq Empire\n4\n0\n4\n0.295686\n0.0000000\n0.0000000\n1.0000000\n0.2956860\nKingdom of Georgia\n5\n1\n4\n0.295686\n0.0769231\n0.0227451\n0.9230769\n0.2729409\n\narrange(wars_in_1100s.nodes.stat, desc(eigen.dc\n\n))%>%\n  slice(1:5) %>% kable()\n\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nKingdom of Jerusalem\n23\n11\n12\n0.5378974\n0.2980769\n0.1603348\n0.7019231\n0.3775626\nAyyubid Dynasty\n18\n10\n8\n0.5188525\n0.3200000\n0.1660328\n0.6800000\n0.3528197\nSultanate of Rum\n23\n0\n23\n0.2893116\n0.0000000\n0.0000000\n1.0000000\n0.2893116\nAlmoravid Dynasty\n10\n0\n10\n0.2202864\n0.0000000\n0.0000000\n1.0000000\n0.2202864\nAbbasid Caliphate\n10\n0\n10\n0.2202864\n0.0000000\n0.0000000\n1.0000000\n0.2202864\n\narrange(wars_in_1200s.nodes.stat, desc(eigen.dc\n\n))%>%\n  slice(1:5) %>% kable()\n\n\nname\ntotdegree\nindegree\noutdegree\neigen\nrc\neigen.rc\ndc\neigen.dc\nKnights Templar\n22\n3\n19\n0.4663665\n0.1428571\n0.0666238\n0.8571429\n0.3997427\nAyyubid Dynasty\n23\n9\n14\n0.4263748\n0.1046512\n0.0446206\n0.8953488\n0.3817542\nBahris\n7\n0\n7\n0.2911519\n0.0000000\n0.0000000\n1.0000000\n0.2911519\nRepublic of Genoa\n18\n5\n13\n0.2880895\n0.1428571\n0.0411556\n0.8571429\n0.2469339\nAnti-Imperial faction\n7\n0\n7\n0.2025283\n0.0000000\n0.0000000\n1.0000000\n0.2025283\n\nAnswer: Finally Derived Centrality measures the centrality a nation receives from fighting another nation that is not a reflection of the first nation’s contribution of centrality to the others. Here the Byzantine Empire remains one of the largest, potentially suggesting their status as a cosmopolitan being high in both derived and reflected centrality. However, nations like the Mirdasid Emirate of Aleppo have little to no reflected centrality by high derived centrality suggesting that they are are a pure bridge.\n\n\nwars_in_1000s.nodes<-data.frame(name=V(wars_in_1000s.ig)$name,\n    totdegree=igraph::degree(wars_in_1000s.ig, loops=FALSE),\n    indegree=igraph::degree(wars_in_1000s.ig, mode=\"in\", loops=FALSE),\n    outdegree=igraph::degree(wars_in_1000s.ig, mode=\"out\", loops=FALSE))\n\nwars_in_1100s.nodes<-data.frame(name=V(wars_in_1100s.ig)$name,\n    totdegree=igraph::degree(wars_in_1100s.ig, loops=FALSE),\n    indegree=igraph::degree(wars_in_1100s.ig, mode=\"in\", loops=FALSE),\n    outdegree=igraph::degree(wars_in_1100s.ig, mode=\"out\", loops=FALSE))\n\nwars_in_1200s.nodes<-data.frame(name=V(wars_in_1200s.ig)$name,\n    totdegree=igraph::degree(wars_in_1200s.ig, loops=FALSE),\n    indegree=igraph::degree(wars_in_1200s.ig, mode=\"in\", loops=FALSE),\n    outdegree=igraph::degree(wars_in_1200s.ig, mode=\"out\", loops=FALSE))\n\neigen_cent_wars_in_1000s<-centr_eigen(wars_in_1000s.ig,directed=T)\n\neigen_cent_wars_in_1100s<-centr_eigen(wars_in_1100s.ig,directed=T)\n\neigen_cent_wars_in_1200s<-centr_eigen(wars_in_1200s.ig,directed=T)\n\n\n\nSome Attempts at Visualization\nGraph 1:\nExplanation: Below I have graphed the network for the 1000s dataset using base plot and an edgelist\n\n\nplot(wars_in_1000s_edgelist_network_edgelist, vertex.size=2,  edge.arrow.size=.2, vertex.color=\"gold\", vertex.size=20, vertex.frame.color=\"gray\", vertex.label.color=\"black\", vertex.label.cex=0.25, vertex.label.dist=2, edge.curved=0.2)\n\n\n\n\nGraph 2:\nExplanation: Next I used the igraph network object and graphed it using the automatic, proportional clustering.\n\n\n\nGraph 3:\nExplanation: Finally I used the ggplot network and graphed it using a generic network object. This order follows for the visualization of the 1100s datasets and 1200s.\n\n\n\nGraph 4:\n\n\nplot(wars_in_1100s_edgelist_network_edgelist, vertex.size=2,  edge.arrow.size=.2, vertex.color=\"gold\", vertex.size=20, vertex.frame.color=\"gray\", vertex.label.color=\"black\", vertex.label.cex=0.25, vertex.label.dist=2, edge.curved=0.2)\n\n\n\n\nGraph 5:\n\n\n\nGraph 6:\n\n\n\nGraph 7:\n\n\nplot(wars_in_1200s_edgelist_network_edgelist, vertex.size=2,  edge.arrow.size=.2, vertex.color=\"gold\", vertex.size=20, vertex.frame.color=\"gray\", vertex.label.color=\"black\", vertex.label.cex=0.25, vertex.label.dist=2, edge.curved=0.2)\n\n\n\n\nGraph 8:\n\n\n\nGraph 9:\n\n\n\n\n$mut\n[1] 3\n\n$asym\n[1] 147\n\n$null\n[1] 6636\n\n\n [1] 243974  14792    744    288    149    129     11     28     13\n[10]      0      1      0      0      1      0      0\n\n\n[1] 0.06481481\n[1] 0.0774983\n[1] 0.093224\n\n\n[1] 0.01117686\n[1] 0.03698225\n[1] 0.01207515\n\n\n     name               degree      \n Length:117         Min.   : 1.000  \n Class :character   1st Qu.: 1.000  \n Mode  :character   Median : 1.000  \n                    Mean   : 2.615  \n                    3rd Qu.: 3.000  \n                    Max.   :18.000  \n\n\n                name degree\n15  Byzantine Empire     18\n25 Holy Roman Empire     15\n28  Sultanate of Rum     11\n47 Kingdom of France     10\n29  County of Sicily      9\n34           England      9\n\n\n     name               degree      \n Length:117         Min.   : 1.000  \n Class :character   1st Qu.: 1.000  \n Mode  :character   Median : 1.000  \n                    Mean   : 2.615  \n                    3rd Qu.: 3.000  \n                    Max.   :18.000  \n\n\n\n\n",
    "preview": "posts/2022-02-10-workwithmedievalnetworks/workwithmedievalnetworks_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-02-17T14:21:24-05:00",
    "input_file": "workwithmedievalnetworks.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-02-07-florentine-families-week-3-assignment/",
    "title": "Florentine Families Week 3 Assignment",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-02-07",
    "categories": [],
    "contents": "\n\n\n\n\n\nset.seed(1)\ncanvas_flow(colors = colorPalette(\"vrolik3\"), lines = 3000, lwd = 0.5,\n            iterations = 1000, stepmax = 0.1, angles = 'svm')\n\n\n\n# see ?canvas_flow for more input parameters of this function\n\n\n\nData Description:\n\n\ndata(flo, package=\"network\")\n\n\nstatted_florentine<-as.network(flo)\n\nprint(statted_florentine)\n\n\n Network attributes:\n  vertices = 16 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 40 \n    missing edges= 0 \n    non-missing edges= 40 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\n\n\nplot(statted_florentine)\n\n\n\n\nNodes and Edges:\nAs can be seen from the graph and summary statistics above the florentine family dataset is made up of 40 edges and 16 vertices. Each vertex or node is a family in florence. Each edge, or tie represents a marriage between the two.\nAs a result the ties are not weighted, so they are binary. However, though the dataset labels the ties are directed, they are actually all mutual so in essence, the dataset does not measure marriaage “into” a family but simply uses a mutual directed tie for every set of families with a marriage between them.\n\n\nsna::dyad.census(statted_florentine)\n\n\n     Mut Asym Null\n[1,]  20    0  100\n\nAs can be seen from the code above the dyads within the dataset are either mutual or null, meaning that between any two nodes there are no asymmetric dyads despite the fact that the network is labeled as being directed.\n\n\nsna::triad.census(statted_florentine)\n\n\n     003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U\n[1,] 324   0 195    0    0    0    0    0    0    0  38    0    0\n     120C 210 300\n[1,]    0   0   3\n\nAs can be seen from the triad census above the types of triads in the dataset are 324 of the 003-type, 102 of the 102-type, 38 of the 201-type and 3 of the 300-type. The 324 of 003-type and 195 of the 102-type indicate that 519 of the 560 triads are vacuously transitive, the remaining 38 of the 201-type are intransitive and only the 3 300-types are transitive\nTransitivity\nAs can be seen in the code below indicates the the transitivity of the network as a whole is 0.1914894, which follows from the results above. This would indicate a relatively low rate of transitivity and thus low inter-connectedness of the nodes and potientally lower density overall.\n\n\ngtrans(statted_florentine)\n\n\n[1] 0.1914894\n\nIsolates\nThough the code below indicates that there are 12 isolates in the network, the data from our plot suggests that this is likely untrue and due to the discrepancy between the un-directed true nature of the network and its label as directed as a network object.\n\n\nisolates(statted_florentine)\n\n\n[1] 12\n\nDensity\nAs indicated by the code chunk below network density here is 0.1666667 indicating that only 16.67% of possible ties in the network are made.\n\n\nnetwork.density(statted_florentine)\n\n\n[1] 0.1666667\n\nDegree and Interesting Results\nEach of the 16 node’s degrees are acquired from the code bellow in integer form, from the digits alone it is difficult to distinguish any sort of overall pattern in the data.\n\n\nsna::degree(statted_florentine)\n\n\n [1]  2  6  4  6  6  2  8  2 12  2  6  0  6  4  8  6\n\nBelow I create a data frame with the overall degree of each family.\n\n\nflorentine.nodes<-data.frame(name=statted_florentine%v%\"vertex.names\",\n                             degree=sna::degree(statted_florentine))\nflorentine.nodes\n\n\n           name degree\n1    Acciaiuoli      2\n2       Albizzi      6\n3     Barbadori      4\n4      Bischeri      6\n5    Castellani      6\n6        Ginori      2\n7      Guadagni      8\n8  Lamberteschi      2\n9        Medici     12\n10        Pazzi      2\n11      Peruzzi      6\n12        Pucci      0\n13      Ridolfi      6\n14     Salviati      4\n15      Strozzi      8\n16   Tornabuoni      6\n\nBelow I then calculate each individual nodes indegree and out-degree.\nNotewory Results\nUpon running the above code I realized that despite the network being labeled “directed” all of its ties were actually mutual and thus the in-degree and out-degree would be identical since marriage is considered mutual. Though this follows from the fact that are generally considered mutual it is interesting to observe that marriages are not considered “into” another family but are represented as a mutual, but directed ties between nodes in every case.\n\n\nsna::degree(statted_florentine, cmode=\"indegree\")\n\n\n [1] 1 3 2 3 3 1 4 1 6 1 3 0 3 2 4 3\n\nsna::degree(statted_florentine, cmode=\"outdegree\")\n\n\n [1] 1 3 2 3 3 1 4 1 6 1 3 0 3 2 4 3\n\nAs can be seen from the code above each node has identical in-degrees and out-degrees\n\n\nflorentine.nodes <- data.frame(name=statted_florentine%v%\"vertex.names\",\n    totdegree=sna::degree(statted_florentine),\n    indegree=sna::degree(statted_florentine, cmode=\"indegree\"),\n    outdegree=sna::degree(statted_florentine, cmode=\"outdegree\"))\n\nflorentine.nodes\n\n\n           name totdegree indegree outdegree\n1    Acciaiuoli         2        1         1\n2       Albizzi         6        3         3\n3     Barbadori         4        2         2\n4      Bischeri         6        3         3\n5    Castellani         6        3         3\n6        Ginori         2        1         1\n7      Guadagni         8        4         4\n8  Lamberteschi         2        1         1\n9        Medici        12        6         6\n10        Pazzi         2        1         1\n11      Peruzzi         6        3         3\n12        Pucci         0        0         0\n13      Ridolfi         6        3         3\n14     Salviati         4        2         2\n15      Strozzi         8        4         4\n16   Tornabuoni         6        3         3\n\nThe notion is confirmed by the degree summaries in the code below and the table above. In both cases all indicators are identical for both in-degree and out-degree and sum to the values seen in the total-degree column.\n\n\nsummary(florentine.nodes)\n\n\n     name             totdegree     indegree     outdegree  \n Length:16          Min.   : 0   Min.   :0.0   Min.   :0.0  \n Class :character   1st Qu.: 2   1st Qu.:1.0   1st Qu.:1.0  \n Mode  :character   Median : 6   Median :3.0   Median :3.0  \n                    Mean   : 5   Mean   :2.5   Mean   :2.5  \n                    3rd Qu.: 6   3rd Qu.:3.0   3rd Qu.:3.0  \n                    Max.   :12   Max.   :6.0   Max.   :6.0  \n\nHistograms of the distribution of the degrees also illustrate that the only difference between in-degree, out-degree, and total degree is that total degree has values twice as large for marriages but the same relative frequency.\n\n\nhist(florentine.nodes$totdegree, main=\"Florentine Marriages: Total Degrees\", xlab=\"Marriages\")\n\n\n\n\n\n\nhist(florentine.nodes$indegree, main=\"Florentine Marriages: In-Degrees\", xlab=\"Marriages\")\n\n\n\n\n\n\nhist(florentine.nodes$outdegree, main=\"Florentine Marriages: Out-Degrees\", xlab=\"Marriages\")\n\n\n\n\nThese results do make degree analysis ineffectual on the dataset, but offer some potential questions. Specifically it would be useful to look at dynamics of marriages “into” other families as this would have been important at the time. Though marriage is considered symmetric in this dataset it was often used in asymmetric ways during the medieval and renaissance times as a means of diplomacy and establishing goodwill between nobility and as a result the ties between the families are not necessarily symmetric in all cases. It would be interesting to see married into what families in terms of sex and birth status hierarchy i.e. first, second, third born, etc.\n\n\nflo_sorted_by_total_degree<-florentine.nodes[order(-florentine.nodes$totdegree),]\n\nflo_sorted_by_total_degree\n\n\n           name totdegree indegree outdegree\n9        Medici        12        6         6\n7      Guadagni         8        4         4\n15      Strozzi         8        4         4\n2       Albizzi         6        3         3\n4      Bischeri         6        3         3\n5    Castellani         6        3         3\n11      Peruzzi         6        3         3\n13      Ridolfi         6        3         3\n16   Tornabuoni         6        3         3\n3     Barbadori         4        2         2\n14     Salviati         4        2         2\n1    Acciaiuoli         2        1         1\n6        Ginori         2        1         1\n8  Lamberteschi         2        1         1\n10        Pazzi         2        1         1\n12        Pucci         0        0         0\n\nThe results above indicate that the medici family are the most “central” node in the network having the most marriages and thus the most in-degrees and out-degrees. The mean of these two degrees is 2.5 in the dataset thus they have more than double the average marriages, however 4 other families have just 2, while the pucci’s have no ties.\nThis also poses the question as to whether or not there are more families that could be included and what the criteria was for allowance in the florentine family data. In this case 4-6 marriages is the most common with 6 nodes having 4-6 marriages.\nThe least central are the Pucci family who are included in the dataset but have no edges or connections to others families by marriage. The Acciaiuoli, Ginori, Lamberteschi, and Pazzi all have 2 total ties, one in and one out representing a single marriage.\n\n\ncentralization(statted_florentine, degree, cmode=\"indegree\")\n\n\n[1] 0.2488889\n\nThe measure of centralization above represents how concentrated edges between nodes are concentrated among actors. He centralization is slightly less than 25% which would indicate that concentration is relatively low this follows from the low transitivity of the network as a whole, there very few actual connections as compared to the possible connections. As a result of these findings it appears as if the network is not extremely centralized with most families having 2-3 marriages with the largest only being 6. The network also has a low level of transitivity as most nodes are not connected to one another.\nHowever it is important to note that we do not know aspects certain aspects of the families that are relevant to understanding the extent of the network’s true connectivity, this could be heirs with ties to both families marriages and the number of marriages between families that happened out of the possible number afforded to each family by their number of heirs. Having family tree measurements would allow for more meaningful analysis that could make the network truly directed by seeing who married into what family and what they represented to their families in terms of succession including their effects in creating prior generations that continued these connections.\n\n\n\n",
    "preview": "posts/2022-02-07-florentine-families-week-3-assignment/florentine-families-week-3-assignment_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-02-10T20:32:18-05:00",
    "input_file": "florentine-families-week-3-assignment.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-31-homework2network/",
    "title": "Homework_2_Network",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-01-31",
    "categories": [],
    "contents": "\n\n\nlibrary(network)\n\nlibrary(igraph)\n\nlibrary(aRtsy)\n\n\n\n\n\nset.seed(1)\ncanvas_diamonds(colors = colorPalette(\"klimt\"))\n\n\n\n\n\n\ndata(flo, package=\"network\")\n\nmatrix <- as.matrix(flo) \n\nflorentine<- graph.adjacency(flo, mode=\"directed\", weighted=NULL)\n\n\nplot(florentine)\n\n\n\n\n\n\nvcount(florentine)\n\n\n[1] 16\n\necount(florentine)\n\n\n[1] 40\n\n\n\n\n",
    "preview": "posts/2022-01-31-homework2network/homework2network_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-02-01T13:11:48-05:00",
    "input_file": "homework2network.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2022-01-31-the-sharpe-ratio/",
    "title": "Initial Network Analysis Florentine Family",
    "description": "Homework 2: Brief Analysis of the Florentine Family Set",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-01-31",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nChosen Network:\nThe Network I chose was the Florentine family dataset network from the network package. This dataset represents marriages between influential families in Florence. Each node represents a family with arrows indicating which family married into which other family.\nNetwork Size:\nNetwork size for the Florentine Family network dataset is described briefly by the code below:\n\n\nset.seed(2)\n# I access the florentine families dataset through the package \"network\"\n\ndata(flo, package=\"network\")\n\n# In order to make the dataset usable I make it into a matrix using as.matrix()\n\nmatrix_florentine <- as.matrix(flo) \n\n# Next I used the graph.adjacency command to make the florentine dataset into\n# An Adjacency Matrix \n\nflorentine<- graph.adjacency(flo, mode=\"directed\", weighted=NULL)\n\nplot(florentine, vertex.color=\"skyblue1\", vertex.label.color=\"navy\", \n     vertex.label.dist=1, vertex.size=7,  edge.arrow.size=0.5, margin=0000)\n\n\n\n\n\n\nvcount(florentine)\n\n\n[1] 16\n\necount(florentine)\n\n\n[1] 40\n\n\n\nstatted_florentine<-as.network(matrix_florentine)\nprint(statted_florentine)\n\n\n Network attributes:\n  vertices = 16 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 40 \n    missing edges= 0 \n    non-missing edges= 40 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nThe number of vertices in the dataset is 16, in this case these nodes each represent a florentine family. Te number of edges is 40. In this instance ties or edges, represent marriages between these influential families. The is the same in both the statnet and igraph outputs.\nNetwork Features\nNext the features of the dataset are described using the 3 commands in the code chunk below.\n\n\nis_bipartite(florentine)\n\n\n[1] FALSE\n\nis_directed(florentine)\n\n\n[1] TRUE\n\nis_weighted(florentine)\n\n\n[1] FALSE\n\nThese results suggests that the florentine family dataset is not bipartite, which would indicate that they do not fall into to sets. In addition the dataset’s ties are directed do they flow in directions as a result of which family is marrying to which, and the ties are unweighted because marriage cannot vary in numerican significance, it is simply a category that is either true or false between families.\n\n\nvertex_attr_names(florentine)\n\n\n[1] \"name\"\n\nedge_attr_names(florentine)\n\n\ncharacter(0)\n\n\n\nnetwork::list.vertex.attributes(statted_florentine)\n\n\n[1] \"na\"           \"vertex.names\"\n\nnetwork::list.edge.attributes(statted_florentine)\n\n\n[1] \"na\"\n\nThe vector attribute names are the names of the florentine families.\nThe edge attributes do not have name.\nDyad and Triad Census\nDyads\n\n\nigraph::dyad.census(florentine)\n\n\n$mut\n[1] 20\n\n$asym\n[1] 0\n\n$null\n[1] 100\n\n\n\nsna::dyad.census(statted_florentine)\n\n\n     Mut Asym Null\n[1,]  20    0  100\n\nThe dyad census indicates that amongest that among the dyad ties, being between 2 families, 20 are mutual and none are asymptotic. In addition 100 of the dyad are null, or not connected.\nTriads\n\n\nigraph::triad_census(florentine)\n\n\n [1] 324   0 195   0   0   0   0   0   0   0  38   0   0   0   0   3\n\n?triad_census(florentine)\n\n\n\n\n\nsna::triad.census(statted_florentine, mode=\"graph\")\n\n\n       0   1  2 3\n[1,] 324 195 38 3\n\n\n\nsum(sna::triad.census(statted_florentine, mode=\"graph\"))\n\n\n[1] 560\n\nFor dyads there are 16 possibilities. Here our results are as follows:\n324, 003 (A,B,C, the empty graph.)\n195, 102 (A<->B, C, the graph with a mutual connection between two vertices.)\nboth of which are vacuously transitive\n38, 030C (A<-B<-C, A->C.)\n3, 300 (A<->B<->C, A<->C, the complete graph.)\nwhich are both transitive\nBoth statnet and igraph confirm this, however igraph specifies the exact instances of each type of connection.\nNetwork Transitivity\n\n\ntransitivity(florentine, type=\"global\")\n\n\n[1] 0.1914894\n\nThe global transitivity of the graph is 0.1914894 which is the ratio of triangles to connected triangles.\n\n\ntransitivity(florentine, type=\"average\")\n\n\n[1] 0.2181818\n\nThe average transitivity is the average transitivity of local triad clusters. Here it is 0.2181818. This is the ratio of local triangles to all connected triangles.\n\n\ngtrans(statted_florentine)\n\n\n[1] 0.1914894\n\nStatnet code confirms this transitivity assessment.\n\n\nV(florentine)[c(\"Peruzzi\",\"Lamberteschi\", \"Ginori\")]\n\n\n+ 3/16 vertices, named, from e7ac6d4:\n[1] Peruzzi      Lamberteschi Ginori      \n\nNetwork Distances\nFirst we see that the average path length between 2 nodes in our dataset is 2.485714\n\n\naverage.path.length(florentine,directed=F)\n\n\n[1] 2.485714\n\nI then test the distances between 3 families, those being the Peruzzi, Lamberteschi, and Ginori\n\n\ndistances(florentine,\"Peruzzi\", \"Lamberteschi\")\n\n\n        Lamberteschi\nPeruzzi            3\n\nThe distance between the nodes of Peruzzi and Lamberteschi is 3\n\n\ndistances(florentine,\"Peruzzi\", \"Ginori\")\n\n\n        Ginori\nPeruzzi      4\n\nThe distance between the nodes of Peruzzi and Ginori is 4\n\n\ndistances(florentine,\"Lamberteschi\",\"Ginori\")\n\n\n             Ginori\nLamberteschi      3\n\nThe distance between the nodes of Lamberteschi and Ginori is 3.\n\n\nisolates(statted_florentine)\n\n\n[1] 12\n\nThe names of all families in our dataset is as follows:\n\n\nas.vector(statted_florentine%v%'vertex.names')\n\n\n [1] \"Acciaiuoli\"   \"Albizzi\"      \"Barbadori\"    \"Bischeri\"    \n [5] \"Castellani\"   \"Ginori\"       \"Guadagni\"     \"Lamberteschi\"\n [9] \"Medici\"       \"Pazzi\"        \"Peruzzi\"      \"Pucci\"       \n[13] \"Ridolfi\"      \"Salviati\"     \"Strozzi\"      \"Tornabuoni\"  \n\nThe names of the isolated influential families is as follows below:\n\n\nas.vector(statted_florentine%v%'vertex.names')[c(isolates(statted_florentine))]\n\n\n[1] \"Pucci\"\n\n\n\n\n",
    "preview": "posts/2022-01-31-the-sharpe-ratio/Homework_2_Noah_Milstein_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-02-02T23:34:49-05:00",
    "input_file": "Homework_2_Noah_Milstein.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog!",
    "description": "This is my New blog for code relating to Political and Social Networks",
    "author": [
      {
        "name": "Noah Milstein",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-01-31",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-02-03T13:53:55-05:00",
    "input_file": "welcome.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
