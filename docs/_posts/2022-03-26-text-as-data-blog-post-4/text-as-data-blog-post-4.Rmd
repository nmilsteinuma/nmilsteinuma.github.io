---
title: "Text as Data Blog Post 4"
description: |
  In last weeks post I combined a the material for blog posts 2, 3, and began on the 4th post. As a result this will be a continuation of dictionary validation methods.
author:
  - name: Noah Milstein
    url: {}
date: 2022-03-26
output:
  distill::distill_article:
    self_contained: false
---


```{r, echo=FALSE}
library(rmarkdown)
library(RedditExtractoR)
library(jsonlite)
library(tidyverse)
library(stringr)
library(dplyr)
library(httr)
library(tm)
library(corpus)
library(quanteda)
library(textclean)
library(knitr)
library(lubridate)
library(cleanNLP)
library(quanteda.textstats)
library(quanteda.textplots)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stopwords)
library(tidyverse)
library(rvest)
library(tidytext)
library(preText)

```

## Blog Post 3 and 4:

### Third Blog Post and Continuation

**Explanation:** This post covers material from week 5 pre-processing, week 6 representing text, and week 7 dictionary methods. In this case I have slightly modified the data I am working with, this week I am using 1000 new posts for a dictionary analysis rather than the "top" posts to see if there is a tangible difference in content.


#### Inital Loading and Processing

**Explanation:** My code below illustrates how I initially got my information from reddit by scraping. In this case I used RedditExtractoR. The author of this packages describes it as a minimalist r wrapper it scrapes a limited number of posts from reddit. The api on reddit itself only allows 60 requests per minute. In this case I chose posts that were "new" as of march 26 2022 at 11:17 P.M. This has resulted in 980 reddit posts, this subreddit is described as "Firearms and related articles" and much of the subreddit is descriptions of, reviews, and highlights of firearms owned by users.

```{r}

# New_guns_urls <- find_thread_urls(subreddit="guns", sort_by="new")


loadRData <- function(fileName){
#loads an RData file, and returns it
    load(fileName)
    get(ls()[ls() != "fileName"])
}
New_guns_urls_df <- loadRData("/Users/noahmilstein/Desktop/Spring 2022/Networks/Git_Projects/DACCS/nmilsteinuma.github.io/docs/_posts/2022-03-26-text-as-data-blog-post-4/New_reddit_posts_3_26.RData")




```

```{r}
New_guns_urls_df
str(New_guns_urls_df)

```


#### Conversion From Data Frame to Corpus

**Explanation:** Below I have processed my initial dataframe from Reddit into a corpus and saved a summary of the resulting data.


```{r, results='hide'}

new_guns_urls_df<-New_guns_urls_df[,c("title", "date_utc", "comments")]

new_guns_corpus<-corpus(new_guns_urls_df$title)

new_guns_documents<-new_guns_corpus[1:10,]

new_guns_corpus_summary <- summary(new_guns_corpus)

```

### Broad Characteristics

**Explanation:** In order to clean the documents for pre-processing and analysis I have removed punctuation, converted to lowercase and removed stopwords. Though the language of firearms is often associated with punctuation, such as 5.56, 3.57, and a variety of other calibers, which represent the diameter of the barrel required to fire each ammunition. However, losing the punctuation in caliber and firearm titles would not reduce their comprehensibility in analysis if they retain their form as 5.56 and 556 can be considered equally while reducing the complexity of tokens and potentially even sentences. Converting the documents to lowercase can also simplifies the data. However, for my inital analysis I will just being making a lowercase document feature matrix and a more edited one.

```{r}

new_guns_corpus_dfm_tl<-tokens(new_guns_corpus) %>%  dfm(tolower=TRUE) 

new_guns_corpus_dfm_punct_tl_sw <- tokens(new_guns_corpus,
                                    remove_punct = TRUE,) %>%
                           dfm(tolower=TRUE) %>%
                           dfm_remove(stopwords('english'))


```

#### Top features

**Explanation:** Examining the top 20 features below we see a fairly predictable set of response, as may be expected from the gun subreddit, the most used word is gun. Rifle and pistol are also in the top 20. Individual letters such as "ar", "s", and "m" appear frequently as they are commonly used designations for types of firearms or model names, ar-12, ar-15, m-4, m-16, m1911, 5-mm, and s559, s-12 and other designations. This indicates that these numbers are valuable, if difficult to comprehend on their own. With no punctuation removed the first 20 features are not informative.

```{r}

topfeatures(new_guns_corpus_dfm_tl, 20)

```


```{r}

topfeatures(new_guns_corpus_dfm_punct_tl_sw, 20)

```

#### World Cloud

**Explanation:** Though not necessarily statistically informative, the wordcloud below can give some sense of comparative frequency using the limit of minimum count being 6. Reading through these can give a sense of both the communal nature of the forum in asking for recommendations, but also the importance of the word purchase and other words associated with working with, and buying firearms. In the case of the only lowercased data we can gather much less information.

```{r}
set.seed(123456)

textplot_wordcloud(new_guns_corpus_dfm_tl, min_count = 12, random_order = T, rotation = 0)

```

```{r}

set.seed(123456)

textplot_wordcloud(new_guns_corpus_dfm_punct_tl_sw, min_count = 6, random_order = T, rotation = 0)

```

#### Types, Tokens, and Sentances

##### Types

**Explanation:** The gun corpus summary gives 3 counting categories that we can interpret in order to get a sense of the complexity of the documents that we are using. Looking at the number of types on average we see a mean of 9.18 and qunantiles that indicate a range of 2-45 with 50% being between 4 and 13 types. 

```{r}

mean(new_guns_corpus_summary$Types)

quantile(new_guns_corpus_summary$Types)

```

##### Tokens

**Explanation:** Tokens are relatively similar to types in this case. Here there is a mean of 9.72 but a range of 2-55 with the middle 50% ranging from 4-13 tokens, as was the case for types.

```{r}

mean(new_guns_corpus_summary$Tokens)

quantile(new_guns_corpus_summary$Tokens)


```

##### Sentances

**Explanation:** As is indicated below, it appears that the number of sentences in each post is generally one. Arroding to the qunatile statistics the most sentences in any post

```{r}

mean(new_guns_corpus_summary$Sentences)

quantile(new_guns_corpus_summary$Sentences)

```

##### Word counts

**Explanation:** Looking at word counts we see a similar trend reflected where including stopwords and punctuation decreases the quality of data as little information but punctuation and stopwords are included.

```{r}
word_counts_new_1 <- as.data.frame(sort(colSums(new_guns_corpus_dfm_tl),dec=T))

colnames(word_counts_new_1) <- c("Frequency")

word_counts_new_1$Rank <- c(1:ncol(new_guns_corpus_dfm_tl))

head(word_counts_new_1)
```

```{r}

word_counts_new <- as.data.frame(sort(colSums(new_guns_corpus_dfm_punct_tl_sw),dec=T))

colnames(word_counts_new) <- c("Frequency")

word_counts_new$Rank <- c(1:ncol(new_guns_corpus_dfm_punct_tl_sw))

head(word_counts_new)

```
##### Zipf's Law

**Explanation:**  As can be seen from the frequency graphs below, Ziph's Law of inverse proportion. In this case a words rank in freqency is inversely prorportional to the number of times it is observed. Though the uncleaned dataset has far more frequency for its most common words (much of which is punctuation) it appears to follow the law.

```{r}

ggplot(word_counts_new, mapping = aes(x = Rank, y = Frequency)) + 
  geom_point() +
  labs(title = "Zipf's Law", x = "Rank", y = "Frequency") + 
  theme_bw()

```

```{r}
ggplot(word_counts_new_1, mapping = aes(x = Rank, y = Frequency)) + 
  geom_point() +
  labs(title = "Zipf's Law", x = "Rank", y = "Frequency") + 
  theme_bw()
```

##### Data Trimming

**Explanation:** Much of what I do here will be explained in the code and results. Many words appear with a minimum frequency of 4, though non are included in 10% and only 3 words are included in 5%. At a level of 2.5% we get 4 words.

```{r}
# First I trim the data to only include words that appear at least 4 times

smaller_dfm_4_freq <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 4)

# Next I will look at proportions are see if there are words that are seen in
# More than 10% and 5% of documents
smaller_dfm_10_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.1, docfreq_type = "prop")

smaller_dfm_5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.05, docfreq_type = "prop")

smaller_dfm_2.5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.025,  docfreq_type = "prop")

```

#### Data Readability

**Explanation:** Before making general modifications to the data, it is valuable to also get a sense of readability, as in week 5. In this case we will calculate readability scores based on 3 different measures, FOG, Coleman Liau, and Flesch Kincaid. Though this step will not indicate what sort of pre-processing is best, or how the data should be reduced, it does give us some insight into the complexity of the language in our data. In this case we just observe the readability based on the post number.

```{r}

readability_new_guns <- textstat_readability(new_guns_corpus, 
                                    measure = c("Flesch.Kincaid", "FOG", "Coleman.Liau.grade")) 

# add in a chapter number

readability_new_guns$reddit_post <- c(1:nrow(readability_new_guns))

# plot results
ggplot(readability_new_guns, aes(x = reddit_post)) +
  geom_line(aes(y = Flesch.Kincaid), color = "black",  alpha=0.3) + 
  geom_line(aes(y = FOG), color = "red", alpha=0.3) + 
  geom_line(aes(y = Coleman.Liau.grade), color = "blue", alpha=0.3) + 
  theme_bw()

```
**Explanation:** In this part we will add dates to our data to see how the complexity changes over time or if it was relatively constant. As can be seen below, the amount of complexity in the data varies more smoothly when the data are sorted by data and not arbitrarily by their post number, in this case all 3 complexity method exhibit similar trends.

```{r, warning=FALSE}


readability_new_guns$added_dates <- as.Date(New_guns_urls_df$date_utc)

ggplot(readability_new_guns, aes(x = added_dates)) +
  geom_smooth(aes(y = Flesch.Kincaid), color = "black") + 
  geom_smooth(aes(y = FOG), color = "red") + 
  geom_smooth(aes(y = Coleman.Liau.grade), color = "blue") + 
  theme_minimal()

```

**Explanation:** Looking at the readability of the we see that all correlations between the methods of complexity measurement are similar except of FOG and Coleman Liau, however the graphs above do indicate some similarity in trend between them, though not direct correlation in their estimates potentially.

```{r}

cor(readability_new_guns$Flesch.Kincaid, readability_new_guns$FOG, use = "complete.obs")

```

```{r}

cor(readability_new_guns$Flesch.Kincaid, readability_new_guns$Coleman.Liau.grade, use = "complete.obs")

```

```{r}

cor(readability_new_guns$FOG, readability_new_guns$Coleman.Liau.grade, use = "complete.obs")

```

#### Pre-processing Before Reduction and Co-Occurance

**Explanation:** Next I used the factorial_preprocessing() command to both use n-grams processing and use an infrequent term threshold. This is in order to see what techniques, such as removing punctuation, stopwords, etc lead to a pre-text score devised by Denny and Spirling. This pre-text score indicatess how many k-pairs of terms change the most when the pre-processing strategy is changed. Lower scores indicate more usual results while higher scores indicate more unusual results and they are between 0 and 1. Here we have used n-grams and set an infreqent term threshold. Because of the nature of our data I will use 30% of documents as 

```{r}
?factorial_preprocessing
preprocessed_documents <- factorial_preprocessing(
    new_guns_corpus,
    use_ngrams = TRUE,
    infrequent_term_threshold = 0.3,
    verbose = FALSE)

```

```{r}

names(preprocessed_documents)

```

**Explanation:** As can be seen below the possible choices are coded on the first column with each subsequent column indicating whether or not each choice includes each of the specified choices in its assessment.

```{r}

head(preprocessed_documents$choices)

```

**Explanation:** Next preText is calculated using 50 comparisons and a cosine distance calculation.

```{r}
#set.seed(12366)
#preText_results <- preText(
#    preprocessed_documents,
#   dataset_name = "Gun Pretext Results",
#   distance_method = "cosine",
#   num_comparisons = 50,
#  verbose = TRUE)

```

```{r}
#save(preText_results, file="preText_results_3_27_gun_50_comp.RData")
```

```{r}

preText_results <- loadRData("~/Desktop/Spring 2022/Networks/Git_Projects/DACCS/nmilsteinuma.github.io/docs/_posts/2022-03-26-text-as-data-blog-post-4/preText_results_3_27_gun_50_comp.RData")

preText_results
```


```{r}

#load("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData")

preText_score_plot(preText_results)

```

**Explanation:** After plotting we access the pretext score with the minimum score, which is least unusual. This is the row with the pre-processing steps refered to as "3" in the data. In addition L-3 results in the same preText Score.

```{r}

scores_new_pretext<-preText_results$preText_score 

# head(sort(scores_new_pretext))

```

**Explanation:** Looking at the choices below I see that "3" does not do anything but use n-grams. L-3 does use lowercase and n-grams.

```{r}
# preprocessed_documents$choices
```

**Explanation Continued:** Looking at the regression coefficients we see negative scores as usual results and positive coefficients as unusual ones. In this case removing puncuation, stopwords, and n-grams would not lead to a great deal of abnormality. The scores below indicate that stemming would result in the most abnormality while all others but lowercase is the only other that has a non-negative coefficinet.

```{r}

regression_coefficient_plot(preText_results,
                            remove_intercept = TRUE)

```

##### Feature Co-occurance Matrix

**Explanation:** The feature co-occurance matrix can give us a sense of which words in the dataset are occurring together 

```{r}
# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters
smaller_dfm <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 5)

#smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = "prop")

# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)
```


```{r}

# pull the top features
myFeatures <- names(topfeatures(smaller_fcm, 40))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# check dimensions
dim(even_smaller_fcm)

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)

```

### Sentiment Results Using NRC

```{r}
# get_sentiments("nrc")
# get_sentiments("bing")
# get_sentiments("afinn")
```
```{r}

sentimetnsdf <- read_csv("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv")

```

```{r}

new_guns_urls_df_2<-new_guns_urls_df

new_guns_urls_df_2$text<- seq(1, 980, by=1)

nrc_joy <- sentimetnsdf %>% 
  filter(sentiment == "joy")

tidy_posts_for_guns <- new_guns_urls_df_2 %>%
  unnest_tokens(word, title) 

tidy_posts_for_guns %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) %>% head() %>% kable()

```

```{r}

tidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%
  inner_join(sentimetnsdf) %>%
  count(text, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

```{r}

nrc_sentiment <- get_sentiments("nrc")


nrc_guns_word_counts <- tidy_posts_for_guns %>%
  inner_join(nrc_sentiment) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

nrc_guns_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

```{r}

Bing_sentiments<-get_sentiments("bing")

tidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%
  inner_join(Bing_sentiments) %>%
  count(text, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bing_word_counts <- tidy_posts_for_guns %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


```{r}


tidy_posts_for_guns$added_dates <- as.Date(tidy_posts_for_guns$date_utc)


afinn <- tidy_posts_for_guns %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = added_dates) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")
afinn
```

```{r}
afinn %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE,   width = 0.7)  + 
  geom_smooth(aes(y = sentiment), color = "black")+
facet_wrap(~method, ncol = 1, scales = "free_y")+
  theme_minimal()

```


### Sentiment Results Using BING


**Explanation:** Using nrc appears to have had some unintended effects that may require an analysis of the specific words used to describe sentiment. One difficult part of the data being used is that firearms, and the words used to describe them, are percieved 



```{r, eval=FALSE}

library(methods)

too_gun_dfm<- quanteda::dfm(new_guns_corpus, verbose = FALSE)

too_gun_dfm

```


```{r, eval=FALSE}

library(topicmodels)

gun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))
gun_dfm_lda

```

```{r,  eval=FALSE}

gun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = "beta")
gun_dfm_lda_topics

```

```{r,  eval=FALSE}

gun_top_terms <- gun_dfm_lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

gun_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```
```{r,  eval=FALSE}

beta_wide <- gun_dfm_lda_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide

```
```{r, eval=FALSE}

beta_wide %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)

```

#### Topic Modeling analysis

**Response:** As can be seen above topic modeling may benefit from some data reduction, removing punctuation and stop words would likely be beneficial as can be seen above where a number of the differences between topics are modeled as punctuation and stop words.

```{r, eval=FALSE}

gun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(new_guns_corpus, remove_punct = TRUE), c(stopwords("english")))

gun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=" ")
)

gun_corpus_stopwords_and_punct_removed

```

```{r,  eval=FALSE}

library(methods)

too_gun_dfm_no_punct_stopwords<- quanteda::dfm(tokens(gun_corpus_stopwords_and_punct_removed), verbose = FALSE)

too_gun_dfm_no_punct_stopwords

```

```{r,  eval=FALSE}

library(topicmodels)

gun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))

gun_dfm_lda_nopunct_stop


```

```{r,  eval=FALSE}

gun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = "beta")

gun_dfm_lda_topics_nopunct_stop

```

```{r,  eval=FALSE}

gun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

gun_top_terms_no_punct_or_stop %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

```{r,  eval=FALSE}

beta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

```

```{r,  eval=FALSE}

beta_wide_no_punct_stop %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)

```

#### Tokens and Corpus Work

```{r}

top_guns_tokens <- tokens(new_guns_corpus)

print(top_guns_tokens)

top_guns_tokens_no_punct <- tokens(new_guns_corpus, 
    remove_punct = T)

print(top_guns_tokens_no_punct)

```

```{r}

top_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)

print(top_guns_tokens_no_punct_no_upper)

```

```{r}

top_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords("en"), selection = "remove")

length(top_guns_tokens_no_punct_no_upper_no_stop)

print(top_guns_tokens_no_punct_no_upper_no_stop)

```


```{r, include=FALSE}

cnlp_init_udpipe()

text_for_top_guns <- as.character(new_guns_corpus)

top_guns_corpus_2 <- docvars(new_guns_corpus)

top_guns_corpus_2$text <- text_for_top_guns

annotated.guns_corpus <- cnlp_annotate(top_guns_corpus_2)

```

```{r}

top_guns_corpus_tokens <- tokens(new_guns_corpus)

print(top_guns_corpus_tokens)

```

```{r}

head(annotated.guns_corpus$token)

head(annotated.guns_corpus$document)

doc_id_guns<-annotated.guns_corpus$document

doc_id_guns$date<-new_guns_urls_df$date_utc

annoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = "doc_id")

annoData$date<-as.Date(annoData$date)

```

```{r}

annoData %>% 
  group_by(date) %>% 
  summarize(Sentences = max(sid)) %>%
  ggplot(aes(date, Sentences)) +
    geom_line() +
    geom_smooth() +
    theme_bw()

```




```{r}

#sentimetnsdf<-get_sentiments("nrc")

#write.csv(sentimetnsdf, file = "sentimetnsdf.csv")

#save(sentimetnsdf, file="sentimetnsdf_2")

```

`

#### Topic Modeling analysis with stopwords and punctuation removed

**Response:** As can be seen from the results above, removing stopwords and punctuation removes a good deal of the unwanted language from the corpus and does a slightly more comprehensible job in displaying the information. However, any kind of stemming or reduction will be difficult with posts about firearms for a number of reasons. Firstly the language surrounding firearms involves numbers for model numbers, ammunition calibers and the capacity of magazines and other devices that hold bullets. This results in difficulty removing both punctuation and numbers from the data as they give a sense of what sort of each of the aforementioned items people are interesting in talking about. As a results removing the punctuation is difficult because it allows for more comprehensible data by reducing the usage of unneeded punctuation like exclamaintion points and questions marks that are common on a forum of this nature but not useful in analyzing the common topics and language.


