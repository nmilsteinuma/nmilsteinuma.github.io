---
title: "Text as Data Blog Post 2"
description: |
  A short description of the post.
author:
  - name: Noah Milstein
    url: {}
date: 2022-03-03
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library(rmarkdown)
library(RedditExtractoR)
library(jsonlite)
library(tidyverse)
library(stringr)
library(dplyr)
library(httr)
library(tm)
#install.packages("textclean")
#install.packages("tmap")
#library(tmap)
library(corpus)
library(quanteda)
library(textclean)
library(knitr)
library(lubridate)
library(cleanNLP)
library(quanteda.textstats)
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("stopwords")
library(tidyverse)
library(rvest)
library(tidytext)
# colors from RColorBrewer::brewer.pal(6, "Set1")
palette(c("#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33"))
library(preText)
```

### Blog Post 2:

**Question:** For my second blog post I will be observing the content of the sub-reddit I am scraping, extracting information, and performing initial natural language processing and text analysis on it.

Navigate and describe the characteristics of the data source of your
interest, if you’ve specified one (or plan B). The characteristics include
(1) its ‘content’ and/or (2) how it can be scraped.
The characteristics include

(1) its ‘content’ and/or (2) how it can be scraped.
I Summon up your knowledge of some useful packages we’ve reviewed

and/or NLP tools in relation to your research project.

I Summon up your knowledge of some useful packages we’ve reviewed
and/or NLP tools in relation to your research project.
 Sorting out adjectives?
 Extracting major verbs or named entities? ...

#### Inital Loading and Processing

**Explanation:** My code below illustrates how I initially got my information from reddit by scraping. In this case I used RedditExtractoR. The author of this packages describes it as a minimalist r wrapper it scrapes a limited number of posts from reddit. The api on reddit itself only allows 60 requests per minute so I will have to extract the next set of data later and pick a period before my first post occured.

```{r, results='hide'}
#top_guns_urls <- find_thread_urls(subreddit="guns", sort_by="top")

load("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/df_guns.RData")

str(top_guns_urls)

top_guns_urls_df=top_guns_urls[,c("title", "date_utc", "comments")]
top_guns_urls_df
#guns_contents <- get_thread_content(top_guns_urls_df$url[1:1000])
#str(guns_contents$threads)

```

#### Conversion From Data Frame to Corpus

**Explanation:** Below I have processed my inital dataframe from reddit into a corpus.


```{r, results='hide'}

top_guns_urls_df=top_guns_urls[,c("title", "date_utc", "comments")]

top_guns_corpus<-corpus(top_guns_urls_df$title)

top_guns_documents<-top_guns_corpus[1:10,]

top_guns_corpus_summary <- summary(top_guns_corpus)

```

#### Pre-processing 

**Explanation:** Next I used the factorial_preprocessing() command to both use n-grams processing and use an infrequent term threshold.

```{r}

preprocessed_documents <- factorial_preprocessing(
    top_guns_corpus,
    use_ngrams = TRUE,
    infrequent_term_threshold = 0.2,
    verbose = FALSE)

```

```{r}

names(preprocessed_documents)

```

```{r}

head(preprocessed_documents$choices)

```


#### preText Results

**Explanation:** Next I used preText() to pre-process the documents that I have so far to acquire pre-text scores that can give me a sense of what techniques may be necessary for natural language processing as the project develops.

```{r}

#preText_results <- preText(
#    preprocessed_documents,
#    dataset_name = "Gun Pretext Results",
#    distance_method = "cosine",
#    num_comparisons = 50,
#    verbose = TRUE)

```

**Explanation Continued:** Next we look at the pre-text scores with 50 comparisons, which was acquired from the code above. Below these are graphed with intercept. 

```{r}

load("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData")
preText_results
preText_score_plot(preText_results)

```

The lowest score with intercepts according to the graph is N-3 which removes numbers, and uses n-grams. This plot represents the potential risk of using more complex pre-processing at the pre-text score goes up but may remove more information. The highest score belongs to L-S which is lowercased and stemmed which is quite risky to do.

**Explanation Continued:** Looking at the regression coefficients we see negative scores as usual results and positive coefficients as unusual ones. In this case removing puncuation, stopwords, and n-grams would not lead to a great deal of abnormalilty.

```{r}

regression_coefficient_plot(preText_results,
                            remove_intercept = TRUE)

```

#### Tokens and Corpus Work

```{r}

top_guns_tokens <- tokens(top_guns_corpus)

print(top_guns_tokens)

top_guns_tokens_no_punct <- tokens(top_guns_corpus, 
    remove_punct = T)

print(top_guns_tokens_no_punct)

```

```{r}

top_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)

print(top_guns_tokens_no_punct_no_upper)

```

```{r}

top_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords("en"), selection = "remove")

length(top_guns_tokens_no_punct_no_upper_no_stop)

print(top_guns_tokens_no_punct_no_upper_no_stop)

```


```{r}
cnlp_init_udpipe()

text_for_top_guns <- as.character(top_guns_corpus)

top_guns_corpus_2 <- docvars(top_guns_corpus)

top_guns_corpus_2$text <- text_for_top_guns

annotated.guns_corpus <- cnlp_annotate(top_guns_corpus_2)

```

```{r}

top_guns_corpus_tokens <- tokens(top_guns_corpus)

print(top_guns_corpus_tokens)

```

```{r}

head(annotated.guns_corpus$token)

head(annotated.guns_corpus$document)

doc_id_guns<-annotated.guns_corpus$document
doc_id_guns
doc_id_guns$date<-top_guns_urls_df$date_utc

annoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = "doc_id")

annoData$date<-as.Date(annoData$date)

```

```{r}

annoData %>% 
  group_by(date) %>% 
  summarize(Sentences = max(sid)) %>%
  ggplot(aes(date, Sentences)) +
    geom_line() +
    geom_smooth() +
    theme_bw()

```


```{r}

# calculate readability
readability <- textstat_readability(top_guns_corpus, 
                                    measure = c("Flesch.Kincaid")) 

# add in a chapter number
readability$chapter <- c(1:nrow(readability))

# plot results
ggplot(readability, aes(x = chapter, y = Flesch.Kincaid)) +
  geom_line() + 
  geom_smooth() + 
  theme_bw()

```


```{r}

readability <- textstat_readability(top_guns_corpus, 
                                    measure = c("Flesch.Kincaid", "FOG", "Coleman.Liau.grade")) 

# add in a chapter number

readability$post <- c(1:nrow(readability))

# plot results
ggplot(readability, aes(x = post)) +
  geom_line(aes(y = Flesch.Kincaid), color = "black") + 
  geom_line(aes(y = FOG), color = "red") + 
  geom_line(aes(y = Coleman.Liau.grade), color = "blue") + 
  theme_bw()

```

```{r}

annoData$date<-as.Date(annoData$date)

readability$added_dates<-as.Date(top_guns_urls_df$date_utc)

ggplot(readability, aes(x = added_dates)) +
  geom_smooth(aes(y = Flesch.Kincaid), color = "black") + 
  geom_smooth(aes(y = FOG), color = "red") + 
  geom_smooth(aes(y = Coleman.Liau.grade), color = "blue") + 
  theme_minimal()

```


```{r}

cor(readability$Flesch.Kincaid, readability$FOG, use = "complete.obs")

```

```{r}

cor(readability$Flesch.Kincaid, readability$Coleman.Liau.grade, use = "complete.obs")

```

```{r}

cor(readability$FOG, readability$Coleman.Liau.grade, use = "complete.obs")

```

```{r}

#sentimetnsdf<-get_sentiments("nrc")

#write.csv(sentimetnsdf, file = "sentimetnsdf.csv")

#save(sentimetnsdf, file="sentimetnsdf_2")

```

```{r}
library(readr)
sentimetnsdf <- read_csv("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv")

```

```{r}

top_guns_urls_df_2<-top_guns_urls_df
top_guns_urls_df_2$text<- seq(1, 998, by=1)


nrc_joy <- sentimetnsdf %>% 
  filter(sentiment == "joy")

tidy_posts_for_guns <- top_guns_urls_df_2 %>%
  unnest_tokens(word, title) 

tidy_posts_for_guns %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)


tidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%
  inner_join(sentimetnsdf) %>%
  count(text, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

```{r}
tidy_posts_for_guns_sentiment
sentimetnsdf
```


```{r}

nrc_guns_word_counts <- tidy_posts_for_guns %>%
  inner_join(sentimetnsdf) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

```{r}
nrc_guns_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

### Sentiment Results Using NRC

**Explanation:** Using nrc appears to have had some unintended effects that may require an analysis of the specific words used to describe sentiment. One difficult part of the data being used is that firearms, and the words used to describe them, are percieved 


