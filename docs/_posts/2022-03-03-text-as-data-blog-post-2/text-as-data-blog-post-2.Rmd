---
title: "Text as Data Blog Post 2"
description: |
  A short description of the post.
author:
  - name: Noah Milstein
    url: {}
date: 2022-03-03
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library(rmarkdown)
library(RedditExtractoR)
library(jsonlite)
library(tidyverse)
library(stringr)
library(dplyr)
library(httr)
library(tm)
#install.packages("textclean")
#install.packages("tmap")
#library(tmap)
library(corpus)
library(quanteda)
library(textclean)
library(knitr)
library(lubridate)
library(cleanNLP)
library(quanteda.textstats)
library(quanteda.textplots)

library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stopwords)
library(tidyverse)
library(rvest)
library(tidytext)
library(preText)
library(plyr)

```

### Blog Post 2:

**Question:** For my second blog post I will be observing the content of the sub-reddit I am scraping, extracting information, and performing initial natural language processing and text analysis on it.

Navigate and describe the characteristics of the data source of your
interest, if you’ve specified one (or plan B). The characteristics include
(1) its ‘content’ and/or (2) how it can be scraped.
The characteristics include

(1) its ‘content’ and/or (2) how it can be scraped.
I Summon up your knowledge of some useful packages we’ve reviewed

and/or NLP tools in relation to your research project.

I Summon up your knowledge of some useful packages we’ve reviewed
and/or NLP tools in relation to your research project.
 Sorting out adjectives?
 Extracting major verbs or named entities? ...

#### Inital Loading and Processing

**Explanation:** My code below illustrates how I initially got my information from reddit by scraping. In this case I used RedditExtractoR. The author of this packages describes it as a minimalist r wrapper it scrapes a limited number of posts from reddit. The api on reddit itself only allows 60 requests per minute so I will have to extract the next set of data later and pick a period before my first post occured.

```{r, results='hide'}
#top_guns_urls <- find_thread_urls(subreddit="guns", sort_by="top")

load("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/df_guns.RData")

str(top_guns_urls)

top_guns_urls_df=top_guns_urls[,c("title", "date_utc", "comments")]

#guns_contents <- get_thread_content(top_guns_urls_df$url[1:1000])
#str(guns_contents$threads)

```

#### Conversion From Data Frame to Corpus

**Explanation:** Below I have processed my initial dataframe from Reddit into a corpus.


```{r, results='hide'}

top_guns_urls_df=top_guns_urls[,c("title", "date_utc", "comments")]

top_guns_corpus<-corpus(top_guns_urls_df$title)

top_guns_documents<-top_guns_corpus[1:10,]

top_guns_corpus_summary <- summary(top_guns_corpus)

```

### Broad Characteristics

```{r}

top_guns_corpus_dfm <- tokens(top_guns_corpus,
                                    remove_punct = TRUE,) %>%
                           dfm(tolower=TRUE) %>%
                           dfm_remove(stopwords('english'))

```

```{r}

topfeatures(top_guns_corpus_dfm, 20)

```

```{r}

set.seed(123456)

textplot_wordcloud(top_guns_corpus_dfm, min_count = 8, random_order = FALSE)

```
```{r}


top_guns_corpus_summary <- summary(top_guns_corpus)

mean(top_guns_corpus_summary$Types)

mean(top_guns_corpus_summary$Tokens)

mean(top_guns_corpus_summary$Sentences)


```

```{r}

word_counts <- as.data.frame(sort(colSums(top_guns_corpus_dfm),dec=T))

colnames(word_counts) <- c("Frequency")

word_counts$Rank <- c(1:ncol(top_guns_corpus_dfm))

head(word_counts)

```

```{r}

ggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + 
  geom_point() +
  labs(title = "Zipf's Law", x = "Rank", y = "Frequency") + 
  theme_bw()

```

```{r}
# trim based on the overall frequency (i.e., the word counts)
smaller_dfm <- dfm_trim(top_guns_corpus_dfm, min_termfreq = 5)

# trim based on the proportion of documents that the feature appears in; here, 
# the feature needs to appear in more than 10% of documents (chapters)
smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = 0, docfreq_type = "prop")

smaller_dfm
```

```{r}
# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters
smaller_dfm <- dfm_trim(top_guns_corpus_dfm, min_termfreq = 5)

# create fcm from dfm
smaller_fcm <- fcm(smaller_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(smaller_fcm)
```


```{r}

# pull the top features
myFeatures <- names(topfeatures(smaller_fcm, 30))

# retain only those top features as part of our matrix
even_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = "keep")

# check dimensions
dim(even_smaller_fcm)

# compute size weight for vertices in network
size <- log(colSums(even_smaller_fcm))

# create plot
textplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)

```


#### Pre-processing 

**Explanation:** Next I used the factorial_preprocessing() command to both use n-grams processing and use an infrequent term threshold.

```{r}

preprocessed_documents <- factorial_preprocessing(
    top_guns_corpus,
    use_ngrams = TRUE,
    infrequent_term_threshold = 0.2,
    verbose = FALSE)

```

```{r}

names(preprocessed_documents)

```

```{r}

head(preprocessed_documents$choices)

```




#### preText Results

**Explanation:** Next I used preText() to pre-process the documents that I have so far to acquire pre-text scores that can give me a sense of what techniques may be necessary for natural language processing as the project develops.

```{r}

#preText_results <- preText(
#    preprocessed_documents,
#    dataset_name = "Gun Pretext Results",
#    distance_method = "cosine",
#    num_comparisons = 50,
#    verbose = TRUE)

```

**Explanation Continued:** Next we look at the pre-text scores with 50 comparisons, which was acquired from the code above. Below these are graphed with intercept. 

```{r}

load("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData")

preText_score_plot(preText_results)

```

The lowest score with intercepts according to the graph is N-3 which removes numbers, and uses n-grams. This plot represents the potential risk of using more complex pre-processing at the pre-text score goes up but may remove more information. The highest score belongs to L-S which is lowercased and stemmed which is quite risky to do.

**Explanation Continued:** Looking at the regression coefficients we see negative scores as usual results and positive coefficients as unusual ones. In this case removing puncuation, stopwords, and n-grams would not lead to a great deal of abnormalilty.

```{r}

regression_coefficient_plot(preText_results,
                            remove_intercept = TRUE)

```

#### Tokens and Corpus Work

```{r}

top_guns_tokens <- tokens(top_guns_corpus)

print(top_guns_tokens)

top_guns_tokens_no_punct <- tokens(top_guns_corpus, 
    remove_punct = T)

print(top_guns_tokens_no_punct)

```

```{r}

top_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)

print(top_guns_tokens_no_punct_no_upper)

```

```{r}

top_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords("en"), selection = "remove")

length(top_guns_tokens_no_punct_no_upper_no_stop)

print(top_guns_tokens_no_punct_no_upper_no_stop)

```


```{r, include=FALSE}
cnlp_init_udpipe()

text_for_top_guns <- as.character(top_guns_corpus)

top_guns_corpus_2 <- docvars(top_guns_corpus)

top_guns_corpus_2$text <- text_for_top_guns

annotated.guns_corpus <- cnlp_annotate(top_guns_corpus_2)

```

```{r}

top_guns_corpus_tokens <- tokens(top_guns_corpus)

print(top_guns_corpus_tokens)

```

```{r}

head(annotated.guns_corpus$token)

head(annotated.guns_corpus$document)

doc_id_guns<-annotated.guns_corpus$document

doc_id_guns$date<-top_guns_urls_df$date_utc

annoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = "doc_id")

annoData$date<-as.Date(annoData$date)

```

```{r}

annoData %>% 
  group_by(date) %>% 
  summarize(Sentences = max(sid)) %>%
  ggplot(aes(date, Sentences)) +
    geom_line() +
    geom_smooth() +
    theme_bw()

```


```{r}

# calculate readability
readability <- textstat_readability(top_guns_corpus, 
                                    measure = c("Flesch.Kincaid")) 

# add in a chapter number
readability$chapter <- c(1:nrow(readability))

# plot results
ggplot(readability, aes(x = chapter, y = Flesch.Kincaid)) +
  geom_line() + 
  geom_smooth() + 
  theme_bw()

```


```{r}

readability <- textstat_readability(top_guns_corpus, 
                                    measure = c("Flesch.Kincaid", "FOG", "Coleman.Liau.grade")) 

# add in a chapter number

readability$post <- c(1:nrow(readability))

# plot results
ggplot(readability, aes(x = post)) +
  geom_line(aes(y = Flesch.Kincaid), color = "black") + 
  geom_line(aes(y = FOG), color = "red") + 
  geom_line(aes(y = Coleman.Liau.grade), color = "blue") + 
  theme_bw()

```

```{r}

annoData$date<-as.Date(annoData$date)

readability$added_dates<-as.Date(top_guns_urls_df$date_utc)

ggplot(readability, aes(x = added_dates)) +
  geom_smooth(aes(y = Flesch.Kincaid), color = "black") + 
  geom_smooth(aes(y = FOG), color = "red") + 
  geom_smooth(aes(y = Coleman.Liau.grade), color = "blue") + 
  theme_minimal()

```


```{r}

cor(readability$Flesch.Kincaid, readability$FOG, use = "complete.obs")

```

```{r}

cor(readability$Flesch.Kincaid, readability$Coleman.Liau.grade, use = "complete.obs")

```

```{r}

cor(readability$FOG, readability$Coleman.Liau.grade, use = "complete.obs")

```

```{r}

#sentimetnsdf<-get_sentiments("nrc")

#write.csv(sentimetnsdf, file = "sentimetnsdf.csv")

#save(sentimetnsdf, file="sentimetnsdf_2")

```

```{r}
library(readr)
sentimetnsdf <- read_csv("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv")

```

### Sentiment Results Using NRC

```{r}

top_guns_urls_df_2<-top_guns_urls_df
top_guns_urls_df_2$text<- seq(1, 998, by=1)


nrc_joy <- sentimetnsdf %>% 
  filter(sentiment == "joy")

tidy_posts_for_guns <- top_guns_urls_df_2 %>%
  unnest_tokens(word, title) 

tidy_posts_for_guns %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) %>% head() %>% kable()


tidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%
  inner_join(sentimetnsdf) %>%
  count(text, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

```{r}
tidy_posts_for_guns_sentiment
sentimetnsdf
```


```{r}

nrc_guns_word_counts <- tidy_posts_for_guns %>%
  inner_join(sentimetnsdf) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```

```{r}
nrc_guns_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

**Explanation:** Using nrc appears to have had some unintended effects that may require an analysis of the specific words used to describe sentiment. One difficult part of the data being used is that firearms, and the words used to describe them, are percieved 

### Sentiment Results Using BING

```{r}

Bing_sentiments<-get_sentiments("bing")

tidy_posts_for_guns <- top_guns_urls_df_2 %>%
  unnest_tokens(word, title) 

tidy_posts_for_guns %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) %>% head() %>% kable()


tidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%
  inner_join(Bing_sentiments) %>%
  count(text, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

```{r}

bing_word_counts <- tidy_posts_for_guns %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)

```

```{r}

library(methods)

too_gun_dfm<- quanteda::dfm(top_guns_corpus, verbose = FALSE)

too_gun_dfm

```


```{r}

library(topicmodels)

gun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))
gun_dfm_lda

```

```{r}

gun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = "beta")
gun_dfm_lda_topics

```

```{r}

gun_top_terms <- gun_dfm_lda_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

gun_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```
```{r}

beta_wide <- gun_dfm_lda_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_wide

```
```{r}

beta_wide %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)

```

#### Topic Modeling analysis

**Response:** As can be seen above topic modeling may benefit from some data reduction, removing punctuation and stop words would likely be beneficial as can be seen above where a number of the differences between topics are modeled as punctuation and stop words.

```{r}

gun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(top_guns_corpus, remove_punct = TRUE), c(stopwords("english")))

gun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=" ")
)

gun_corpus_stopwords_and_punct_removed

```

```{r}

library(methods)

too_gun_dfm_no_punct_stopwords<- quanteda::dfm(gun_corpus_stopwords_and_punct_removed, verbose = FALSE)

too_gun_dfm_no_punct_stopwords

```

```{r}

library(topicmodels)

gun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))
gun_dfm_lda_nopunct_stop


```

```{r}

gun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = "beta")

gun_dfm_lda_topics_nopunct_stop

```

```{r}

gun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

gun_top_terms_no_punct_or_stop %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

```{r}

beta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

```

```{r}

beta_wide_no_punct_stop %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)

```

#### Topic Modeling analysis with stopwords and punctuation removed

**Response:** As can be seen from the results above, removing stopwords and punctuation removes a good deal of the unwanted language from the corpus and does a slightly more comprehensible job in displaying the information. However, any kind of stemming or reduction will be difficult with posts about firearms for a number of reasons. Firstly the language surrounding firearms involves numbers for model numbers, ammunition calibers and the capacity of magazines and other devices that hold bullets. This results in difficulty removing both punctuation and numbers from the data as they give a sense of what sort of each of the aforementioned items people are interesting in talking about. As a results removing the punctuation is difficult because it allows for more comprehensible data by reducing the usage of unneeded punctuation like exclamaintion points and questions marks that are common on a forum of this nature but not useful in analyzing the common topics and language.

